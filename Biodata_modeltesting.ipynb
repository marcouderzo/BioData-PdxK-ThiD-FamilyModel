{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEt4zB1HBlzR"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8emDDVacQdH",
        "outputId": "f89e5a23-275e-489d-c8fa-1adee89cf0ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\marco\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Flatten, Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive/')\n",
        "#data_dir = '/content/drive/MyDrive/bioproject/biological_data_pfp/biological_data_pfp/train/'\n",
        "data_dir = 'data/train/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "nusH7y0XcuJp",
        "outputId": "615a22b9-a31e-40bc-d7bd-f1f253869823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4277047, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protein_ID</th>\n",
              "      <th>aspect</th>\n",
              "      <th>GO_term</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P91124</td>\n",
              "      <td>cellular_component</td>\n",
              "      <td>GO:0005575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P91124</td>\n",
              "      <td>cellular_component</td>\n",
              "      <td>GO:0110165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P91124</td>\n",
              "      <td>cellular_component</td>\n",
              "      <td>GO:0005737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P91124</td>\n",
              "      <td>cellular_component</td>\n",
              "      <td>GO:0005622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P91124</td>\n",
              "      <td>cellular_component</td>\n",
              "      <td>GO:0043226</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Protein_ID              aspect     GO_term\n",
              "0     P91124  cellular_component  GO:0005575\n",
              "1     P91124  cellular_component  GO:0110165\n",
              "2     P91124  cellular_component  GO:0005737\n",
              "3     P91124  cellular_component  GO:0005622\n",
              "4     P91124  cellular_component  GO:0043226"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set = pd.read_csv(data_dir+'train_set.tsv', delimiter='\\t')\n",
        "print(train_set.shape)\n",
        "train_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "tLgd0gVocxLr",
        "outputId": "3e8ded5a-5be7-419e-b99f-c258e2d1e6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(123969, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protein_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P91124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q55DL5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>O81027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q04418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q8IXT2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Protein_ID\n",
              "0     P91124\n",
              "1     Q55DL5\n",
              "2     O81027\n",
              "3     Q04418\n",
              "4     Q8IXT2"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ids_df = pd.read_csv(data_dir+'train_ids.txt',header = None)\n",
        "train_ids_df.columns = ['Protein_ID']\n",
        "print(train_ids_df.shape)\n",
        "train_ids_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "-ZflpY15iTpC",
        "outputId": "b7abddfc-e3ec-4b6e-8df8-71782d5191e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(123969, 1024)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column_1</th>\n",
              "      <th>Column_2</th>\n",
              "      <th>Column_3</th>\n",
              "      <th>Column_4</th>\n",
              "      <th>Column_5</th>\n",
              "      <th>Column_6</th>\n",
              "      <th>Column_7</th>\n",
              "      <th>Column_8</th>\n",
              "      <th>Column_9</th>\n",
              "      <th>Column_10</th>\n",
              "      <th>...</th>\n",
              "      <th>Column_1015</th>\n",
              "      <th>Column_1016</th>\n",
              "      <th>Column_1017</th>\n",
              "      <th>Column_1018</th>\n",
              "      <th>Column_1019</th>\n",
              "      <th>Column_1020</th>\n",
              "      <th>Column_1021</th>\n",
              "      <th>Column_1022</th>\n",
              "      <th>Column_1023</th>\n",
              "      <th>Column_1024</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.068176</td>\n",
              "      <td>-0.046478</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-0.008583</td>\n",
              "      <td>0.003763</td>\n",
              "      <td>0.046265</td>\n",
              "      <td>-0.059662</td>\n",
              "      <td>-0.050385</td>\n",
              "      <td>-0.005173</td>\n",
              "      <td>0.008865</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040771</td>\n",
              "      <td>-0.013138</td>\n",
              "      <td>-0.049591</td>\n",
              "      <td>-0.101074</td>\n",
              "      <td>0.066406</td>\n",
              "      <td>0.008980</td>\n",
              "      <td>-0.003506</td>\n",
              "      <td>-0.024612</td>\n",
              "      <td>0.034760</td>\n",
              "      <td>-0.031006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.016434</td>\n",
              "      <td>-0.001583</td>\n",
              "      <td>0.003889</td>\n",
              "      <td>0.073425</td>\n",
              "      <td>0.012428</td>\n",
              "      <td>0.028168</td>\n",
              "      <td>-0.040375</td>\n",
              "      <td>-0.093811</td>\n",
              "      <td>-0.017807</td>\n",
              "      <td>0.025497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011879</td>\n",
              "      <td>-0.033325</td>\n",
              "      <td>-0.031342</td>\n",
              "      <td>-0.005245</td>\n",
              "      <td>0.014732</td>\n",
              "      <td>0.081970</td>\n",
              "      <td>0.017456</td>\n",
              "      <td>-0.032959</td>\n",
              "      <td>0.053192</td>\n",
              "      <td>0.029907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.007904</td>\n",
              "      <td>0.087708</td>\n",
              "      <td>-0.001715</td>\n",
              "      <td>0.037659</td>\n",
              "      <td>0.017883</td>\n",
              "      <td>0.025589</td>\n",
              "      <td>-0.011749</td>\n",
              "      <td>-0.084717</td>\n",
              "      <td>-0.016266</td>\n",
              "      <td>-0.034973</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004829</td>\n",
              "      <td>-0.049713</td>\n",
              "      <td>-0.027176</td>\n",
              "      <td>-0.037415</td>\n",
              "      <td>-0.006241</td>\n",
              "      <td>-0.039703</td>\n",
              "      <td>0.001784</td>\n",
              "      <td>0.004719</td>\n",
              "      <td>-0.004288</td>\n",
              "      <td>0.001847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.002447</td>\n",
              "      <td>0.007053</td>\n",
              "      <td>0.064453</td>\n",
              "      <td>0.007271</td>\n",
              "      <td>-0.033569</td>\n",
              "      <td>-0.009933</td>\n",
              "      <td>-0.022186</td>\n",
              "      <td>-0.083862</td>\n",
              "      <td>-0.003841</td>\n",
              "      <td>-0.018631</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053589</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.016647</td>\n",
              "      <td>-0.069458</td>\n",
              "      <td>0.042206</td>\n",
              "      <td>-0.051758</td>\n",
              "      <td>-0.025436</td>\n",
              "      <td>0.057373</td>\n",
              "      <td>0.099121</td>\n",
              "      <td>0.032898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.049316</td>\n",
              "      <td>0.020691</td>\n",
              "      <td>0.108643</td>\n",
              "      <td>0.016342</td>\n",
              "      <td>-0.051056</td>\n",
              "      <td>-0.017334</td>\n",
              "      <td>-0.042084</td>\n",
              "      <td>-0.154053</td>\n",
              "      <td>0.007347</td>\n",
              "      <td>0.029907</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100647</td>\n",
              "      <td>-0.063293</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>-0.104675</td>\n",
              "      <td>-0.000757</td>\n",
              "      <td>-0.047485</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>-0.036774</td>\n",
              "      <td>0.103577</td>\n",
              "      <td>0.005245</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Column_1  Column_2  Column_3  Column_4  Column_5  Column_6  Column_7  \\\n",
              "0  0.068176 -0.046478  0.001752 -0.008583  0.003763  0.046265 -0.059662   \n",
              "1 -0.016434 -0.001583  0.003889  0.073425  0.012428  0.028168 -0.040375   \n",
              "2  0.007904  0.087708 -0.001715  0.037659  0.017883  0.025589 -0.011749   \n",
              "3  0.002447  0.007053  0.064453  0.007271 -0.033569 -0.009933 -0.022186   \n",
              "4  0.049316  0.020691  0.108643  0.016342 -0.051056 -0.017334 -0.042084   \n",
              "\n",
              "   Column_8  Column_9  Column_10  ...  Column_1015  Column_1016  Column_1017  \\\n",
              "0 -0.050385 -0.005173   0.008865  ...    -0.040771    -0.013138    -0.049591   \n",
              "1 -0.093811 -0.017807   0.025497  ...     0.011879    -0.033325    -0.031342   \n",
              "2 -0.084717 -0.016266  -0.034973  ...     0.004829    -0.049713    -0.027176   \n",
              "3 -0.083862 -0.003841  -0.018631  ...    -0.053589    -0.002508    -0.016647   \n",
              "4 -0.154053  0.007347   0.029907  ...    -0.100647    -0.063293     0.002346   \n",
              "\n",
              "   Column_1018  Column_1019  Column_1020  Column_1021  Column_1022  \\\n",
              "0    -0.101074     0.066406     0.008980    -0.003506    -0.024612   \n",
              "1    -0.005245     0.014732     0.081970     0.017456    -0.032959   \n",
              "2    -0.037415    -0.006241    -0.039703     0.001784     0.004719   \n",
              "3    -0.069458     0.042206    -0.051758    -0.025436     0.057373   \n",
              "4    -0.104675    -0.000757    -0.047485     0.003002    -0.036774   \n",
              "\n",
              "   Column_1023  Column_1024  \n",
              "0     0.034760    -0.031006  \n",
              "1     0.053192     0.029907  \n",
              "2    -0.004288     0.001847  \n",
              "3     0.099121     0.032898  \n",
              "4     0.103577     0.005245  \n",
              "\n",
              "[5 rows x 1024 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_embeddings = []\n",
        "protein_ids = []\n",
        "\n",
        "with h5py.File(data_dir+'train_embeddings.h5', 'r') as f:\n",
        "    for protein_id in f.keys():  # protein ids\n",
        "        embeddings = f[protein_id][:]\n",
        "        train_embeddings.append(embeddings)\n",
        "        protein_ids.append(protein_id)\n",
        "\n",
        "# Convert the list of embeddings to a numpy array\n",
        "prott5_embeddings = np.array(train_embeddings)\n",
        "\n",
        "# Create a DataFrame from the embeddings array\n",
        "column_num = prott5_embeddings.shape[1]\n",
        "train_df = pd.DataFrame(prott5_embeddings, columns=[\"Column_\" + str(i) for i in range(1, column_num + 1)])\n",
        "\n",
        "# Set protein_ids as the index of the DataFrame\n",
        "#train_df.index = protein_ids\n",
        "\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "SPlCh0tjGrxD",
        "outputId": "bd76348f-de75-4797-b696-f973549ac7fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "123969\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'A0A009IHW8'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(protein_ids))\n",
        "protein_ids[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "TNQgRF9Wip8z",
        "outputId": "85fe1a66-6a45-452b-bdd4-6b7259721eb6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protein_ID</th>\n",
              "      <th>Column_1</th>\n",
              "      <th>Column_2</th>\n",
              "      <th>Column_3</th>\n",
              "      <th>Column_4</th>\n",
              "      <th>Column_5</th>\n",
              "      <th>Column_6</th>\n",
              "      <th>Column_7</th>\n",
              "      <th>Column_8</th>\n",
              "      <th>Column_9</th>\n",
              "      <th>...</th>\n",
              "      <th>Column_1015</th>\n",
              "      <th>Column_1016</th>\n",
              "      <th>Column_1017</th>\n",
              "      <th>Column_1018</th>\n",
              "      <th>Column_1019</th>\n",
              "      <th>Column_1020</th>\n",
              "      <th>Column_1021</th>\n",
              "      <th>Column_1022</th>\n",
              "      <th>Column_1023</th>\n",
              "      <th>Column_1024</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A0A009IHW8</td>\n",
              "      <td>0.068176</td>\n",
              "      <td>-0.046478</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-0.008583</td>\n",
              "      <td>0.003763</td>\n",
              "      <td>0.046265</td>\n",
              "      <td>-0.059662</td>\n",
              "      <td>-0.050385</td>\n",
              "      <td>-0.005173</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040771</td>\n",
              "      <td>-0.013138</td>\n",
              "      <td>-0.049591</td>\n",
              "      <td>-0.101074</td>\n",
              "      <td>0.066406</td>\n",
              "      <td>0.008980</td>\n",
              "      <td>-0.003506</td>\n",
              "      <td>-0.024612</td>\n",
              "      <td>0.034760</td>\n",
              "      <td>-0.031006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A0A021WW32</td>\n",
              "      <td>-0.016434</td>\n",
              "      <td>-0.001583</td>\n",
              "      <td>0.003889</td>\n",
              "      <td>0.073425</td>\n",
              "      <td>0.012428</td>\n",
              "      <td>0.028168</td>\n",
              "      <td>-0.040375</td>\n",
              "      <td>-0.093811</td>\n",
              "      <td>-0.017807</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011879</td>\n",
              "      <td>-0.033325</td>\n",
              "      <td>-0.031342</td>\n",
              "      <td>-0.005245</td>\n",
              "      <td>0.014732</td>\n",
              "      <td>0.081970</td>\n",
              "      <td>0.017456</td>\n",
              "      <td>-0.032959</td>\n",
              "      <td>0.053192</td>\n",
              "      <td>0.029907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A0A021WZA4</td>\n",
              "      <td>0.007904</td>\n",
              "      <td>0.087708</td>\n",
              "      <td>-0.001715</td>\n",
              "      <td>0.037659</td>\n",
              "      <td>0.017883</td>\n",
              "      <td>0.025589</td>\n",
              "      <td>-0.011749</td>\n",
              "      <td>-0.084717</td>\n",
              "      <td>-0.016266</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004829</td>\n",
              "      <td>-0.049713</td>\n",
              "      <td>-0.027176</td>\n",
              "      <td>-0.037415</td>\n",
              "      <td>-0.006241</td>\n",
              "      <td>-0.039703</td>\n",
              "      <td>0.001784</td>\n",
              "      <td>0.004719</td>\n",
              "      <td>-0.004288</td>\n",
              "      <td>0.001847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A0A023FBW4</td>\n",
              "      <td>0.002447</td>\n",
              "      <td>0.007053</td>\n",
              "      <td>0.064453</td>\n",
              "      <td>0.007271</td>\n",
              "      <td>-0.033569</td>\n",
              "      <td>-0.009933</td>\n",
              "      <td>-0.022186</td>\n",
              "      <td>-0.083862</td>\n",
              "      <td>-0.003841</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053589</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.016647</td>\n",
              "      <td>-0.069458</td>\n",
              "      <td>0.042206</td>\n",
              "      <td>-0.051758</td>\n",
              "      <td>-0.025436</td>\n",
              "      <td>0.057373</td>\n",
              "      <td>0.099121</td>\n",
              "      <td>0.032898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A0A023FBW7</td>\n",
              "      <td>0.049316</td>\n",
              "      <td>0.020691</td>\n",
              "      <td>0.108643</td>\n",
              "      <td>0.016342</td>\n",
              "      <td>-0.051056</td>\n",
              "      <td>-0.017334</td>\n",
              "      <td>-0.042084</td>\n",
              "      <td>-0.154053</td>\n",
              "      <td>0.007347</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100647</td>\n",
              "      <td>-0.063293</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>-0.104675</td>\n",
              "      <td>-0.000757</td>\n",
              "      <td>-0.047485</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>-0.036774</td>\n",
              "      <td>0.103577</td>\n",
              "      <td>0.005245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123964</th>\n",
              "      <td>X6RLK1</td>\n",
              "      <td>0.052277</td>\n",
              "      <td>-0.062469</td>\n",
              "      <td>0.046478</td>\n",
              "      <td>0.046082</td>\n",
              "      <td>-0.041992</td>\n",
              "      <td>-0.009956</td>\n",
              "      <td>0.027161</td>\n",
              "      <td>-0.061401</td>\n",
              "      <td>0.038422</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036865</td>\n",
              "      <td>-0.017426</td>\n",
              "      <td>0.019196</td>\n",
              "      <td>-0.034882</td>\n",
              "      <td>-0.044739</td>\n",
              "      <td>0.024338</td>\n",
              "      <td>-0.055084</td>\n",
              "      <td>0.013901</td>\n",
              "      <td>0.059326</td>\n",
              "      <td>0.014725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123965</th>\n",
              "      <td>X6RLN4</td>\n",
              "      <td>-0.011299</td>\n",
              "      <td>-0.036957</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.031891</td>\n",
              "      <td>0.006027</td>\n",
              "      <td>0.079468</td>\n",
              "      <td>-0.014832</td>\n",
              "      <td>-0.047791</td>\n",
              "      <td>0.055023</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040680</td>\n",
              "      <td>-0.006996</td>\n",
              "      <td>0.018005</td>\n",
              "      <td>-0.024414</td>\n",
              "      <td>0.061401</td>\n",
              "      <td>0.041229</td>\n",
              "      <td>-0.021011</td>\n",
              "      <td>-0.014709</td>\n",
              "      <td>0.019791</td>\n",
              "      <td>0.052887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123966</th>\n",
              "      <td>X6RLP6</td>\n",
              "      <td>0.040405</td>\n",
              "      <td>-0.013908</td>\n",
              "      <td>0.025421</td>\n",
              "      <td>0.075012</td>\n",
              "      <td>-0.050293</td>\n",
              "      <td>0.058685</td>\n",
              "      <td>-0.032135</td>\n",
              "      <td>-0.073975</td>\n",
              "      <td>0.061798</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.066711</td>\n",
              "      <td>-0.011276</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>-0.014954</td>\n",
              "      <td>0.015190</td>\n",
              "      <td>0.010483</td>\n",
              "      <td>-0.010162</td>\n",
              "      <td>0.027557</td>\n",
              "      <td>0.027039</td>\n",
              "      <td>0.017517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123967</th>\n",
              "      <td>X6RLR1</td>\n",
              "      <td>-0.008362</td>\n",
              "      <td>-0.026291</td>\n",
              "      <td>0.037354</td>\n",
              "      <td>0.033264</td>\n",
              "      <td>-0.044861</td>\n",
              "      <td>-0.003189</td>\n",
              "      <td>-0.009598</td>\n",
              "      <td>-0.061432</td>\n",
              "      <td>0.027451</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037476</td>\n",
              "      <td>-0.020966</td>\n",
              "      <td>0.011360</td>\n",
              "      <td>0.027267</td>\n",
              "      <td>-0.006855</td>\n",
              "      <td>-0.005241</td>\n",
              "      <td>-0.057404</td>\n",
              "      <td>-0.012024</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>0.045898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123968</th>\n",
              "      <td>X6RM59</td>\n",
              "      <td>0.014755</td>\n",
              "      <td>0.088684</td>\n",
              "      <td>-0.011307</td>\n",
              "      <td>0.012398</td>\n",
              "      <td>0.001452</td>\n",
              "      <td>0.059967</td>\n",
              "      <td>-0.099915</td>\n",
              "      <td>-0.042175</td>\n",
              "      <td>0.059662</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063416</td>\n",
              "      <td>-0.009308</td>\n",
              "      <td>0.101318</td>\n",
              "      <td>-0.014511</td>\n",
              "      <td>0.032562</td>\n",
              "      <td>-0.042633</td>\n",
              "      <td>-0.009727</td>\n",
              "      <td>-0.068665</td>\n",
              "      <td>-0.048859</td>\n",
              "      <td>0.014969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>123969 rows × 1025 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Protein_ID  Column_1  Column_2  Column_3  Column_4  Column_5  \\\n",
              "0       A0A009IHW8  0.068176 -0.046478  0.001752 -0.008583  0.003763   \n",
              "1       A0A021WW32 -0.016434 -0.001583  0.003889  0.073425  0.012428   \n",
              "2       A0A021WZA4  0.007904  0.087708 -0.001715  0.037659  0.017883   \n",
              "3       A0A023FBW4  0.002447  0.007053  0.064453  0.007271 -0.033569   \n",
              "4       A0A023FBW7  0.049316  0.020691  0.108643  0.016342 -0.051056   \n",
              "...            ...       ...       ...       ...       ...       ...   \n",
              "123964      X6RLK1  0.052277 -0.062469  0.046478  0.046082 -0.041992   \n",
              "123965      X6RLN4 -0.011299 -0.036957  0.029297  0.031891  0.006027   \n",
              "123966      X6RLP6  0.040405 -0.013908  0.025421  0.075012 -0.050293   \n",
              "123967      X6RLR1 -0.008362 -0.026291  0.037354  0.033264 -0.044861   \n",
              "123968      X6RM59  0.014755  0.088684 -0.011307  0.012398  0.001452   \n",
              "\n",
              "        Column_6  Column_7  Column_8  Column_9  ...  Column_1015  Column_1016  \\\n",
              "0       0.046265 -0.059662 -0.050385 -0.005173  ...    -0.040771    -0.013138   \n",
              "1       0.028168 -0.040375 -0.093811 -0.017807  ...     0.011879    -0.033325   \n",
              "2       0.025589 -0.011749 -0.084717 -0.016266  ...     0.004829    -0.049713   \n",
              "3      -0.009933 -0.022186 -0.083862 -0.003841  ...    -0.053589    -0.002508   \n",
              "4      -0.017334 -0.042084 -0.154053  0.007347  ...    -0.100647    -0.063293   \n",
              "...          ...       ...       ...       ...  ...          ...          ...   \n",
              "123964 -0.009956  0.027161 -0.061401  0.038422  ...    -0.036865    -0.017426   \n",
              "123965  0.079468 -0.014832 -0.047791  0.055023  ...    -0.040680    -0.006996   \n",
              "123966  0.058685 -0.032135 -0.073975  0.061798  ...    -0.066711    -0.011276   \n",
              "123967 -0.003189 -0.009598 -0.061432  0.027451  ...    -0.037476    -0.020966   \n",
              "123968  0.059967 -0.099915 -0.042175  0.059662  ...    -0.063416    -0.009308   \n",
              "\n",
              "        Column_1017  Column_1018  Column_1019  Column_1020  Column_1021  \\\n",
              "0         -0.049591    -0.101074     0.066406     0.008980    -0.003506   \n",
              "1         -0.031342    -0.005245     0.014732     0.081970     0.017456   \n",
              "2         -0.027176    -0.037415    -0.006241    -0.039703     0.001784   \n",
              "3         -0.016647    -0.069458     0.042206    -0.051758    -0.025436   \n",
              "4          0.002346    -0.104675    -0.000757    -0.047485     0.003002   \n",
              "...             ...          ...          ...          ...          ...   \n",
              "123964     0.019196    -0.034882    -0.044739     0.024338    -0.055084   \n",
              "123965     0.018005    -0.024414     0.061401     0.041229    -0.021011   \n",
              "123966     0.002800    -0.014954     0.015190     0.010483    -0.010162   \n",
              "123967     0.011360     0.027267    -0.006855    -0.005241    -0.057404   \n",
              "123968     0.101318    -0.014511     0.032562    -0.042633    -0.009727   \n",
              "\n",
              "        Column_1022  Column_1023  Column_1024  \n",
              "0         -0.024612     0.034760    -0.031006  \n",
              "1         -0.032959     0.053192     0.029907  \n",
              "2          0.004719    -0.004288     0.001847  \n",
              "3          0.057373     0.099121     0.032898  \n",
              "4         -0.036774     0.103577     0.005245  \n",
              "...             ...          ...          ...  \n",
              "123964     0.013901     0.059326     0.014725  \n",
              "123965    -0.014709     0.019791     0.052887  \n",
              "123966     0.027557     0.027039     0.017517  \n",
              "123967    -0.012024     0.000682     0.045898  \n",
              "123968    -0.068665    -0.048859     0.014969  \n",
              "\n",
              "[123969 rows x 1025 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_embed_df = pd.concat((pd.DataFrame(protein_ids, columns=['Protein_ID']), train_df), axis=1)   ################################# Concatenating protein_ids instead of train_ids_df\n",
        "id_embed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7RwIyJdj_VAI"
      },
      "outputs": [],
      "source": [
        "prot_id_has_cc = train_set[train_set['aspect'] == 'cellular_component']['Protein_ID'].unique().tolist()\n",
        "cc_train = id_embed_df[id_embed_df['Protein_ID'].isin(prot_id_has_cc)]\n",
        "prot_id_has_mf = train_set[train_set['aspect'] == 'molecular_function']['Protein_ID'].unique().tolist() ###################### Other aspects:\n",
        "mf_train = id_embed_df[id_embed_df['Protein_ID'].isin(prot_id_has_mf)]                                  ######################\n",
        "prot_id_has_bp = train_set[train_set['aspect'] == 'biological_process']['Protein_ID'].unique().tolist() ######################\n",
        "bp_train = id_embed_df[id_embed_df['Protein_ID'].isin(prot_id_has_bp)]                                  ######################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ASdXmalpDYsA",
        "outputId": "ed8783d4-0cf9-4a7f-dedb-461ae57ff3ca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protein_ID</th>\n",
              "      <th>Column_1</th>\n",
              "      <th>Column_2</th>\n",
              "      <th>Column_3</th>\n",
              "      <th>Column_4</th>\n",
              "      <th>Column_5</th>\n",
              "      <th>Column_6</th>\n",
              "      <th>Column_7</th>\n",
              "      <th>Column_8</th>\n",
              "      <th>Column_9</th>\n",
              "      <th>...</th>\n",
              "      <th>Column_1015</th>\n",
              "      <th>Column_1016</th>\n",
              "      <th>Column_1017</th>\n",
              "      <th>Column_1018</th>\n",
              "      <th>Column_1019</th>\n",
              "      <th>Column_1020</th>\n",
              "      <th>Column_1021</th>\n",
              "      <th>Column_1022</th>\n",
              "      <th>Column_1023</th>\n",
              "      <th>Column_1024</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A0A021WW32</td>\n",
              "      <td>-0.016434</td>\n",
              "      <td>-0.001583</td>\n",
              "      <td>0.003889</td>\n",
              "      <td>0.073425</td>\n",
              "      <td>0.012428</td>\n",
              "      <td>0.028168</td>\n",
              "      <td>-0.040375</td>\n",
              "      <td>-0.093811</td>\n",
              "      <td>-0.017807</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011879</td>\n",
              "      <td>-0.033325</td>\n",
              "      <td>-0.031342</td>\n",
              "      <td>-0.005245</td>\n",
              "      <td>0.014732</td>\n",
              "      <td>0.081970</td>\n",
              "      <td>0.017456</td>\n",
              "      <td>-0.032959</td>\n",
              "      <td>0.053192</td>\n",
              "      <td>0.029907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A0A021WZA4</td>\n",
              "      <td>0.007904</td>\n",
              "      <td>0.087708</td>\n",
              "      <td>-0.001715</td>\n",
              "      <td>0.037659</td>\n",
              "      <td>0.017883</td>\n",
              "      <td>0.025589</td>\n",
              "      <td>-0.011749</td>\n",
              "      <td>-0.084717</td>\n",
              "      <td>-0.016266</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004829</td>\n",
              "      <td>-0.049713</td>\n",
              "      <td>-0.027176</td>\n",
              "      <td>-0.037415</td>\n",
              "      <td>-0.006241</td>\n",
              "      <td>-0.039703</td>\n",
              "      <td>0.001784</td>\n",
              "      <td>0.004719</td>\n",
              "      <td>-0.004288</td>\n",
              "      <td>0.001847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>A0A023GPJ3</td>\n",
              "      <td>0.015121</td>\n",
              "      <td>0.011017</td>\n",
              "      <td>0.021698</td>\n",
              "      <td>-0.025116</td>\n",
              "      <td>0.039612</td>\n",
              "      <td>0.008392</td>\n",
              "      <td>-0.051453</td>\n",
              "      <td>-0.025406</td>\n",
              "      <td>-0.020264</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027298</td>\n",
              "      <td>-0.029373</td>\n",
              "      <td>-0.051300</td>\n",
              "      <td>-0.062500</td>\n",
              "      <td>0.023376</td>\n",
              "      <td>0.066040</td>\n",
              "      <td>0.024963</td>\n",
              "      <td>-0.040497</td>\n",
              "      <td>0.026001</td>\n",
              "      <td>0.018082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>A0A023GUT0</td>\n",
              "      <td>-0.004139</td>\n",
              "      <td>-0.012878</td>\n",
              "      <td>0.071594</td>\n",
              "      <td>0.016052</td>\n",
              "      <td>-0.039825</td>\n",
              "      <td>0.022476</td>\n",
              "      <td>-0.110901</td>\n",
              "      <td>-0.085144</td>\n",
              "      <td>0.010040</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027267</td>\n",
              "      <td>-0.075562</td>\n",
              "      <td>0.030533</td>\n",
              "      <td>0.075623</td>\n",
              "      <td>-0.061554</td>\n",
              "      <td>-0.042664</td>\n",
              "      <td>0.042511</td>\n",
              "      <td>-0.025208</td>\n",
              "      <td>0.014473</td>\n",
              "      <td>0.057037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>A0A023IM54</td>\n",
              "      <td>-0.016510</td>\n",
              "      <td>0.025253</td>\n",
              "      <td>0.043335</td>\n",
              "      <td>0.015579</td>\n",
              "      <td>-0.016785</td>\n",
              "      <td>0.031143</td>\n",
              "      <td>-0.068115</td>\n",
              "      <td>-0.047333</td>\n",
              "      <td>0.101135</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.006344</td>\n",
              "      <td>-0.015419</td>\n",
              "      <td>0.016693</td>\n",
              "      <td>-0.050568</td>\n",
              "      <td>-0.024155</td>\n",
              "      <td>-0.000398</td>\n",
              "      <td>-0.006649</td>\n",
              "      <td>0.007561</td>\n",
              "      <td>0.051880</td>\n",
              "      <td>0.052826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123964</th>\n",
              "      <td>X6RLK1</td>\n",
              "      <td>0.052277</td>\n",
              "      <td>-0.062469</td>\n",
              "      <td>0.046478</td>\n",
              "      <td>0.046082</td>\n",
              "      <td>-0.041992</td>\n",
              "      <td>-0.009956</td>\n",
              "      <td>0.027161</td>\n",
              "      <td>-0.061401</td>\n",
              "      <td>0.038422</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036865</td>\n",
              "      <td>-0.017426</td>\n",
              "      <td>0.019196</td>\n",
              "      <td>-0.034882</td>\n",
              "      <td>-0.044739</td>\n",
              "      <td>0.024338</td>\n",
              "      <td>-0.055084</td>\n",
              "      <td>0.013901</td>\n",
              "      <td>0.059326</td>\n",
              "      <td>0.014725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123965</th>\n",
              "      <td>X6RLN4</td>\n",
              "      <td>-0.011299</td>\n",
              "      <td>-0.036957</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.031891</td>\n",
              "      <td>0.006027</td>\n",
              "      <td>0.079468</td>\n",
              "      <td>-0.014832</td>\n",
              "      <td>-0.047791</td>\n",
              "      <td>0.055023</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040680</td>\n",
              "      <td>-0.006996</td>\n",
              "      <td>0.018005</td>\n",
              "      <td>-0.024414</td>\n",
              "      <td>0.061401</td>\n",
              "      <td>0.041229</td>\n",
              "      <td>-0.021011</td>\n",
              "      <td>-0.014709</td>\n",
              "      <td>0.019791</td>\n",
              "      <td>0.052887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123966</th>\n",
              "      <td>X6RLP6</td>\n",
              "      <td>0.040405</td>\n",
              "      <td>-0.013908</td>\n",
              "      <td>0.025421</td>\n",
              "      <td>0.075012</td>\n",
              "      <td>-0.050293</td>\n",
              "      <td>0.058685</td>\n",
              "      <td>-0.032135</td>\n",
              "      <td>-0.073975</td>\n",
              "      <td>0.061798</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.066711</td>\n",
              "      <td>-0.011276</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>-0.014954</td>\n",
              "      <td>0.015190</td>\n",
              "      <td>0.010483</td>\n",
              "      <td>-0.010162</td>\n",
              "      <td>0.027557</td>\n",
              "      <td>0.027039</td>\n",
              "      <td>0.017517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123967</th>\n",
              "      <td>X6RLR1</td>\n",
              "      <td>-0.008362</td>\n",
              "      <td>-0.026291</td>\n",
              "      <td>0.037354</td>\n",
              "      <td>0.033264</td>\n",
              "      <td>-0.044861</td>\n",
              "      <td>-0.003189</td>\n",
              "      <td>-0.009598</td>\n",
              "      <td>-0.061432</td>\n",
              "      <td>0.027451</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037476</td>\n",
              "      <td>-0.020966</td>\n",
              "      <td>0.011360</td>\n",
              "      <td>0.027267</td>\n",
              "      <td>-0.006855</td>\n",
              "      <td>-0.005241</td>\n",
              "      <td>-0.057404</td>\n",
              "      <td>-0.012024</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>0.045898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123968</th>\n",
              "      <td>X6RM59</td>\n",
              "      <td>0.014755</td>\n",
              "      <td>0.088684</td>\n",
              "      <td>-0.011307</td>\n",
              "      <td>0.012398</td>\n",
              "      <td>0.001452</td>\n",
              "      <td>0.059967</td>\n",
              "      <td>-0.099915</td>\n",
              "      <td>-0.042175</td>\n",
              "      <td>0.059662</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063416</td>\n",
              "      <td>-0.009308</td>\n",
              "      <td>0.101318</td>\n",
              "      <td>-0.014511</td>\n",
              "      <td>0.032562</td>\n",
              "      <td>-0.042633</td>\n",
              "      <td>-0.009727</td>\n",
              "      <td>-0.068665</td>\n",
              "      <td>-0.048859</td>\n",
              "      <td>0.014969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84638 rows × 1025 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Protein_ID  Column_1  Column_2  Column_3  Column_4  Column_5  \\\n",
              "1       A0A021WW32 -0.016434 -0.001583  0.003889  0.073425  0.012428   \n",
              "2       A0A021WZA4  0.007904  0.087708 -0.001715  0.037659  0.017883   \n",
              "12      A0A023GPJ3  0.015121  0.011017  0.021698 -0.025116  0.039612   \n",
              "15      A0A023GUT0 -0.004139 -0.012878  0.071594  0.016052 -0.039825   \n",
              "17      A0A023IM54 -0.016510  0.025253  0.043335  0.015579 -0.016785   \n",
              "...            ...       ...       ...       ...       ...       ...   \n",
              "123964      X6RLK1  0.052277 -0.062469  0.046478  0.046082 -0.041992   \n",
              "123965      X6RLN4 -0.011299 -0.036957  0.029297  0.031891  0.006027   \n",
              "123966      X6RLP6  0.040405 -0.013908  0.025421  0.075012 -0.050293   \n",
              "123967      X6RLR1 -0.008362 -0.026291  0.037354  0.033264 -0.044861   \n",
              "123968      X6RM59  0.014755  0.088684 -0.011307  0.012398  0.001452   \n",
              "\n",
              "        Column_6  Column_7  Column_8  Column_9  ...  Column_1015  Column_1016  \\\n",
              "1       0.028168 -0.040375 -0.093811 -0.017807  ...     0.011879    -0.033325   \n",
              "2       0.025589 -0.011749 -0.084717 -0.016266  ...     0.004829    -0.049713   \n",
              "12      0.008392 -0.051453 -0.025406 -0.020264  ...    -0.027298    -0.029373   \n",
              "15      0.022476 -0.110901 -0.085144  0.010040  ...    -0.027267    -0.075562   \n",
              "17      0.031143 -0.068115 -0.047333  0.101135  ...    -0.006344    -0.015419   \n",
              "...          ...       ...       ...       ...  ...          ...          ...   \n",
              "123964 -0.009956  0.027161 -0.061401  0.038422  ...    -0.036865    -0.017426   \n",
              "123965  0.079468 -0.014832 -0.047791  0.055023  ...    -0.040680    -0.006996   \n",
              "123966  0.058685 -0.032135 -0.073975  0.061798  ...    -0.066711    -0.011276   \n",
              "123967 -0.003189 -0.009598 -0.061432  0.027451  ...    -0.037476    -0.020966   \n",
              "123968  0.059967 -0.099915 -0.042175  0.059662  ...    -0.063416    -0.009308   \n",
              "\n",
              "        Column_1017  Column_1018  Column_1019  Column_1020  Column_1021  \\\n",
              "1         -0.031342    -0.005245     0.014732     0.081970     0.017456   \n",
              "2         -0.027176    -0.037415    -0.006241    -0.039703     0.001784   \n",
              "12        -0.051300    -0.062500     0.023376     0.066040     0.024963   \n",
              "15         0.030533     0.075623    -0.061554    -0.042664     0.042511   \n",
              "17         0.016693    -0.050568    -0.024155    -0.000398    -0.006649   \n",
              "...             ...          ...          ...          ...          ...   \n",
              "123964     0.019196    -0.034882    -0.044739     0.024338    -0.055084   \n",
              "123965     0.018005    -0.024414     0.061401     0.041229    -0.021011   \n",
              "123966     0.002800    -0.014954     0.015190     0.010483    -0.010162   \n",
              "123967     0.011360     0.027267    -0.006855    -0.005241    -0.057404   \n",
              "123968     0.101318    -0.014511     0.032562    -0.042633    -0.009727   \n",
              "\n",
              "        Column_1022  Column_1023  Column_1024  \n",
              "1         -0.032959     0.053192     0.029907  \n",
              "2          0.004719    -0.004288     0.001847  \n",
              "12        -0.040497     0.026001     0.018082  \n",
              "15        -0.025208     0.014473     0.057037  \n",
              "17         0.007561     0.051880     0.052826  \n",
              "...             ...          ...          ...  \n",
              "123964     0.013901     0.059326     0.014725  \n",
              "123965    -0.014709     0.019791     0.052887  \n",
              "123966     0.027557     0.027039     0.017517  \n",
              "123967    -0.012024     0.000682     0.045898  \n",
              "123968    -0.068665    -0.048859     0.014969  \n",
              "\n",
              "[84638 rows x 1025 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cc_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "g1BixmEKDbn0",
        "outputId": "3bdd0b1c-423e-429c-b835-df612b692500"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protein_ID</th>\n",
              "      <th>Column_1</th>\n",
              "      <th>Column_2</th>\n",
              "      <th>Column_3</th>\n",
              "      <th>Column_4</th>\n",
              "      <th>Column_5</th>\n",
              "      <th>Column_6</th>\n",
              "      <th>Column_7</th>\n",
              "      <th>Column_8</th>\n",
              "      <th>Column_9</th>\n",
              "      <th>...</th>\n",
              "      <th>Column_1015</th>\n",
              "      <th>Column_1016</th>\n",
              "      <th>Column_1017</th>\n",
              "      <th>Column_1018</th>\n",
              "      <th>Column_1019</th>\n",
              "      <th>Column_1020</th>\n",
              "      <th>Column_1021</th>\n",
              "      <th>Column_1022</th>\n",
              "      <th>Column_1023</th>\n",
              "      <th>Column_1024</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A0A009IHW8</td>\n",
              "      <td>0.068176</td>\n",
              "      <td>-0.046478</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-0.008583</td>\n",
              "      <td>0.003763</td>\n",
              "      <td>0.046265</td>\n",
              "      <td>-0.059662</td>\n",
              "      <td>-0.050385</td>\n",
              "      <td>-0.005173</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040771</td>\n",
              "      <td>-0.013138</td>\n",
              "      <td>-0.049591</td>\n",
              "      <td>-0.101074</td>\n",
              "      <td>0.066406</td>\n",
              "      <td>0.008980</td>\n",
              "      <td>-0.003506</td>\n",
              "      <td>-0.024612</td>\n",
              "      <td>0.034760</td>\n",
              "      <td>-0.031006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A0A023FBW4</td>\n",
              "      <td>0.002447</td>\n",
              "      <td>0.007053</td>\n",
              "      <td>0.064453</td>\n",
              "      <td>0.007271</td>\n",
              "      <td>-0.033569</td>\n",
              "      <td>-0.009933</td>\n",
              "      <td>-0.022186</td>\n",
              "      <td>-0.083862</td>\n",
              "      <td>-0.003841</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053589</td>\n",
              "      <td>-0.002508</td>\n",
              "      <td>-0.016647</td>\n",
              "      <td>-0.069458</td>\n",
              "      <td>0.042206</td>\n",
              "      <td>-0.051758</td>\n",
              "      <td>-0.025436</td>\n",
              "      <td>0.057373</td>\n",
              "      <td>0.099121</td>\n",
              "      <td>0.032898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A0A023FBW7</td>\n",
              "      <td>0.049316</td>\n",
              "      <td>0.020691</td>\n",
              "      <td>0.108643</td>\n",
              "      <td>0.016342</td>\n",
              "      <td>-0.051056</td>\n",
              "      <td>-0.017334</td>\n",
              "      <td>-0.042084</td>\n",
              "      <td>-0.154053</td>\n",
              "      <td>0.007347</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100647</td>\n",
              "      <td>-0.063293</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>-0.104675</td>\n",
              "      <td>-0.000757</td>\n",
              "      <td>-0.047485</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>-0.036774</td>\n",
              "      <td>0.103577</td>\n",
              "      <td>0.005245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>A0A023FDY8</td>\n",
              "      <td>0.056488</td>\n",
              "      <td>0.019241</td>\n",
              "      <td>0.112122</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>-0.055939</td>\n",
              "      <td>-0.016129</td>\n",
              "      <td>-0.045105</td>\n",
              "      <td>-0.152466</td>\n",
              "      <td>0.003454</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.096985</td>\n",
              "      <td>-0.064880</td>\n",
              "      <td>0.009117</td>\n",
              "      <td>-0.106934</td>\n",
              "      <td>0.004780</td>\n",
              "      <td>-0.051544</td>\n",
              "      <td>0.001547</td>\n",
              "      <td>-0.038788</td>\n",
              "      <td>0.106018</td>\n",
              "      <td>0.013321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>A0A023FF81</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0.041138</td>\n",
              "      <td>0.098633</td>\n",
              "      <td>0.012909</td>\n",
              "      <td>-0.031494</td>\n",
              "      <td>-0.016129</td>\n",
              "      <td>-0.014793</td>\n",
              "      <td>-0.157837</td>\n",
              "      <td>-0.018585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.054810</td>\n",
              "      <td>0.015991</td>\n",
              "      <td>-0.014160</td>\n",
              "      <td>-0.086670</td>\n",
              "      <td>0.051880</td>\n",
              "      <td>-0.059387</td>\n",
              "      <td>-0.004559</td>\n",
              "      <td>0.048309</td>\n",
              "      <td>0.110474</td>\n",
              "      <td>0.027573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123918</th>\n",
              "      <td>X5KCU9</td>\n",
              "      <td>0.030869</td>\n",
              "      <td>-0.041443</td>\n",
              "      <td>-0.026169</td>\n",
              "      <td>0.019669</td>\n",
              "      <td>0.008804</td>\n",
              "      <td>0.005413</td>\n",
              "      <td>-0.055847</td>\n",
              "      <td>-0.071655</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053162</td>\n",
              "      <td>0.002834</td>\n",
              "      <td>-0.033478</td>\n",
              "      <td>-0.068848</td>\n",
              "      <td>0.033661</td>\n",
              "      <td>0.056885</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.009277</td>\n",
              "      <td>0.021133</td>\n",
              "      <td>0.016586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123919</th>\n",
              "      <td>X5KJC0</td>\n",
              "      <td>0.053864</td>\n",
              "      <td>0.097351</td>\n",
              "      <td>0.010437</td>\n",
              "      <td>0.018051</td>\n",
              "      <td>-0.049103</td>\n",
              "      <td>0.045563</td>\n",
              "      <td>-0.038574</td>\n",
              "      <td>-0.054688</td>\n",
              "      <td>0.066528</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008675</td>\n",
              "      <td>0.034363</td>\n",
              "      <td>0.033905</td>\n",
              "      <td>-0.014412</td>\n",
              "      <td>0.088074</td>\n",
              "      <td>-0.040558</td>\n",
              "      <td>0.013382</td>\n",
              "      <td>-0.019440</td>\n",
              "      <td>-0.031494</td>\n",
              "      <td>-0.006958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123920</th>\n",
              "      <td>X5L1L5</td>\n",
              "      <td>0.052826</td>\n",
              "      <td>0.097229</td>\n",
              "      <td>0.010933</td>\n",
              "      <td>0.022873</td>\n",
              "      <td>-0.047455</td>\n",
              "      <td>0.046021</td>\n",
              "      <td>-0.037720</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>0.068909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005947</td>\n",
              "      <td>0.033142</td>\n",
              "      <td>0.035278</td>\n",
              "      <td>-0.013283</td>\n",
              "      <td>0.085327</td>\n",
              "      <td>-0.039551</td>\n",
              "      <td>0.012573</td>\n",
              "      <td>-0.017914</td>\n",
              "      <td>-0.030838</td>\n",
              "      <td>-0.008064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123921</th>\n",
              "      <td>X5L565</td>\n",
              "      <td>0.053955</td>\n",
              "      <td>0.097717</td>\n",
              "      <td>0.010117</td>\n",
              "      <td>0.022079</td>\n",
              "      <td>-0.047607</td>\n",
              "      <td>0.045807</td>\n",
              "      <td>-0.037964</td>\n",
              "      <td>-0.053772</td>\n",
              "      <td>0.068237</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006489</td>\n",
              "      <td>0.033722</td>\n",
              "      <td>0.035767</td>\n",
              "      <td>-0.013329</td>\n",
              "      <td>0.088318</td>\n",
              "      <td>-0.039795</td>\n",
              "      <td>0.011810</td>\n",
              "      <td>-0.018219</td>\n",
              "      <td>-0.031830</td>\n",
              "      <td>-0.008003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123922</th>\n",
              "      <td>X5M5N0</td>\n",
              "      <td>0.015915</td>\n",
              "      <td>-0.009361</td>\n",
              "      <td>0.043060</td>\n",
              "      <td>-0.021835</td>\n",
              "      <td>0.018005</td>\n",
              "      <td>0.070496</td>\n",
              "      <td>-0.071533</td>\n",
              "      <td>-0.091553</td>\n",
              "      <td>-0.020630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.049805</td>\n",
              "      <td>0.019257</td>\n",
              "      <td>0.005798</td>\n",
              "      <td>-0.065430</td>\n",
              "      <td>0.087585</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>-0.058472</td>\n",
              "      <td>0.031113</td>\n",
              "      <td>0.069092</td>\n",
              "      <td>-0.017303</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>55698 rows × 1025 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Protein_ID  Column_1  Column_2  Column_3  Column_4  Column_5  \\\n",
              "0       A0A009IHW8  0.068176 -0.046478  0.001752 -0.008583  0.003763   \n",
              "3       A0A023FBW4  0.002447  0.007053  0.064453  0.007271 -0.033569   \n",
              "4       A0A023FBW7  0.049316  0.020691  0.108643  0.016342 -0.051056   \n",
              "5       A0A023FDY8  0.056488  0.019241  0.112122  0.019608 -0.055939   \n",
              "6       A0A023FF81 -0.000163  0.041138  0.098633  0.012909 -0.031494   \n",
              "...            ...       ...       ...       ...       ...       ...   \n",
              "123918      X5KCU9  0.030869 -0.041443 -0.026169  0.019669  0.008804   \n",
              "123919      X5KJC0  0.053864  0.097351  0.010437  0.018051 -0.049103   \n",
              "123920      X5L1L5  0.052826  0.097229  0.010933  0.022873 -0.047455   \n",
              "123921      X5L565  0.053955  0.097717  0.010117  0.022079 -0.047607   \n",
              "123922      X5M5N0  0.015915 -0.009361  0.043060 -0.021835  0.018005   \n",
              "\n",
              "        Column_6  Column_7  Column_8  Column_9  ...  Column_1015  Column_1016  \\\n",
              "0       0.046265 -0.059662 -0.050385 -0.005173  ...    -0.040771    -0.013138   \n",
              "3      -0.009933 -0.022186 -0.083862 -0.003841  ...    -0.053589    -0.002508   \n",
              "4      -0.017334 -0.042084 -0.154053  0.007347  ...    -0.100647    -0.063293   \n",
              "5      -0.016129 -0.045105 -0.152466  0.003454  ...    -0.096985    -0.064880   \n",
              "6      -0.016129 -0.014793 -0.157837 -0.018585  ...    -0.054810     0.015991   \n",
              "...          ...       ...       ...       ...  ...          ...          ...   \n",
              "123918  0.005413 -0.055847 -0.071655  0.000172  ...    -0.053162     0.002834   \n",
              "123919  0.045563 -0.038574 -0.054688  0.066528  ...     0.008675     0.034363   \n",
              "123920  0.046021 -0.037720 -0.054840  0.068909  ...     0.005947     0.033142   \n",
              "123921  0.045807 -0.037964 -0.053772  0.068237  ...     0.006489     0.033722   \n",
              "123922  0.070496 -0.071533 -0.091553 -0.020630  ...    -0.049805     0.019257   \n",
              "\n",
              "        Column_1017  Column_1018  Column_1019  Column_1020  Column_1021  \\\n",
              "0         -0.049591    -0.101074     0.066406     0.008980    -0.003506   \n",
              "3         -0.016647    -0.069458     0.042206    -0.051758    -0.025436   \n",
              "4          0.002346    -0.104675    -0.000757    -0.047485     0.003002   \n",
              "5          0.009117    -0.106934     0.004780    -0.051544     0.001547   \n",
              "6         -0.014160    -0.086670     0.051880    -0.059387    -0.004559   \n",
              "...             ...          ...          ...          ...          ...   \n",
              "123918    -0.033478    -0.068848     0.033661     0.056885     0.000184   \n",
              "123919     0.033905    -0.014412     0.088074    -0.040558     0.013382   \n",
              "123920     0.035278    -0.013283     0.085327    -0.039551     0.012573   \n",
              "123921     0.035767    -0.013329     0.088318    -0.039795     0.011810   \n",
              "123922     0.005798    -0.065430     0.087585     0.005543    -0.058472   \n",
              "\n",
              "        Column_1022  Column_1023  Column_1024  \n",
              "0         -0.024612     0.034760    -0.031006  \n",
              "3          0.057373     0.099121     0.032898  \n",
              "4         -0.036774     0.103577     0.005245  \n",
              "5         -0.038788     0.106018     0.013321  \n",
              "6          0.048309     0.110474     0.027573  \n",
              "...             ...          ...          ...  \n",
              "123918     0.009277     0.021133     0.016586  \n",
              "123919    -0.019440    -0.031494    -0.006958  \n",
              "123920    -0.017914    -0.030838    -0.008064  \n",
              "123921    -0.018219    -0.031830    -0.008003  \n",
              "123922     0.031113     0.069092    -0.017303  \n",
              "\n",
              "[55698 rows x 1025 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mf_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "z_e_veWEDdco",
        "outputId": "1b2c969e-d0cd-47d0-95ef-6bf7c2f993a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protein_ID</th>\n",
              "      <th>Column_1</th>\n",
              "      <th>Column_2</th>\n",
              "      <th>Column_3</th>\n",
              "      <th>Column_4</th>\n",
              "      <th>Column_5</th>\n",
              "      <th>Column_6</th>\n",
              "      <th>Column_7</th>\n",
              "      <th>Column_8</th>\n",
              "      <th>Column_9</th>\n",
              "      <th>...</th>\n",
              "      <th>Column_1015</th>\n",
              "      <th>Column_1016</th>\n",
              "      <th>Column_1017</th>\n",
              "      <th>Column_1018</th>\n",
              "      <th>Column_1019</th>\n",
              "      <th>Column_1020</th>\n",
              "      <th>Column_1021</th>\n",
              "      <th>Column_1022</th>\n",
              "      <th>Column_1023</th>\n",
              "      <th>Column_1024</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A0A009IHW8</td>\n",
              "      <td>0.068176</td>\n",
              "      <td>-0.046478</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-0.008583</td>\n",
              "      <td>0.003763</td>\n",
              "      <td>0.046265</td>\n",
              "      <td>-0.059662</td>\n",
              "      <td>-0.050385</td>\n",
              "      <td>-0.005173</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040771</td>\n",
              "      <td>-0.013138</td>\n",
              "      <td>-0.049591</td>\n",
              "      <td>-0.101074</td>\n",
              "      <td>0.066406</td>\n",
              "      <td>0.008980</td>\n",
              "      <td>-0.003506</td>\n",
              "      <td>-0.024612</td>\n",
              "      <td>0.034760</td>\n",
              "      <td>-0.031006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A0A021WW32</td>\n",
              "      <td>-0.016434</td>\n",
              "      <td>-0.001583</td>\n",
              "      <td>0.003889</td>\n",
              "      <td>0.073425</td>\n",
              "      <td>0.012428</td>\n",
              "      <td>0.028168</td>\n",
              "      <td>-0.040375</td>\n",
              "      <td>-0.093811</td>\n",
              "      <td>-0.017807</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011879</td>\n",
              "      <td>-0.033325</td>\n",
              "      <td>-0.031342</td>\n",
              "      <td>-0.005245</td>\n",
              "      <td>0.014732</td>\n",
              "      <td>0.081970</td>\n",
              "      <td>0.017456</td>\n",
              "      <td>-0.032959</td>\n",
              "      <td>0.053192</td>\n",
              "      <td>0.029907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>A0A023FFD0</td>\n",
              "      <td>0.021286</td>\n",
              "      <td>0.043488</td>\n",
              "      <td>0.130493</td>\n",
              "      <td>-0.020630</td>\n",
              "      <td>-0.053345</td>\n",
              "      <td>-0.022629</td>\n",
              "      <td>-0.033478</td>\n",
              "      <td>-0.135376</td>\n",
              "      <td>0.018372</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.065613</td>\n",
              "      <td>-0.061371</td>\n",
              "      <td>0.019577</td>\n",
              "      <td>-0.103210</td>\n",
              "      <td>0.091614</td>\n",
              "      <td>-0.088135</td>\n",
              "      <td>-0.035736</td>\n",
              "      <td>-0.008224</td>\n",
              "      <td>0.113586</td>\n",
              "      <td>0.057220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>A0A023GPJ3</td>\n",
              "      <td>0.015121</td>\n",
              "      <td>0.011017</td>\n",
              "      <td>0.021698</td>\n",
              "      <td>-0.025116</td>\n",
              "      <td>0.039612</td>\n",
              "      <td>0.008392</td>\n",
              "      <td>-0.051453</td>\n",
              "      <td>-0.025406</td>\n",
              "      <td>-0.020264</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027298</td>\n",
              "      <td>-0.029373</td>\n",
              "      <td>-0.051300</td>\n",
              "      <td>-0.062500</td>\n",
              "      <td>0.023376</td>\n",
              "      <td>0.066040</td>\n",
              "      <td>0.024963</td>\n",
              "      <td>-0.040497</td>\n",
              "      <td>0.026001</td>\n",
              "      <td>0.018082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>A0A023GPK8</td>\n",
              "      <td>-0.006481</td>\n",
              "      <td>0.010765</td>\n",
              "      <td>-0.005814</td>\n",
              "      <td>0.027664</td>\n",
              "      <td>0.030334</td>\n",
              "      <td>0.023651</td>\n",
              "      <td>-0.018524</td>\n",
              "      <td>-0.036652</td>\n",
              "      <td>0.031708</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020264</td>\n",
              "      <td>0.021179</td>\n",
              "      <td>-0.001020</td>\n",
              "      <td>-0.069702</td>\n",
              "      <td>0.036530</td>\n",
              "      <td>0.028183</td>\n",
              "      <td>-0.004238</td>\n",
              "      <td>-0.016342</td>\n",
              "      <td>0.045044</td>\n",
              "      <td>0.017700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123918</th>\n",
              "      <td>X5KCU9</td>\n",
              "      <td>0.030869</td>\n",
              "      <td>-0.041443</td>\n",
              "      <td>-0.026169</td>\n",
              "      <td>0.019669</td>\n",
              "      <td>0.008804</td>\n",
              "      <td>0.005413</td>\n",
              "      <td>-0.055847</td>\n",
              "      <td>-0.071655</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053162</td>\n",
              "      <td>0.002834</td>\n",
              "      <td>-0.033478</td>\n",
              "      <td>-0.068848</td>\n",
              "      <td>0.033661</td>\n",
              "      <td>0.056885</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.009277</td>\n",
              "      <td>0.021133</td>\n",
              "      <td>0.016586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123919</th>\n",
              "      <td>X5KJC0</td>\n",
              "      <td>0.053864</td>\n",
              "      <td>0.097351</td>\n",
              "      <td>0.010437</td>\n",
              "      <td>0.018051</td>\n",
              "      <td>-0.049103</td>\n",
              "      <td>0.045563</td>\n",
              "      <td>-0.038574</td>\n",
              "      <td>-0.054688</td>\n",
              "      <td>0.066528</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008675</td>\n",
              "      <td>0.034363</td>\n",
              "      <td>0.033905</td>\n",
              "      <td>-0.014412</td>\n",
              "      <td>0.088074</td>\n",
              "      <td>-0.040558</td>\n",
              "      <td>0.013382</td>\n",
              "      <td>-0.019440</td>\n",
              "      <td>-0.031494</td>\n",
              "      <td>-0.006958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123920</th>\n",
              "      <td>X5L1L5</td>\n",
              "      <td>0.052826</td>\n",
              "      <td>0.097229</td>\n",
              "      <td>0.010933</td>\n",
              "      <td>0.022873</td>\n",
              "      <td>-0.047455</td>\n",
              "      <td>0.046021</td>\n",
              "      <td>-0.037720</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>0.068909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005947</td>\n",
              "      <td>0.033142</td>\n",
              "      <td>0.035278</td>\n",
              "      <td>-0.013283</td>\n",
              "      <td>0.085327</td>\n",
              "      <td>-0.039551</td>\n",
              "      <td>0.012573</td>\n",
              "      <td>-0.017914</td>\n",
              "      <td>-0.030838</td>\n",
              "      <td>-0.008064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123921</th>\n",
              "      <td>X5L565</td>\n",
              "      <td>0.053955</td>\n",
              "      <td>0.097717</td>\n",
              "      <td>0.010117</td>\n",
              "      <td>0.022079</td>\n",
              "      <td>-0.047607</td>\n",
              "      <td>0.045807</td>\n",
              "      <td>-0.037964</td>\n",
              "      <td>-0.053772</td>\n",
              "      <td>0.068237</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006489</td>\n",
              "      <td>0.033722</td>\n",
              "      <td>0.035767</td>\n",
              "      <td>-0.013329</td>\n",
              "      <td>0.088318</td>\n",
              "      <td>-0.039795</td>\n",
              "      <td>0.011810</td>\n",
              "      <td>-0.018219</td>\n",
              "      <td>-0.031830</td>\n",
              "      <td>-0.008003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123922</th>\n",
              "      <td>X5M5N0</td>\n",
              "      <td>0.015915</td>\n",
              "      <td>-0.009361</td>\n",
              "      <td>0.043060</td>\n",
              "      <td>-0.021835</td>\n",
              "      <td>0.018005</td>\n",
              "      <td>0.070496</td>\n",
              "      <td>-0.071533</td>\n",
              "      <td>-0.091553</td>\n",
              "      <td>-0.020630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.049805</td>\n",
              "      <td>0.019257</td>\n",
              "      <td>0.005798</td>\n",
              "      <td>-0.065430</td>\n",
              "      <td>0.087585</td>\n",
              "      <td>0.005543</td>\n",
              "      <td>-0.058472</td>\n",
              "      <td>0.031113</td>\n",
              "      <td>0.069092</td>\n",
              "      <td>-0.017303</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>83064 rows × 1025 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Protein_ID  Column_1  Column_2  Column_3  Column_4  Column_5  \\\n",
              "0       A0A009IHW8  0.068176 -0.046478  0.001752 -0.008583  0.003763   \n",
              "1       A0A021WW32 -0.016434 -0.001583  0.003889  0.073425  0.012428   \n",
              "8       A0A023FFD0  0.021286  0.043488  0.130493 -0.020630 -0.053345   \n",
              "12      A0A023GPJ3  0.015121  0.011017  0.021698 -0.025116  0.039612   \n",
              "13      A0A023GPK8 -0.006481  0.010765 -0.005814  0.027664  0.030334   \n",
              "...            ...       ...       ...       ...       ...       ...   \n",
              "123918      X5KCU9  0.030869 -0.041443 -0.026169  0.019669  0.008804   \n",
              "123919      X5KJC0  0.053864  0.097351  0.010437  0.018051 -0.049103   \n",
              "123920      X5L1L5  0.052826  0.097229  0.010933  0.022873 -0.047455   \n",
              "123921      X5L565  0.053955  0.097717  0.010117  0.022079 -0.047607   \n",
              "123922      X5M5N0  0.015915 -0.009361  0.043060 -0.021835  0.018005   \n",
              "\n",
              "        Column_6  Column_7  Column_8  Column_9  ...  Column_1015  Column_1016  \\\n",
              "0       0.046265 -0.059662 -0.050385 -0.005173  ...    -0.040771    -0.013138   \n",
              "1       0.028168 -0.040375 -0.093811 -0.017807  ...     0.011879    -0.033325   \n",
              "8      -0.022629 -0.033478 -0.135376  0.018372  ...    -0.065613    -0.061371   \n",
              "12      0.008392 -0.051453 -0.025406 -0.020264  ...    -0.027298    -0.029373   \n",
              "13      0.023651 -0.018524 -0.036652  0.031708  ...     0.020264     0.021179   \n",
              "...          ...       ...       ...       ...  ...          ...          ...   \n",
              "123918  0.005413 -0.055847 -0.071655  0.000172  ...    -0.053162     0.002834   \n",
              "123919  0.045563 -0.038574 -0.054688  0.066528  ...     0.008675     0.034363   \n",
              "123920  0.046021 -0.037720 -0.054840  0.068909  ...     0.005947     0.033142   \n",
              "123921  0.045807 -0.037964 -0.053772  0.068237  ...     0.006489     0.033722   \n",
              "123922  0.070496 -0.071533 -0.091553 -0.020630  ...    -0.049805     0.019257   \n",
              "\n",
              "        Column_1017  Column_1018  Column_1019  Column_1020  Column_1021  \\\n",
              "0         -0.049591    -0.101074     0.066406     0.008980    -0.003506   \n",
              "1         -0.031342    -0.005245     0.014732     0.081970     0.017456   \n",
              "8          0.019577    -0.103210     0.091614    -0.088135    -0.035736   \n",
              "12        -0.051300    -0.062500     0.023376     0.066040     0.024963   \n",
              "13        -0.001020    -0.069702     0.036530     0.028183    -0.004238   \n",
              "...             ...          ...          ...          ...          ...   \n",
              "123918    -0.033478    -0.068848     0.033661     0.056885     0.000184   \n",
              "123919     0.033905    -0.014412     0.088074    -0.040558     0.013382   \n",
              "123920     0.035278    -0.013283     0.085327    -0.039551     0.012573   \n",
              "123921     0.035767    -0.013329     0.088318    -0.039795     0.011810   \n",
              "123922     0.005798    -0.065430     0.087585     0.005543    -0.058472   \n",
              "\n",
              "        Column_1022  Column_1023  Column_1024  \n",
              "0         -0.024612     0.034760    -0.031006  \n",
              "1         -0.032959     0.053192     0.029907  \n",
              "8         -0.008224     0.113586     0.057220  \n",
              "12        -0.040497     0.026001     0.018082  \n",
              "13        -0.016342     0.045044     0.017700  \n",
              "...             ...          ...          ...  \n",
              "123918     0.009277     0.021133     0.016586  \n",
              "123919    -0.019440    -0.031494    -0.006958  \n",
              "123920    -0.017914    -0.030838    -0.008064  \n",
              "123921    -0.018219    -0.031830    -0.008003  \n",
              "123922     0.031113     0.069092    -0.017303  \n",
              "\n",
              "[83064 rows x 1025 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bp_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protein_ID</th>\n",
              "      <th>IPR_ID</th>\n",
              "      <th>desc</th>\n",
              "      <th>db</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A0A009IHW8</td>\n",
              "      <td>IPR000157</td>\n",
              "      <td>Toll/interleukin-1 receptor homology (TIR) domain</td>\n",
              "      <td>PF13676</td>\n",
              "      <td>138</td>\n",
              "      <td>231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A0A009IHW8</td>\n",
              "      <td>IPR000157</td>\n",
              "      <td>Toll/interleukin-1 receptor homology (TIR) domain</td>\n",
              "      <td>PS50104</td>\n",
              "      <td>133</td>\n",
              "      <td>266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A0A009IHW8</td>\n",
              "      <td>IPR000157</td>\n",
              "      <td>Toll/interleukin-1 receptor homology (TIR) domain</td>\n",
              "      <td>SM00255</td>\n",
              "      <td>134</td>\n",
              "      <td>258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A0A009IHW8</td>\n",
              "      <td>IPR035897</td>\n",
              "      <td>Toll/interleukin-1 receptor homology (TIR) dom...</td>\n",
              "      <td>G3DSA:3.40.50.10140</td>\n",
              "      <td>80</td>\n",
              "      <td>266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A0A009IHW8</td>\n",
              "      <td>IPR035897</td>\n",
              "      <td>Toll/interleukin-1 receptor homology (TIR) dom...</td>\n",
              "      <td>SSF52200</td>\n",
              "      <td>128</td>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103541</th>\n",
              "      <td>X6RM59</td>\n",
              "      <td>IPR006434</td>\n",
              "      <td>Pyrimidine 5'-nucleotidase, eukaryotic</td>\n",
              "      <td>SFLDG01128</td>\n",
              "      <td>42</td>\n",
              "      <td>331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103542</th>\n",
              "      <td>X6RM59</td>\n",
              "      <td>IPR006434</td>\n",
              "      <td>Pyrimidine 5'-nucleotidase, eukaryotic</td>\n",
              "      <td>TIGR01544</td>\n",
              "      <td>50</td>\n",
              "      <td>330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103543</th>\n",
              "      <td>X6RM59</td>\n",
              "      <td>IPR006434</td>\n",
              "      <td>Pyrimidine 5'-nucleotidase, eukaryotic</td>\n",
              "      <td>cd07504</td>\n",
              "      <td>59</td>\n",
              "      <td>331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103544</th>\n",
              "      <td>X6RM59</td>\n",
              "      <td>IPR023214</td>\n",
              "      <td>HAD superfamily</td>\n",
              "      <td>G3DSA:3.40.50.1000</td>\n",
              "      <td>70</td>\n",
              "      <td>327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103545</th>\n",
              "      <td>X6RM59</td>\n",
              "      <td>IPR036412</td>\n",
              "      <td>HAD-like superfamily</td>\n",
              "      <td>SSF56784</td>\n",
              "      <td>42</td>\n",
              "      <td>331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1103546 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Protein_ID     IPR_ID  \\\n",
              "0        A0A009IHW8  IPR000157   \n",
              "1        A0A009IHW8  IPR000157   \n",
              "2        A0A009IHW8  IPR000157   \n",
              "3        A0A009IHW8  IPR035897   \n",
              "4        A0A009IHW8  IPR035897   \n",
              "...             ...        ...   \n",
              "1103541      X6RM59  IPR006434   \n",
              "1103542      X6RM59  IPR006434   \n",
              "1103543      X6RM59  IPR006434   \n",
              "1103544      X6RM59  IPR023214   \n",
              "1103545      X6RM59  IPR036412   \n",
              "\n",
              "                                                      desc  \\\n",
              "0        Toll/interleukin-1 receptor homology (TIR) domain   \n",
              "1        Toll/interleukin-1 receptor homology (TIR) domain   \n",
              "2        Toll/interleukin-1 receptor homology (TIR) domain   \n",
              "3        Toll/interleukin-1 receptor homology (TIR) dom...   \n",
              "4        Toll/interleukin-1 receptor homology (TIR) dom...   \n",
              "...                                                    ...   \n",
              "1103541             Pyrimidine 5'-nucleotidase, eukaryotic   \n",
              "1103542             Pyrimidine 5'-nucleotidase, eukaryotic   \n",
              "1103543             Pyrimidine 5'-nucleotidase, eukaryotic   \n",
              "1103544                                    HAD superfamily   \n",
              "1103545                               HAD-like superfamily   \n",
              "\n",
              "                          db  start  end  \n",
              "0                    PF13676    138  231  \n",
              "1                    PS50104    133  266  \n",
              "2                    SM00255    134  258  \n",
              "3        G3DSA:3.40.50.10140     80  266  \n",
              "4                   SSF52200    128  249  \n",
              "...                      ...    ...  ...  \n",
              "1103541           SFLDG01128     42  331  \n",
              "1103542            TIGR01544     50  330  \n",
              "1103543              cd07504     59  331  \n",
              "1103544   G3DSA:3.40.50.1000     70  327  \n",
              "1103545             SSF56784     42  331  \n",
              "\n",
              "[1103546 rows x 6 columns]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_protein2ipr = pd.read_csv(data_dir+'train_protein2ipr.dat',header = None, sep='\\t')\n",
        "train_protein2ipr.columns = ['Protein_ID','IPR_ID','desc','db','start','end']\n",
        "\n",
        "train_protein2ipr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UQERKmbj2vR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIWvrm-q_7dZ",
        "outputId": "7e4fbb17-4169-4d89-808a-d37cfff98755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['GO:0005575', 'GO:0110165', 'GO:0008150', 'GO:0005622', 'GO:0043226', 'GO:0009987', 'GO:0003674', 'GO:0043229', 'GO:0043227', 'GO:0005737', 'GO:0043231', 'GO:0065007', 'GO:0005488', 'GO:0050789', 'GO:0050794', 'GO:0008152', 'GO:0050896', 'GO:0005515', 'GO:0005634', 'GO:0071704', 'GO:0003824', 'GO:0032501', 'GO:0016020', 'GO:0044237', 'GO:0032502', 'GO:0044238', 'GO:0048856', 'GO:0006807', 'GO:0071944', 'GO:0019222', 'GO:0071840', 'GO:0005829', 'GO:0007275', 'GO:0048518', 'GO:0016043', 'GO:0031323', 'GO:0032991', 'GO:0043170', 'GO:0060255', 'GO:0051716', 'GO:0005886', 'GO:0043228', 'GO:0043232', 'GO:0031974', 'GO:0043233', 'GO:0070013', 'GO:0048522', 'GO:0009889', 'GO:0009058', 'GO:0048519', 'GO:0031326', 'GO:0006950', 'GO:1901576', 'GO:0010556', 'GO:0080090', 'GO:0048731', 'GO:0031981', 'GO:0010468', 'GO:0044249', 'GO:1901564', 'GO:0042221', 'GO:0051171', 'GO:0051179', 'GO:0048523', 'GO:0012505', 'GO:0097159', 'GO:1901360', 'GO:0051234', 'GO:0034641', 'GO:0048869', 'GO:0030154', 'GO:0005654', 'GO:0007154', 'GO:0006725', 'GO:0006810', 'GO:0046483', 'GO:0009653', 'GO:0003676', 'GO:0019219', 'GO:0009605', 'GO:0016740', 'GO:0048513', 'GO:0048583', 'GO:0006996', 'GO:0010033', 'GO:0009893', 'GO:0023052', 'GO:0051252', 'GO:0005739', 'GO:0006139', 'GO:0031325', 'GO:0019538', 'GO:0009059', 'GO:2001141', 'GO:0006355', 'GO:0010604', 'GO:0016787', 'GO:0007165', 'GO:0000003', 'GO:0048468', 'GO:0031090', 'GO:0022414', 'GO:0010646', 'GO:0023051', 'GO:0044085', 'GO:0044281', 'GO:0051239', 'GO:0065008', 'GO:0090304', 'GO:0050793', 'GO:0031982', 'GO:0009892', 'GO:0140096', 'GO:0044419', 'GO:0070887', 'GO:0051641', 'GO:0009891', 'GO:0051173', 'GO:0031328', 'GO:1901700', 'GO:0009628', 'GO:0005576', 'GO:0031324', 'GO:0042995', 'GO:0010605', 'GO:0010557', 'GO:0120025', 'GO:0007399', 'GO:0010467', 'GO:0022607', 'GO:0033554', 'GO:0009607', 'GO:0009888', 'GO:0009966', 'GO:0051128', 'GO:0009056', 'GO:0003677', 'GO:0043207', 'GO:0030054', 'GO:0051707', 'GO:0042802', 'GO:0009890', 'GO:0005856', 'GO:0031327', 'GO:1902494', 'GO:0003006', 'GO:0010558', 'GO:0005783', 'GO:0043167', 'GO:1901575', 'GO:0071702', 'GO:0033036', 'GO:0097708', 'GO:0031410', 'GO:1901566', 'GO:0005615', 'GO:0043412', 'GO:0051172', 'GO:0009719', 'GO:0045935', 'GO:0032879', 'GO:0031975', 'GO:0008283', 'GO:0044271', 'GO:0048584', 'GO:0016070', 'GO:0005215', 'GO:0009790', 'GO:0016491', 'GO:0042127', 'GO:0006629', 'GO:0006357', 'GO:0051649', 'GO:0071310', 'GO:0006952', 'GO:0051254', 'GO:0031967', 'GO:0043933', 'GO:0022857', 'GO:0043565', 'GO:0098588', 'GO:0006082', 'GO:0019953', 'GO:0022008', 'GO:0043436', 'GO:0006793', 'GO:0070727', 'GO:0036211', 'GO:1901362', 'GO:0016772', 'GO:0006796', 'GO:0019752', 'GO:0051246', 'GO:0003723', 'GO:0005694', 'GO:0008104', 'GO:1902680', 'GO:0045893', 'GO:0032504', 'GO:0005794', 'GO:0042592', 'GO:0051049', 'GO:0098772', 'GO:0002376', 'GO:0071705', 'GO:0140513', 'GO:0003008', 'GO:0048699', 'GO:0007049', 'GO:0019899', 'GO:0009887', 'GO:0048646', 'GO:0003690', 'GO:0044255', 'GO:0098542', 'GO:0030182', 'GO:0044877', 'GO:0015630', 'GO:0048585', 'GO:0060429', 'GO:0009791', 'GO:0044248', 'GO:0030030', 'GO:0022402', 'GO:0051240', 'GO:0080134', 'GO:0010647', 'GO:0023056', 'GO:0045934', 'GO:2000026', 'GO:0018130', 'GO:0140110', 'GO:0009725', 'GO:0048609', 'GO:0019438', 'GO:0120036', 'GO:0045595', 'GO:0009536', 'GO:0007010', 'GO:0030312', 'GO:1990837', 'GO:0046907', 'GO:0016301', 'GO:0033993', 'GO:1901135', 'GO:1901701', 'GO:0099080', 'GO:0051253', 'GO:1902531', 'GO:0045202', 'GO:0036094', 'GO:0005730', 'GO:0005773', 'GO:0016788', 'GO:0051094', 'GO:0098796', 'GO:0071495', 'GO:0043169', 'GO:0043067', 'GO:0007166', 'GO:1902679', 'GO:0045892', 'GO:0007276', 'GO:0010648', 'GO:0023057', 'GO:0009967', 'GO:0009617', 'GO:0140535', 'GO:1901698', 'GO:0002682', 'GO:0043005', 'GO:0043168', 'GO:0016773', 'GO:0048666', 'GO:0098590', 'GO:0005929', 'GO:0001067', 'GO:1901565', 'GO:0014070', 'GO:0016192', 'GO:0000976', 'GO:0046872', 'GO:0000902', 'GO:0005102', 'GO:0010629', 'GO:0006259', 'GO:0042981', 'GO:0045944', 'GO:0032101', 'GO:0065009', 'GO:0009991', 'GO:0010628', 'GO:0010035', 'GO:0048870', 'GO:1990904', 'GO:0009057', 'GO:0008610', 'GO:0065003', 'GO:0055085', 'GO:0040007', 'GO:0022412', 'GO:0007417', 'GO:0033043', 'GO:0010243', 'GO:0009968', 'GO:0003700', 'GO:0035556', 'GO:0019637', 'GO:0060089', 'GO:0035295', 'GO:0044283', 'GO:0007610', 'GO:0006396', 'GO:0051726', 'GO:0032993', 'GO:0070161', 'GO:0031984', 'GO:0051130', 'GO:0030234', 'GO:0038023', 'GO:0048878', 'GO:0008284', 'GO:0015075', 'GO:0043603', 'GO:0032989', 'GO:0031667', 'GO:0007423', 'GO:1901363', 'GO:0072359', 'GO:0061458', 'GO:0006974', 'GO:1990234', 'GO:0048608', 'GO:0007389', 'GO:0015318', 'GO:0034654', 'GO:0050877', 'GO:0051241', 'GO:0046983', 'GO:0043230', 'GO:0065010', 'GO:0045184', 'GO:0000785', 'GO:0040008', 'GO:0004672', 'GO:0044087', 'GO:0007017', 'GO:0009792', 'GO:0140640', 'GO:1903561', 'GO:0097367', 'GO:0009507', 'GO:0003002', 'GO:0048598', 'GO:0006955', 'GO:0016477', 'GO:0040012', 'GO:0016746', 'GO:0031175', 'GO:0005768', 'GO:0006508', 'GO:0070925', 'GO:0048729', 'GO:0032787', 'GO:0000166', 'GO:1901265', 'GO:0051247', 'GO:0005815', 'GO:0005740', 'GO:0070062', 'GO:0060341', 'GO:0000278', 'GO:0009314', 'GO:0051093', 'GO:0006811', 'GO:0022603', 'GO:0009894', 'GO:0005975', 'GO:0008324', 'GO:0015031', 'GO:0031399', 'GO:1901137', 'GO:0019725', 'GO:0036477', 'GO:0007267', 'GO:0099081', 'GO:0031966', 'GO:0051050', 'GO:0005911', 'GO:0004888', 'GO:1901702', 'GO:0099512', 'GO:0140677', 'GO:0031347', 'GO:0060322', 'GO:0048589', 'GO:0022890', 'GO:0003729', 'GO:0000323', 'GO:0051174', 'GO:0007281', 'GO:0042803', 'GO:0019220', 'GO:1901615', 'GO:0002684', 'GO:0010608', 'GO:0002009', 'GO:0008092', 'GO:0008233', 'GO:0048880', 'GO:0043604', 'GO:1903047', 'GO:0032990', 'GO:0030029', 'GO:0010564', 'GO:2000145', 'GO:0032880', 'GO:0099503', 'GO:0017076', 'GO:0007420', 'GO:0034660', 'GO:0060284', 'GO:0006812', 'GO:0050776', 'GO:0035239', 'GO:0043069', 'GO:0042742', 'GO:1902533', 'GO:0033365', 'GO:0071824', 'GO:0006979', 'GO:0048858', 'GO:0044403', 'GO:0050790', 'GO:0000122', 'GO:0009416', 'GO:0120039', 'GO:0098827', 'GO:0042175', 'GO:0048812', 'GO:0071496', 'GO:0045597', 'GO:0016071', 'GO:0008219', 'GO:0016829', 'GO:0012506', 'GO:0043066', 'GO:0150063', 'GO:0140657', 'GO:0055086', 'GO:0001654', 'GO:0012501', 'GO:0051276', 'GO:0009266', 'GO:0098657', 'GO:0030424', 'GO:0016604', 'GO:0005789', 'GO:0061024', 'GO:0030659', 'GO:0061061', 'GO:0048667', 'GO:0030334', 'GO:0016053', 'GO:0030036', 'GO:0055082', 'GO:0046394', 'GO:0099568', 'GO:0051248', 'GO:0031012', 'GO:0009986', 'GO:0030855', 'GO:0000226', 'GO:0030163', 'GO:0007507', 'GO:0006518', 'GO:0031344', 'GO:0051129', 'GO:0010817', 'GO:0006520', 'GO:0006325', 'GO:0030097', 'GO:0042325', 'GO:1902074', 'GO:0032870', 'GO:0022803', 'GO:0015267', 'GO:0008285', 'GO:0046914', 'GO:0051640', 'GO:0043009', 'GO:0005618', 'GO:0031668', 'GO:0019866', 'GO:0050801', 'GO:0032553', 'GO:0120035', 'GO:0044093', 'GO:0090066', 'GO:0044297', 'GO:0097305', 'GO:0090407', 'GO:0030001', 'GO:0000981', 'GO:0048232', 'GO:0055080', 'GO:0097435', 'GO:0002831', 'GO:0071396', 'GO:0042578', 'GO:0008289', 'GO:0006886', 'GO:0046873', 'GO:0051701', 'GO:0005840', 'GO:0048285', 'GO:0051046', 'GO:0005635', 'GO:0010038', 'GO:0016757', 'GO:0061564', 'GO:0001932', 'GO:0043687', 'GO:0005938', 'GO:0022804', 'GO:0048568', 'GO:0004674', 'GO:0006281', 'GO:0032555', 'GO:0007292', 'GO:1901699', 'GO:0005743', 'GO:0031329', 'GO:0019900', 'GO:1901361', 'GO:0007186', 'GO:0043025', 'GO:0042594', 'GO:0140098', 'GO:0051321', 'GO:0007155', 'GO:0043043', 'GO:0098793', 'GO:0048732', 'GO:2000241', 'GO:0032103', 'GO:0034097', 'GO:0000280', 'GO:0006091', 'GO:0044282', 'GO:1903530', 'GO:0005813', 'GO:0048638', 'GO:0030554', 'GO:0007283', 'GO:0008134', 'GO:0030155', 'GO:0000977', 'GO:0016817', 'GO:0034248', 'GO:0016818', 'GO:0034330', 'GO:0070482', 'GO:0034470', 'GO:0007409', 'GO:0016462', 'GO:0006338', 'GO:0045177', 'GO:0080135', 'GO:0098797', 'GO:0044057', 'GO:0005216', 'GO:0009410', 'GO:0071417', 'GO:0019439', 'GO:0015629', 'GO:0003682', 'GO:0032838', 'GO:0046903', 'GO:0019904', 'GO:0031669', 'GO:0016310', 'GO:0006753', 'GO:0048871', 'GO:0006915', 'GO:0051301', 'GO:0044391', 'GO:0031401', 'GO:0051603', 'GO:0098771', 'GO:0032774', 'GO:0005764', 'GO:0006790', 'GO:0045087', 'GO:1903046', 'GO:0060627', 'GO:0006970', 'GO:0140352', 'GO:0070647', 'GO:0090596', 'GO:0009117', 'GO:0051960', 'GO:0098798', 'GO:0006873', 'GO:0098794', 'GO:0005774', 'GO:0001817', 'GO:0045596', 'GO:0006351', 'GO:0051656', 'GO:0036293', 'GO:0006412', 'GO:0097447', 'GO:0019901', 'GO:0048471', 'GO:0097014', 'GO:0030425', 'GO:0006897', 'GO:0007600', 'GO:0048477', 'GO:0030003', 'GO:0035639', 'GO:0019748', 'GO:0007167', 'GO:0019867', 'GO:0022613', 'GO:0099402', 'GO:0000987', 'GO:0005759', 'GO:0040017', 'GO:0098660', 'GO:0071407', 'GO:0071214', 'GO:0104004', 'GO:0046700', 'GO:0098687', 'GO:0050778', 'GO:0072521', 'GO:0007005', 'GO:0001944', 'GO:0045229', 'GO:0001775', 'GO:0051604', 'GO:1901652', 'GO:0006631', 'GO:0015711', 'GO:0030141', 'GO:0051493', 'GO:0044270', 'GO:0009611', 'GO:0045165', 'GO:0010975', 'GO:0043408', 'GO:0043068', 'GO:0034220', 'GO:0001666', 'GO:0071345', 'GO:0007059', 'GO:0006468', 'GO:0007346', 'GO:0040011', 'GO:1901605', 'GO:0051668', 'GO:0016791', 'GO:0030031', 'GO:0043632', 'GO:0045937', 'GO:0010562', 'GO:0009896', 'GO:0007444', 'GO:0048736', 'GO:0032535', 'GO:0010256', 'GO:0006417', 'GO:0140694', 'GO:0002165', 'GO:0006397', 'GO:0099513', 'GO:0060090', 'GO:0006644', 'GO:0070848', 'GO:0019941', 'GO:0099177', 'GO:0050804', 'GO:1902532', 'GO:0005667', 'GO:0016798', 'GO:0071554', 'GO:0062197', 'GO:0044089', 'GO:0098552', 'GO:0021700', 'GO:0001501', 'GO:0006511', 'GO:0070201', 'GO:0032940', 'GO:0009267', 'GO:0016747', 'GO:1990351', 'GO:0017111', 'GO:0016758', 'GO:0006066', 'GO:0044092', 'GO:0008047', 'GO:0140678', 'GO:0016755', 'GO:0010638', 'GO:0045088', 'GO:0001558', 'GO:0001568', 'GO:2000147', 'GO:0005819', 'GO:0032446', 'GO:0031349', 'GO:0098662', 'GO:0140297', 'GO:0043085', 'GO:0051223', 'GO:0071363', 'GO:0006163', 'GO:0051052', 'GO:0120031', 'GO:0006720', 'GO:0009886', 'GO:0042579', 'GO:0008380', 'GO:0005777', 'GO:0043065', 'GO:0032559', 'GO:0043434', 'GO:0009620', 'GO:0007552', 'GO:1902495', 'GO:0016049', 'GO:0019787', 'GO:0004930', 'GO:0035107', 'GO:0061695', 'GO:0016741', 'GO:0031968', 'GO:0042327', 'GO:0000978', 'GO:0099536', 'GO:0098978', 'GO:0140097', 'GO:0001216', 'GO:0007163', 'GO:0098655', 'GO:0060560', 'GO:0016874', 'GO:0030335', 'GO:0097485', 'GO:0045321', 'GO:0072594', 'GO:0010720', 'GO:0048592', 'GO:0072657', 'GO:0007548', 'GO:0051051', 'GO:0007411', 'GO:0018193', 'GO:0016614', 'GO:0034655', 'GO:0048707', 'GO:0016054', 'GO:0046486', 'GO:0022626', 'GO:0005976', 'GO:0005261', 'GO:0003013', 'GO:0001934', 'GO:0022853', 'GO:0046395', 'GO:0015849', 'GO:0010154', 'GO:0042692', 'GO:0098791', 'GO:1901987', 'GO:0042254', 'GO:0016853', 'GO:0030162', 'GO:1903829', 'GO:0035821', 'GO:0016705', 'GO:0045786', 'GO:0030139', 'GO:0046942', 'GO:0062012', 'GO:0004175', 'GO:0050767', 'GO:0050865', 'GO:0045927', 'GO:0050890', 'GO:0008514', 'GO:0050808', 'GO:0000793', 'GO:0016324', 'GO:0043010', 'GO:0022836', 'GO:0004553', 'GO:0099537', 'GO:0004842', 'GO:0001101', 'GO:0030695', 'GO:0098852', 'GO:0060589', 'GO:0048562', 'GO:0030545', 'GO:1901617', 'GO:0051703', 'GO:0008168', 'GO:0098813', 'GO:0009526', 'GO:0002833', 'GO:0072001', 'GO:0044782', 'GO:0007018', 'GO:0042330', 'GO:0071695', 'GO:0009408', 'GO:0060562', 'GO:0032970', 'GO:0044550', 'GO:0048316', 'GO:0140013', 'GO:0007268', 'GO:0098916', 'GO:0009100', 'GO:0055044', 'GO:0002683', 'GO:0009506', 'GO:0000375', 'GO:0002237', 'GO:0062023', 'GO:0002064', 'GO:0015980', 'GO:0007611', 'GO:0030427', 'GO:0043269', 'GO:0016567', 'GO:0015291', 'GO:0009615', 'GO:0009636', 'GO:0005198', 'GO:0005524', 'GO:0007560', 'GO:0016616', 'GO:0008015', 'GO:0048367', 'GO:0016032', 'GO:0000377', 'GO:0030674', 'GO:0030546', 'GO:0051336', 'GO:0050803', 'GO:0040029', 'GO:0048514', 'GO:0051606', 'GO:0003712', 'GO:0004518', 'GO:0060541', 'GO:0000325', 'GO:1902903', 'GO:0043292', 'GO:0016835', 'GO:0001667', 'GO:0004857', 'GO:0048018', 'GO:0002694', 'GO:0030900', 'GO:0031346', 'GO:0000228', 'GO:0035770', 'GO:0032102', 'GO:0098609', 'GO:0045787', 'GO:0018995', 'GO:0009274', 'GO:0016052', 'GO:0000398', 'GO:0019216', 'GO:0034762', 'GO:0016051', 'GO:0050829', 'GO:0032956', 'GO:0043657', 'GO:0009952', 'GO:0006310', 'GO:1903311', 'GO:0060271', 'GO:0007517', 'GO:0061919', 'GO:0043254', 'GO:0033643', 'GO:0001819', 'GO:0008270', 'GO:0022407', 'GO:0043410', 'GO:0006914', 'GO:0000775', 'GO:0019693', 'GO:0072330', 'GO:0005543', 'GO:0036464', 'GO:0050807', 'GO:0034599', 'GO:0001228', 'GO:0051338', 'GO:0150034', 'GO:0048469', 'GO:0002164', 'GO:0007169', 'GO:0010498', 'GO:0009259', 'GO:0022411', 'GO:0016810', 'GO:0010876', 'GO:0006399', 'GO:0045785', 'GO:0035220', 'GO:0007626', 'GO:0048511', 'GO:1901654', 'GO:0030100', 'GO:0044706', 'GO:0009532', 'GO:0060537', 'GO:0000302', 'GO:0048545', 'GO:0005874', 'GO:0030133', 'GO:0006721', 'GO:0097060', 'GO:0009570', 'GO:0051146', 'GO:0032496', 'GO:0006935', 'GO:0042391', 'GO:0016607', 'GO:0015934', 'GO:0051962', 'GO:0010927', 'GO:0051047', 'GO:0010948', 'GO:0006366', 'GO:0045926', 'GO:0031331', 'GO:0009651', 'GO:0006650', 'GO:0007015', 'GO:0016072', 'GO:0048737', 'GO:0051249', 'GO:0007369', 'GO:0010639', 'GO:0009743', 'GO:0045178', 'GO:0044003', 'GO:0033218', 'GO:0000139', 'GO:0005509', 'GO:0008361', 'GO:0046649', 'GO:0002697', 'GO:0010959', 'GO:0009150', 'GO:0003779', 'GO:0050839', 'GO:0001822', 'GO:0035114', 'GO:0050832', 'GO:0009101', 'GO:1903532', 'GO:0006643', 'GO:0031099', 'GO:1901990', 'GO:0030016', 'GO:0031252', 'GO:0006605', 'GO:0010506', 'GO:0008544', 'GO:0030447', 'GO:0008654', 'GO:0008202', 'GO:0035120', 'GO:0016042', 'GO:0022604', 'GO:1901293', 'GO:0045089', 'GO:0071241', 'GO:0000910', 'GO:0043161', 'GO:0045137', 'GO:0004497', 'GO:0071453', 'GO:0015631', 'GO:0008299', 'GO:0008757', 'GO:0009451', 'GO:0005802', 'GO:0006260', 'GO:0008194', 'GO:0008406', 'GO:0000287', 'GO:0000322', 'GO:0032153', 'GO:0016779', 'GO:0009409', 'GO:1901653', 'GO:0002252', 'GO:0031400', 'GO:0030055', 'GO:0061629', 'GO:0009165', 'GO:0002521', 'GO:0005342', 'GO:0009612', 'GO:0006954', 'GO:0055123', 'GO:0030111', 'GO:0007584', 'GO:0075136', 'GO:0003007', 'GO:0032386', 'GO:0051169', 'GO:0046883', 'GO:0009798', 'GO:0046943', 'GO:0009925', 'GO:0006913', 'GO:0000151', 'GO:0006302', 'GO:0044272', 'GO:0034976', 'GO:0034329', 'GO:0045333', 'GO:0043549', 'GO:0061982', 'GO:0001701', 'GO:0052200', 'GO:0052173', 'GO:0000324', 'GO:0035282', 'GO:0048872', 'GO:0009579', 'GO:0036294', 'GO:0052689', 'GO:0050878', 'GO:0048749', 'GO:0034249', 'GO:0006869', 'GO:1904951', 'GO:1901888', 'GO:0006401', 'GO:0009941', 'GO:0030099', 'GO:0055001', 'GO:0090068', 'GO:0099572', 'GO:0042063', 'GO:0002253', 'GO:0010008', 'GO:0140993', 'GO:0060187', 'GO:0001217', 'GO:0043086', 'GO:0071456', 'GO:0031965', 'GO:1904949', 'GO:0090287', 'GO:0048193', 'GO:0005996', 'GO:0030017', 'GO:0050830', 'GO:0009755', 'GO:0050708', 'GO:0015931', 'GO:0043235', 'GO:1903706', 'GO:0016050', 'GO:0005765', 'GO:0045859', 'GO:0005769', 'GO:0060485', 'GO:0051222', 'GO:0140014', 'GO:0007472', 'GO:0045335', 'GO:0008652', 'GO:0098984', 'GO:0009737', 'GO:0048515', 'GO:0051020', 'GO:0016765', 'GO:0042176', 'GO:0098800', 'GO:0140546', 'GO:1990204', 'GO:0048565', 'GO:0070085', 'GO:0022622', 'GO:0050678', 'GO:0009895', 'GO:0048364', 'GO:0005925', 'GO:0042445', 'GO:0048827', 'GO:0050769', 'GO:0003012', 'GO:0001525', 'GO:0009897', 'GO:0006364', 'GO:0005930', 'GO:0010563', 'GO:0051607', 'GO:0007623', 'GO:0046982', 'GO:0045936', 'GO:0042060', 'GO:0030135', 'GO:0009799', 'GO:0007476', 'GO:0015399', 'GO:0051286', 'GO:0009855', 'GO:0051259', 'GO:0017148', 'GO:2000243', 'GO:1905368', 'GO:0022834', 'GO:0072522', 'GO:0110053', 'GO:0005681', 'GO:0045017', 'GO:2001233', 'GO:0061640', 'GO:0043583', 'GO:0050795', 'GO:0008340', 'GO:0007286', 'GO:0015276', 'GO:0031348', 'GO:0004721', 'GO:0042440', 'GO:0052031', 'GO:0010721', 'GO:0044242', 'GO:0000779', 'GO:0034765', 'GO:1901988', 'GO:0031647', 'GO:0048588', 'GO:0015081', 'GO:0052572', 'GO:0016323', 'GO:0009415', 'GO:0048639', 'GO:0071248', 'GO:0009953', 'GO:0016887', 'GO:0032279', 'GO:0006261', 'GO:0043413', 'GO:0071375', 'GO:0019058', 'GO:0006486', 'GO:0007127', 'GO:0017171', 'GO:0032259', 'GO:0040013', 'GO:0032868', 'GO:0090575', 'GO:0009880', 'GO:0050867', 'GO:0048229', 'GO:0019207', 'GO:0005770', 'GO:0048705', 'GO:0099111', 'GO:0015935', 'GO:0031253', 'GO:0090257', 'GO:0060972', 'GO:0048839', 'GO:0055074', 'GO:0032886', 'GO:0001664', 'GO:0042277', 'GO:0050727', 'GO:0061659', 'GO:0071478', 'GO:0044182', 'GO:0048580', 'GO:0043656', 'GO:0034702', 'GO:0016236', 'GO:0033646', 'GO:0006997', 'GO:0031267', 'GO:0030534', 'GO:0045930', 'GO:0006109', 'GO:0005741', 'GO:1903522', 'GO:0050863', 'GO:0046530', 'GO:0043062', 'GO:0000819', 'GO:0006403', 'GO:0051056', 'GO:1901607', 'GO:0016331', 'GO:0003924', 'GO:0090087', 'GO:0061448', 'GO:0061008', 'GO:0019001', 'GO:1902075', 'GO:0007051', 'GO:0006164', 'GO:0009308', 'GO:0070382', 'GO:0014069', 'GO:0015078', 'GO:0050954', 'GO:0000776', 'GO:0009411', 'GO:0036064', 'GO:0031032', 'GO:0009414', 'GO:0045471', 'GO:0046467', 'GO:0002696', 'GO:0001889', 'GO:0060041', 'GO:0046661', 'GO:0032561', 'GO:0007368', 'GO:0140101', 'GO:0071466', 'GO:0030863', 'GO:0090130', 'GO:0090092', 'GO:0001745', 'GO:1903509', 'GO:0002791', 'GO:0046165', 'GO:1990778', 'GO:0090558', 'GO:0016298', 'GO:0045862', 'GO:0071555', 'GO:0045814', 'GO:0051090', 'GO:0042170', 'GO:0060538', 'GO:0018958', 'GO:1904062', 'GO:0052553', 'GO:0043523', 'GO:0008017', 'GO:0043177', 'GO:0008033', 'GO:0061630', 'GO:0006633', 'GO:0090276', 'GO:0043414', 'GO:0015103', 'GO:0000070', 'GO:0030010', 'GO:1903037', 'GO:0051345', 'GO:0042546', 'GO:0002764', 'GO:0033500', 'GO:0000271', 'GO:1901342', 'GO:0071216', 'GO:0048863', 'GO:0097190', 'GO:0015079', 'GO:1901136', 'GO:0033044', 'GO:0007613', 'GO:0046890', 'GO:0006457', 'GO:0031406', 'GO:0016836', 'GO:0016830', 'GO:1905392', 'GO:0072686', 'GO:0050920', 'GO:0051235', 'GO:0034284', 'GO:1901681', 'GO:0000041', 'GO:0098727', 'GO:0045664', 'GO:0035091', 'GO:0007033', 'GO:0051963', 'GO:0060249', 'GO:0016197', 'GO:0019887', 'GO:0031976', 'GO:0046474', 'GO:0006865', 'GO:1903131', 'GO:0031047', 'GO:0009826', 'GO:0004519', 'GO:2000146', 'GO:0090150', 'GO:0031960', 'GO:0005126', 'GO:0022625', 'GO:0044703', 'GO:0009060', 'GO:0090132', 'GO:0042110', 'GO:0051302', 'GO:0051054', 'GO:0030705', 'GO:0019827', 'GO:0016114', 'GO:0045765', 'GO:0007498', 'GO:0004540', 'GO:0021537', 'GO:0006887', 'GO:0043934', 'GO:0001708', 'GO:0009534', 'GO:0042180', 'GO:0016485', 'GO:1902850', 'GO:0042593', 'GO:0071826', 'GO:0008021', 'GO:0042326', 'GO:0002699', 'GO:0070542', 'GO:0007162', 'GO:0030246', 'GO:0051251', 'GO:0042542', 'GO:0060828', 'GO:0010631', 'GO:0001906', 'GO:0061134', 'GO:0030198', 'GO:0030587', 'GO:0061013', 'GO:0099120', 'GO:0098857', 'GO:0046390', 'GO:0010001', 'GO:0051347', 'GO:0009746', 'GO:0031514', 'GO:0045121', 'GO:0045211', 'GO:0090567', 'GO:0006816', 'GO:0001894', 'GO:0007034', 'GO:1902105', 'GO:0006402', 'GO:0008234', 'GO:0010565', 'GO:0070372', 'GO:0003713', 'GO:0008037', 'GO:0140053', 'GO:0016358', 'GO:0000313', 'GO:0044306', 'GO:0001503', 'GO:0000281', 'GO:0019318', 'GO:0022409', 'GO:0016407', 'GO:0034357', 'GO:0009582', 'GO:0043487', 'GO:0072329', 'GO:0002757', 'GO:0009260', 'GO:0015850', 'GO:0005761', 'GO:0045132', 'GO:0030336', 'GO:0048762', 'GO:0009581', 'GO:0022618', 'GO:0034250', 'GO:0001818', 'GO:0097306', 'GO:0015293', 'GO:0044389', 'GO:0007424', 'GO:0006665', 'GO:0030707', 'GO:0098562', 'GO:0010970', 'GO:0055088', 'GO:0070663', 'GO:0009994', 'GO:0045807', 'GO:0009749', 'GO:0042886', 'GO:0009793', 'GO:0098754', 'GO:1901657', 'GO:0009908', 'GO:0051048', 'GO:0015698', 'GO:1905369', 'GO:0070469', 'GO:0030880', 'GO:0023061', 'GO:0019098', 'GO:0001933', 'GO:0071219', 'GO:0043484', 'GO:0003018', 'GO:0015294', 'GO:0016811', 'GO:1902115', 'GO:0051783', 'GO:0062039', 'GO:1901873', 'GO:0070828', 'GO:0043679', 'GO:0030313', 'GO:0141060', 'GO:0062040', 'GO:0007264', 'GO:0000428', 'GO:0005085', 'GO:0000165', 'GO:0034504', 'GO:0006766', 'GO:0098858', 'GO:0051961', 'GO:0032271', 'GO:0009555', 'GO:0060491', 'GO:0009063', 'GO:0016209', 'GO:0006874', 'GO:0051213', 'GO:0098803', 'GO:0031625', 'GO:0002119', 'GO:0005096', 'GO:0001539', 'GO:0044042', 'GO:0006936', 'GO:0032355', 'GO:0051384', 'GO:0051495', 'GO:0007606', 'GO:0031594', 'GO:0007612', 'GO:0030902', 'GO:0051260', 'GO:0019898', 'GO:0005795', 'GO:0042651', 'GO:0031345', 'GO:0022832', 'GO:0051216', 'GO:0046777', 'GO:0006959', 'GO:0001754', 'GO:0006909', 'GO:1903828', 'GO:0042752', 'GO:0070302', 'GO:1903825', 'GO:0120254', 'GO:0042626', 'GO:0019221', 'GO:0098802', 'GO:0043488', 'GO:0001763', 'GO:0035150', 'GO:0007188', 'GO:1902911', 'GO:0005244', 'GO:0009913', 'GO:1905039', 'GO:0030098', 'GO:0031507', 'GO:0071852', 'GO:0120032', 'GO:0031461', 'GO:0000075', 'GO:0005788', 'GO:0032872', 'GO:0009152', 'GO:0099094', 'GO:0010632', 'GO:0010976', 'GO:0071692', 'GO:0062013', 'GO:0008016', 'GO:0043648', 'GO:0032944', 'GO:0006813', 'GO:0006576', 'GO:0008509', 'GO:0007605', 'GO:0051924', 'GO:0008236', 'GO:0071222', 'GO:0014706', 'GO:0050900', 'GO:0014074', 'GO:0042393', 'GO:1901991', 'GO:1903531', 'GO:0009566', 'GO:0019902', 'GO:0048599', 'GO:0006575', 'GO:0001227', 'GO:0034614', 'GO:0043473', 'GO:0016410', 'GO:0035592', 'GO:0051896', 'GO:0032984', 'GO:0033157', 'GO:0046578', 'GO:0050796', 'GO:0031503', 'GO:0048738', 'GO:0010810', 'GO:0071482', 'GO:0050670', 'GO:0006664', 'GO:0032200', 'GO:0030307', 'GO:0002181', 'GO:0003341', 'GO:0030430', 'GO:0032409', 'GO:0021953', 'GO:0008237', 'GO:0042303', 'GO:0097730', 'GO:0009306', 'GO:0016311', 'GO:0046148', 'GO:0048640', 'GO:0002703', 'GO:0055035', 'GO:0050777', 'GO:0031330', 'GO:0051015', 'GO:0019932', 'GO:0050906', 'GO:0005525', 'GO:0000725', 'GO:0030308', 'GO:0007178', 'GO:0045216', 'GO:0010212', 'GO:0030426', 'GO:0030414', 'GO:1902905', 'GO:0016903', 'GO:0050770', 'GO:0048366', 'GO:0098739', 'GO:0046887', 'GO:0000502', 'GO:0003735', 'GO:0046488', 'GO:0061135', 'GO:0022900', 'GO:0051168', 'GO:0198738', 'GO:0015171', 'GO:0002520', 'GO:0051098', 'GO:0030865', 'GO:0000956', 'GO:0030667', 'GO:0030658', 'GO:0042461', 'GO:0098743', 'GO:0009948', 'GO:1901981', 'GO:0009535', 'GO:0051648', 'GO:0030136', 'GO:1903320', 'GO:0050657', 'GO:0033674', 'GO:0022898', 'GO:0007565', 'GO:0005746', 'GO:0048593', 'GO:0005539', 'GO:0048167', 'GO:0043021', 'GO:0002218', 'GO:0034764', 'GO:0043270', 'GO:0006022', 'GO:0031640', 'GO:0005811', 'GO:0141061', 'GO:0060348', 'GO:0030435', 'GO:0072524', 'GO:0016055', 'GO:0035966', 'GO:1903050', 'GO:0051236', 'GO:0055029', 'GO:0006694', 'GO:1901606', 'GO:0015085', 'GO:2000377', 'GO:0034399', 'GO:0002700', 'GO:1905952', 'GO:0050658', 'GO:0006937', 'GO:0048284', 'GO:0050730', 'GO:0031334', 'GO:0003697', 'GO:0060173', 'GO:0050684', 'GO:0070603', 'GO:0052547', 'GO:0009505', 'GO:0045732', 'GO:0032412', 'GO:0008081', 'GO:0007350', 'GO:0071383', 'GO:0048706', 'GO:0035148', 'GO:0009277', 'GO:0030178', 'GO:0044000', 'GO:2001234', 'GO:0042734', 'GO:0002685', 'GO:0030832', 'GO:0045861', 'GO:1902554', 'GO:1903034', 'GO:0016879', 'GO:1902493', 'GO:0031985', 'GO:0061138', 'GO:0000724', 'GO:1903039', 'GO:0030258', 'GO:0002262', 'GO:0045860', 'GO:0072659', 'GO:0099738', 'GO:0031248', 'GO:0007519', 'GO:0099003', 'GO:0022627', 'GO:0001650', 'GO:0043200', 'GO:0004620', 'GO:0008360', 'GO:0016667', 'GO:0055002', 'GO:0004866', 'GO:0002832', 'GO:0006275', 'GO:0032869', 'GO:0018205', 'GO:0004713', 'GO:0045834', 'GO:0008094', 'GO:0008064', 'GO:0003714', 'GO:0009898', 'GO:0030864', 'GO:0044036', 'GO:0031589', 'GO:0006400', 'GO:0015108', 'GO:0048813', 'GO:0016627', 'GO:0097729', 'GO:0031983', 'GO:0051494', 'GO:0016125', 'GO:0000792', 'GO:0045182', 'GO:0005267', 'GO:0050821', 'GO:0034612', 'GO:1904888', 'GO:0043296', 'GO:0141091', 'GO:0060205', 'GO:1902652', 'GO:0006767', 'GO:0009733', 'GO:0032388', 'GO:0010508', 'GO:0030496', 'GO:0034774', 'GO:0006879', 'GO:0008217', 'GO:0044770', 'GO:0043524', 'GO:0030323', 'GO:0031674', 'GO:0005516', 'GO:0005319', 'GO:0035270', 'GO:0071900', 'GO:0015748', 'GO:1902904', 'GO:0015297', 'GO:0050714', 'GO:0022408', 'GO:0030239', 'GO:0065004', 'GO:0005875', 'GO:0045296', 'GO:0030324', 'GO:0001709', 'GO:0008238', 'GO:0071806', 'GO:0072527', 'GO:0009055', 'GO:0021915', 'GO:0043122', 'GO:0005912', 'GO:0010015', 'GO:0046683', 'GO:0061387', 'GO:0046434', 'GO:0043467', 'GO:0050000', 'GO:0006898', 'GO:0051650', 'GO:0034703', 'GO:0017157', 'GO:0098630', 'GO:0019903', 'GO:0035050', 'GO:0007265', 'GO:0000578', 'GO:0052548', 'GO:0061351', 'GO:0007416', 'GO:0035296', 'GO:0000209', 'GO:0019842', 'GO:0008170', 'GO:0033273', 'GO:0007631', 'GO:0048024', 'GO:0055037', 'GO:0031023', 'GO:0071322', 'GO:0003014', 'GO:0033875', 'GO:0050679', 'GO:0033865', 'GO:0034032', 'GO:0048437', 'GO:0008643', 'GO:0140534', 'GO:0042710', 'GO:0006972', 'GO:1901264', 'GO:0032505', 'GO:0032006', 'GO:0050768', 'GO:0019751', 'GO:0021543', 'GO:0010977', 'GO:0004386', 'GO:0045637', 'GO:0000123', 'GO:0010494', 'GO:0000781', 'GO:0048754', 'GO:0071559', 'GO:0060285', 'GO:0009404', 'GO:0035051', 'GO:0009268', 'GO:0006839', 'GO:0050870', 'GO:0000723', 'GO:0009914', 'GO:0022612', 'GO:0022843', 'GO:2000242', 'GO:0120252', 'GO:0070374', 'GO:0009753', 'GO:0060326', 'GO:0071897', 'GO:0003730', 'GO:0045727', 'GO:0071011', 'GO:0006690', 'GO:1905475', 'GO:0034605', 'GO:0006885', 'GO:0051091', 'GO:1903313', 'GO:0005643', 'GO:0001653', 'GO:0010026', 'GO:0005881', 'GO:0015718', 'GO:0001700', 'GO:0016247', 'GO:0000329', 'GO:0030833', 'GO:0048660', 'GO:0035967', 'GO:0007093', 'GO:0007088', 'GO:0043588', 'GO:0070507', 'GO:0002443', 'GO:0044010', 'GO:0051983', 'GO:0046546', 'GO:0008038', 'GO:0042181', 'GO:0046686', 'GO:0042246', 'GO:0007052', 'GO:0008584', 'GO:0001676', 'GO:0043409', 'GO:0008276', 'GO:0001704', 'GO:1901655', 'GO:0098862', 'GO:0046328', 'GO:0061025', 'GO:0007617', 'GO:0050773', 'GO:0043244', 'GO:0044309', 'GO:0042783', 'GO:0030018', 'GO:0140030', 'GO:0045931', 'GO:0099504', 'GO:0061136', 'GO:0022904', 'GO:0046660', 'GO:0008080', 'GO:0009311', 'GO:0044772', 'GO:0071560', 'GO:0050905', 'GO:0000272', 'GO:0030666', 'GO:0016101', 'GO:0010821', 'GO:0050792', 'GO:0099106', 'GO:0016651', 'GO:0009064', 'GO:0048704', 'GO:0046906', 'GO:0021782', 'GO:0035825', 'GO:0031396', 'GO:0030594', 'GO:0006090', 'GO:0061326', 'GO:0005700', 'GO:0097746', 'GO:0004527', 'GO:0042274', 'GO:0043279', 'GO:0043197', 'GO:0006836', 'GO:0006986', 'GO:0005506', 'GO:0009593', 'GO:0035272', 'GO:0071356', 'GO:0008374', 'GO:0001578', 'GO:0007338', 'GO:0031016', 'GO:0070665', 'GO:0050673', 'GO:0005933', 'GO:0050866', 'GO:2001242', 'GO:1902806', 'GO:0031123', 'GO:0032543', 'GO:0002706', 'GO:0030148', 'GO:0030662', 'GO:0072593', 'GO:0006006', 'GO:0030516', 'GO:0001764', 'GO:0046677', 'GO:0031505', 'GO:0009247', 'GO:0031333', 'GO:0002758', 'GO:0016620', 'GO:0016831', 'GO:0033655', 'GO:0006805', 'GO:0009856', 'GO:0051303', 'GO:0048663', 'GO:0046527', 'GO:0048534', 'GO:0051053', 'GO:0009657', 'GO:0120114', 'GO:2000058', 'GO:0098742', 'GO:0002768', 'GO:0051445', 'GO:0072006', 'GO:0016684', 'GO:0061371', 'GO:0032368', 'GO:0071013', 'GO:0043621', 'GO:0042471', 'GO:0090079', 'GO:0050806', 'GO:0042383', 'GO:0062207', 'GO:0070192', 'GO:0051592', 'GO:0030217', 'GO:0043195', 'GO:0003205', 'GO:0016482', 'GO:0006352', 'GO:0001751', 'GO:0005230', 'GO:0006081', 'GO:0019954', 'GO:0030964', 'GO:0030866', 'GO:0051170', 'GO:0010634', 'GO:0004601', 'GO:0098876', 'GO:0016877', 'GO:0002793', 'GO:0019838', 'GO:0071470', 'GO:0008528', 'GO:0002274', 'GO:2000027', 'GO:0097193', 'GO:0042335', 'GO:0044068', 'GO:0001738', 'GO:0009066', 'GO:0019362', 'GO:0050764', 'GO:0050731', 'GO:1904950', 'GO:0042129', 'GO:0007029', 'GO:0045619', 'GO:0002573', 'GO:0051588', 'GO:0007528', 'GO:0031100', 'GO:0015370', 'GO:0005816', 'GO:0005507', 'GO:0008277', 'GO:0090277', 'GO:0022037', 'GO:0019783', 'GO:0016782', 'GO:0032526', 'GO:0043255', 'GO:0045055', 'GO:0019955', 'GO:0002263', 'GO:0007009', 'GO:0004521', 'GO:0034308', 'GO:0030684', 'GO:0042770', 'GO:0046496', 'GO:1904396', 'GO:0042597', 'GO:0034101', 'GO:0051224', 'GO:0016591', 'GO:0000922', 'GO:0003727', 'GO:0043087', 'GO:0051225', 'GO:0070588', 'GO:0040014', 'GO:0072175', 'GO:0007043', 'GO:0009072', 'GO:0014033', 'GO:0031570', 'GO:0051897', 'GO:0002366', 'GO:0002695', 'GO:0002819', 'GO:0007422', 'GO:1903555', 'GO:0070160', 'GO:0016241', 'GO:0036180', 'GO:0007030', 'GO:0051346', 'GO:0016838', 'GO:0008306', 'GO:0042462', 'GO:0070918', 'GO:0002250', 'GO:0051348', 'GO:0030203', 'GO:0014902', 'GO:0007189', 'GO:0090090', 'GO:0016866', 'GO:0036126', 'GO:0005125', 'GO:0006282', 'GO:0032946', 'GO:0007131', 'GO:0140527', 'GO:0008356', 'GO:0048046', 'GO:0032680', 'GO:0048546', 'GO:0030522', 'GO:0030145', 'GO:0072073', 'GO:0042562', 'GO:1901989', 'GO:0002702', 'GO:0008201', 'GO:0061014', 'GO:0000118', 'GO:0000315', 'GO:0004252', 'GO:0015144', 'GO:0006814', 'GO:0005657', 'GO:0050660', 'GO:1901505', 'GO:0045171', 'GO:0005762', 'GO:0046915', 'GO:0030027', 'GO:0020037', 'GO:0019722', 'GO:0097731', 'GO:0004536', 'GO:0016763', 'GO:0002020', 'GO:0005844', 'GO:0020023', 'GO:0005262', 'GO:0000794', 'GO:0043022', 'GO:0045271', 'GO:0030532', 'GO:0015179', 'GO:0033647', 'GO:0005876', 'GO:0033644', 'GO:0019209', 'GO:0016860', 'GO:0045111', 'GO:0033648', 'GO:0048786', 'GO:0005253', 'GO:0003774', 'GO:0005179', 'GO:0051540', 'GO:0099501', 'GO:0034708', 'GO:0030672', 'GO:0031256', 'GO:0051536', 'GO:0097733', 'GO:0140104', 'GO:0031072', 'GO:0034451', 'GO:0042887', 'GO:0016709', 'GO:0005775', 'GO:0033293', 'GO:0030288', 'GO:0016469', 'GO:0005935', 'GO:0043209', 'GO:0019199', 'GO:0005903', 'GO:0000932', 'GO:0005747', 'GO:0051087', 'GO:0030295', 'GO:0140375', 'GO:0043204', 'GO:0016706', 'GO:0101005', 'GO:0099023', 'GO:0016922', 'GO:0004197', 'GO:0070851', 'GO:0004722', 'GO:0035869', 'GO:0017022', 'GO:0035251', 'GO:0001669', 'GO:0019897', 'GO:0019213', 'GO:0005884', 'GO:0005249', 'GO:0004867', 'GO:0019829', 'GO:0031970', 'GO:0008173', 'GO:0005798', 'GO:0015932', 'GO:0044325', 'GO:0016645', 'GO:0030175', 'GO:0001221', 'GO:0051082', 'GO:0004659', 'GO:0031490', 'GO:0032182', 'GO:0016875', 'GO:0048475', 'GO:0032154', 'GO:0030117', 'GO:0004812', 'GO:0097525', 'GO:0048029', 'GO:0090734', 'GO:0005178', 'GO:0000791', 'GO:0004725', 'GO:0008135', 'GO:0005814', 'GO:0016878', 'GO:0005776', 'GO:0043073', 'GO:0009295', 'GO:0016528', 'GO:0051287', 'GO:0016769', 'GO:0022824', 'GO:0022835', 'GO:0010333', 'GO:0098960', 'GO:1902936', 'GO:0005254', 'GO:0044423', 'GO:0008483', 'GO:0001726', 'GO:0045495', 'GO:0003678', 'GO:0005758', 'GO:0099634', 'GO:0005604', 'GO:0001727', 'GO:0019208', 'GO:0020015', 'GO:0015036', 'GO:0019200', 'GO:0005937', 'GO:0020002', 'GO:0044853', 'GO:0016796', 'GO:0001750', 'GO:0043596', 'GO:0005496', 'GO:0015926', 'GO:0101002', 'GO:1990782', 'GO:0004222', 'GO:0017053', 'GO:0000314', 'GO:0005763', 'GO:0016799', 'GO:0044304', 'GO:0009279', 'GO:0004364', 'GO:0030665', 'GO:0042054', 'GO:0019205', 'GO:0060293', 'GO:0008408', 'GO:0050661', 'GO:0000149', 'GO:0042025', 'GO:0034212', 'GO:0004896', 'GO:0016405', 'GO:0005902', 'GO:0003725', 'GO:0009975', 'GO:0019888', 'GO:0098982', 'GO:0034705', 'GO:0016893', 'GO:0043186', 'GO:0016854', 'GO:0035861', 'GO:0008028', 'GO:0016776', 'GO:0035097', 'GO:0032040', 'GO:0004843', 'GO:0043178', 'GO:0016701', 'GO:0004520', 'GO:0043332', 'GO:0005778', 'GO:0031903', 'GO:0042581', 'GO:0008146', 'GO:1901618', 'GO:1901682', 'GO:0043130', 'GO:0004714', 'GO:0016628', 'GO:0016278', 'GO:0070820', 'GO:0044732', 'GO:0016814', 'GO:0015453', 'GO:0031672', 'GO:0016279', 'GO:0070279', 'GO:0005684', 'GO:0016597', 'GO:0140828', 'GO:0019843', 'GO:0008083', 'GO:0005766', 'GO:0016529', 'GO:0016234', 'GO:0042582', 'GO:0055120', 'GO:0042826', 'GO:0015101', 'GO:0070566', 'GO:0016638', 'GO:0030165', 'GO:0035591', 'GO:0070461', 'GO:0070069', 'GO:0032432', 'GO:0072562', 'GO:0008227', 'GO:0008013', 'GO:0051219', 'GO:0019210', 'GO:0005923', 'GO:0030170', 'GO:0098685', 'GO:0016702', 'GO:0016780', 'GO:0005231', 'GO:0016881', 'GO:0061733', 'GO:0060076', 'GO:0051119', 'GO:0070938', 'GO:0030247', 'GO:0016328', 'GO:0008023', 'GO:0051117', 'GO:1903293', 'GO:0008076', 'GO:0015175', 'GO:0004984', 'GO:0051233', 'GO:0042788', 'GO:0000152', 'GO:0016592', 'GO:0019005', 'GO:0015145', 'GO:1990841', 'GO:0005546', 'GO:0005882', 'GO:0032451', 'GO:0045179', 'GO:0000030', 'GO:0042379', 'GO:0030120', 'GO:0004860', 'GO:0031901', 'GO:0008375', 'GO:0005901', 'GO:0017124', 'GO:0008287', 'GO:0032588', 'GO:0036379', 'GO:0016655', 'GO:0033558', 'GO:0016840', 'GO:0001046', 'GO:0098839', 'GO:0004532', 'GO:0008171', 'GO:0045334', 'GO:0042641', 'GO:0097747', 'GO:0030134', 'GO:0005548', 'GO:0015605', 'GO:0042162', 'GO:0140034', 'GO:0072341', 'GO:0016891', 'GO:0034062', 'GO:0004402', 'GO:0051539', 'GO:0044291', 'GO:0035064', 'GO:0009705', 'GO:0003823', 'GO:0016849', 'GO:0045259', 'GO:0008186', 'GO:0022624', 'GO:0001673', 'GO:0016363', 'GO:0005771', 'GO:0000138', 'GO:0031902', 'GO:0003743', 'GO:0030276', 'GO:0005796', 'GO:0019239', 'GO:0098531', 'GO:0140358', 'GO:0001098', 'GO:0005793', 'GO:0016790', 'GO:1904813', 'GO:0015662', 'GO:0042763', 'GO:0016859', 'GO:0140296', 'GO:0030670', 'GO:0005839', 'GO:0070063', 'GO:0140313', 'GO:0071949', 'GO:1902562', 'GO:0008378', 'GO:0098533', 'GO:0032589', 'GO:0038024', 'GO:0005518', 'GO:0140318', 'GO:0019840', 'GO:0046332', 'GO:0031519', 'GO:0001099', 'GO:0031201', 'GO:0008188', 'GO:1905348', 'GO:0000049', 'GO:0003724', 'GO:0004177', 'GO:0120013', 'GO:0020016', 'GO:0015149', 'GO:0099529', 'GO:0004112', 'GO:0001725', 'GO:0097517', 'GO:0030894', 'GO:0016896', 'GO:0072349', 'GO:0004879', 'GO:0004407', 'GO:0005721', 'GO:0042764', 'GO:0015295', 'GO:0003777', 'GO:0031526', 'GO:0005905', 'GO:0060170', 'GO:0003899', 'GO:0034061', 'GO:0015459', 'GO:0005372', 'GO:0060590', 'GO:0031969', 'GO:0140938', 'GO:0005797', 'GO:0005272', 'GO:1904315', 'GO:0005753', 'GO:0019003', 'GO:0000307', 'GO:0090729', 'GO:0016248', 'GO:0061980', 'GO:0003755', 'GO:0016646', 'GO:0044232', 'GO:0031234', 'GO:0016538', 'GO:0005838', 'GO:0019203', 'GO:0005801', 'GO:0008200', 'GO:0008157', 'GO:0017046', 'GO:0043202', 'GO:0098636', 'GO:0005343', 'GO:0052866', 'GO:0022884', 'GO:0032266', 'GO:0022829', 'GO:0140299', 'GO:0030428', 'GO:0030669', 'GO:0000940', 'GO:0090482', 'GO:0140359', 'GO:0101031', 'GO:0099086', 'GO:0003684', 'GO:0004180', 'GO:0031430', 'GO:0031369', 'GO:0005628', 'GO:0048306', 'GO:0042645', 'GO:0016846', 'GO:0097110', 'GO:0050840', 'GO:0097346', 'GO:0005934', 'GO:0030971', 'GO:0140223', 'GO:0051139', 'GO:0004683', 'GO:0016717', 'GO:0016229', 'GO:0008066', 'GO:0030315', 'GO:0004623', 'GO:0061783', 'GO:0042910', 'GO:0000124', 'GO:0005791', 'GO:0005826', 'GO:0015645', 'GO:0030118', 'GO:0004712', 'GO:0005504', 'GO:0030686', 'GO:0140597', 'GO:0008235', 'GO:0032934', 'GO:1905354', 'GO:0061645', 'GO:0005501', 'GO:0005355', 'GO:0005385', 'GO:0005200', 'GO:0042575', 'GO:0005782', 'GO:0031907', 'GO:0052742', 'GO:0008009', 'GO:0004806', 'GO:0016712', 'GO:0015035', 'GO:0005310', 'GO:0000217', 'GO:0140767', 'GO:0030119', 'GO:0001540', 'GO:0016459', 'GO:0005767', 'GO:0002039', 'GO:0098878', 'GO:0030479', 'GO:0008175', 'GO:0016832', 'GO:0043539', 'GO:0030371', 'GO:0008320', 'GO:0000786', 'GO:0019894', 'GO:0016413', 'GO:0000795', 'GO:0070001', 'GO:0016605', 'GO:0031593', 'GO:0000178', 'GO:0020011', 'GO:0004030', 'GO:0043495', 'GO:0016775', 'GO:0019905', 'GO:0015020', 'GO:0001530', 'GO:0043601', 'GO:0008198', 'GO:0035254', 'GO:0015296', 'GO:1990752', 'GO:0030660', 'GO:0001917', 'GO:0008422', 'GO:0140457', 'GO:0000175', 'GO:0000407', 'GO:0004190', 'GO:0015250', 'GO:0050897', 'GO:0032452', 'GO:0031491', 'GO:0020036', 'GO:0031091', 'GO:0030551', 'GO:0003955', 'GO:0055038', 'GO:0008395', 'GO:0019104', 'GO:0001786', 'GO:0035579', 'GO:0005705', 'GO:0008328', 'GO:0004033', 'GO:0034704', 'GO:0071782', 'GO:0035578', 'GO:0005246', 'GO:0070273', 'GO:0003954', 'GO:0004114', 'GO:0016641', 'GO:0030332', 'GO:0033176', 'GO:0019028', 'GO:0016668', 'GO:0005581', 'GO:0031261', 'GO:0031312', 'GO:0001533', 'GO:0001411', 'GO:0140102', 'GO:0015377', 'GO:0097542', 'GO:0016857', 'GO:0030990', 'GO:0000137', 'GO:0043175', 'GO:1902555', 'GO:0016504', 'GO:0010181', 'GO:0030286', 'GO:1904680', 'GO:0090406', 'GO:0071855', 'GO:0005921', 'GO:0015929', 'GO:0032587', 'GO:0015215', 'GO:0050145', 'GO:0015562', 'GO:0003887', 'GO:0043531', 'GO:0051537', 'GO:0014704', 'GO:0141052', 'GO:0005547', 'GO:0045169', 'GO:0005918', 'GO:0043190', 'GO:0120111', 'GO:0015030', 'GO:0016409', 'GO:0032156', 'GO:0005048', 'GO:0016679', 'GO:0017069', 'GO:0005669', 'GO:0004016', 'GO:0015923', 'GO:0005865', 'GO:0004707', 'GO:0001223', 'GO:0032421', 'GO:0019212', 'GO:0022839', 'GO:0044295', 'GO:0034518', 'GO:0043198', 'GO:0016411', 'GO:1905360', 'GO:0004529', 'GO:0051018', 'GO:0008180', 'GO:0051879', 'GO:0019825', 'GO:0030137', 'GO:0015651', 'GO:0016661', 'GO:0000131', 'GO:0015026', 'GO:0030291', 'GO:0008376', 'GO:0070888', 'GO:0005892', 'GO:0019865', 'GO:0045309', 'GO:1990756', 'GO:0004029', 'GO:0016251', 'GO:0005871', 'GO:0009881', 'GO:0097546', 'GO:0000146', 'GO:0110085', 'GO:0017080', 'GO:0005637', 'GO:0016833', 'GO:0035267', 'GO:0043189', 'GO:0016861', 'GO:0160041', 'GO:0005184', 'GO:0045293', 'GO:0005416', 'GO:0033612', 'GO:0001891', 'GO:0016868', 'GO:0008527', 'GO:0005879', 'GO:0008187', 'GO:0012507', 'GO:0030544', 'GO:0035371', 'GO:0005245', 'GO:0005680', 'GO:0044183', 'GO:0072687', 'GO:0016722', 'GO:0052745', 'GO:0005080', 'GO:0033764', 'GO:0031143', 'GO:0140303', 'GO:0005685', 'GO:0015216', 'GO:0005818', 'GO:0001968', 'GO:0017147', 'GO:0016514', 'GO:0005537', 'GO:0043394', 'GO:0008305', 'GO:0020003', 'GO:0140445', 'GO:0043425', 'GO:0098631', 'GO:0016895', 'GO:0005217', 'GO:0019956', 'GO:0030446', 'GO:0031045', 'GO:1990023', 'GO:0030981', 'GO:0042625', 'GO:0044769', 'GO:0001222', 'GO:0046961', 'GO:0004089', 'GO:0070569', 'GO:0046912', 'GO:0016289', 'GO:0022848', 'GO:0015205', 'GO:0031011', 'GO:0016813', 'GO:0000803', 'GO:0004864', 'GO:0030125', 'GO:0019789', 'GO:0009524', 'GO:0015485', 'GO:0003916', 'GO:0098799', 'GO:0005201', 'GO:0001965', 'GO:0034593', 'GO:0005452', 'GO:0070402', 'GO:0051427', 'GO:0016812', 'GO:0015556', 'GO:0015925', 'GO:0048787', 'GO:0016675', 'GO:0016725', 'GO:0008649', 'GO:0010334', 'GO:0001671', 'GO:0080025', 'GO:0008409', 'GO:0098576', 'GO:0070971', 'GO:0004602', 'GO:0016783', 'GO:0005665', 'GO:0016671', 'GO:0050308', 'GO:0004467', 'GO:0042301', 'GO:0050136', 'GO:0043015', 'GO:0005732', 'GO:0030140', 'GO:0043236', 'GO:0008301', 'GO:0070821', 'GO:0016327', 'GO:0005242', 'GO:0061650', 'GO:0080008', 'GO:0015174', 'GO:0097038', 'GO:0005227', 'GO:0036002', 'GO:0035838', 'GO:0008106', 'GO:0015373', 'GO:0005154', 'GO:0070567', 'GO:0000993', 'GO:0015172', 'GO:0032010', 'GO:0004869', 'GO:0010369', 'GO:0097526', 'GO:0004568', 'GO:0008252', 'GO:0000295', 'GO:0051285', 'GO:0000339', 'GO:0016742', 'GO:0016841', 'GO:0045277', 'GO:0005849', 'GO:0005942', 'GO:0008266', 'GO:0015106', 'GO:0004629', 'GO:0005283', 'GO:0005834', 'GO:0004673', 'GO:0004970', 'GO:0015464', 'GO:0046540', 'GO:0099091', 'GO:0015385', 'GO:0098686', 'GO:0032806', 'GO:0005338', 'GO:0140220', 'GO:0055028', 'GO:0035250', 'GO:0016408', 'GO:0004708', 'GO:0097225', 'GO:0090599', 'GO:0016721', 'GO:0032420', 'GO:0000421', 'GO:0016581', 'GO:0090545', 'GO:0016471', 'GO:0042805', 'GO:0020007', 'GO:0004784', 'GO:0010287', 'GO:0070717', 'GO:0009528', 'GO:0008373', 'GO:0031977', 'GO:0030992', 'GO:0031093', 'GO:0005484', 'GO:0030374', 'GO:0030131', 'GO:0043548', 'GO:0044298', 'GO:0003785', 'GO:0005852', 'GO:0008137', 'GO:0004396', 'GO:0004129', 'GO:0070300', 'GO:0051019', 'GO:0097386', 'GO:0015252', 'GO:0030687', 'GO:0099522', 'GO:0005544', 'GO:0008641', 'GO:0097472', 'GO:0090665', 'GO:0015378', 'GO:0032813', 'GO:0004693', 'GO:0042646', 'GO:0043028', 'GO:0004675', 'GO:0035673', 'GO:0035198', 'GO:0004115', 'GO:0032391', 'GO:0001674', 'GO:0032590', 'GO:0035580', 'GO:0031463', 'GO:0046625', 'GO:0000930', 'GO:0043138', 'GO:0005346', 'GO:0016235', 'GO:0045239', 'GO:0061174', 'GO:0018455', 'GO:0004709', 'GO:0017025', 'GO:0005381', 'GO:0009706', 'GO:0140142', 'GO:0030331', 'GO:0015368', 'GO:0016837', 'GO:0044390', 'GO:0016863', 'GO:0071617', 'GO:0036452', 'GO:0016682', 'GO:0120014', 'GO:0099092', 'GO:0003688', 'GO:0005402', 'GO:0005243', 'GO:0051010', 'GO:0097718', 'GO:0005640', 'GO:0015248', 'GO:0048487', 'GO:0000235', 'GO:0008417', 'GO:0035255', 'GO:0070403', 'GO:0030515', 'GO:0005891', 'GO:0071162', 'GO:0000176', 'GO:0005686', 'GO:0031082', 'GO:0070822', 'GO:0016273', 'GO:0016028', 'GO:0051920', 'GO:0061631', 'GO:0004559', 'GO:0038187', 'GO:0016864', 'GO:0003756', 'GO:0101003', 'GO:0140311', 'GO:0004022', 'GO:0031420', 'GO:0033178', 'GO:0035577', 'GO:0005847', 'GO:0005790', 'GO:0016274', 'GO:0042605', 'GO:0000974', 'GO:0047617', 'GO:0005703', 'GO:0098644', 'GO:0005742', 'GO:0001772', 'GO:0000900', 'GO:0120119', 'GO:0030867', 'GO:0046875', 'GO:0005832', 'GO:0005337', 'GO:0030445', 'GO:0009982', 'GO:0140103', 'GO:0008172', 'GO:0031097', 'GO:0008484', 'GO:0045172', 'GO:0008061', 'GO:0032541', 'GO:0016417', 'GO:0045273', 'GO:0045275', 'GO:0005750', 'GO:0071014', 'GO:0050664', 'GO:0140312', 'GO:0016160', 'GO:0031160', 'GO:0000145', 'GO:0004715', 'GO:0106389', 'GO:0042555', 'GO:0016500', 'GO:0030014', 'GO:1904724', 'GO:0005927', 'GO:0008307', 'GO:0048770', 'GO:0016615', 'GO:0015166', 'GO:0016010', 'GO:0032036', 'GO:0071889', 'GO:0042287', 'GO:0042644', 'GO:0098847', 'GO:0032809', 'GO:0008143', 'GO:0098632', 'GO:0005112', 'GO:0098688', 'GO:0001091', 'GO:0042056', 'GO:0005109', 'GO:0098554', 'GO:0017134', 'GO:0042834', 'GO:0070847', 'GO:0004549', 'GO:0005751', 'GO:0005885', 'GO:0099604', 'GO:0099589', 'GO:0048027', 'GO:0051861', 'GO:0003707', 'GO:0031010', 'GO:0008138', 'GO:0042470', 'GO:0001848', 'GO:0010485', 'GO:0000935', 'GO:0001784', 'GO:0043194', 'GO:0090571', 'GO:0008556', 'GO:0010314', 'GO:0070492', 'GO:0031941', 'GO:0015038', 'GO:0043014', 'GO:0042169', 'GO:0140463', 'GO:0005160', 'GO:0016894']\n",
            "3004\n",
            "['GO:0003674', 'GO:0005488', 'GO:0005515', 'GO:0003824', 'GO:0097159', 'GO:0003676', 'GO:0016740', 'GO:0016787', 'GO:0140096', 'GO:0003677', 'GO:0042802', 'GO:0043167', 'GO:0005215', 'GO:0016491', 'GO:0022857', 'GO:0043565', 'GO:0016772', 'GO:0003723', 'GO:0098772', 'GO:0019899', 'GO:0003690', 'GO:0044877', 'GO:0140110', 'GO:1990837', 'GO:0016301', 'GO:0036094', 'GO:0016788', 'GO:0043169', 'GO:0043168', 'GO:0016773', 'GO:0001067', 'GO:0000976', 'GO:0046872', 'GO:0005102', 'GO:0003700', 'GO:0060089', 'GO:0030234', 'GO:0038023', 'GO:0015075', 'GO:1901363', 'GO:0015318', 'GO:0046983', 'GO:0004672', 'GO:0140640', 'GO:0097367', 'GO:0016746', 'GO:0000166', 'GO:1901265', 'GO:0008324', 'GO:0004888', 'GO:1901702', 'GO:0140677', 'GO:0022890', 'GO:0003729', 'GO:0042803', 'GO:0008092', 'GO:0008233', 'GO:0017076', 'GO:0016829', 'GO:0140657', 'GO:0022803', 'GO:0015267', 'GO:0046914', 'GO:0032553', 'GO:0000981', 'GO:0008289', 'GO:0042578', 'GO:0046873', 'GO:0016757', 'GO:0022804', 'GO:0004674', 'GO:0032555', 'GO:0019900', 'GO:0140098', 'GO:0030554', 'GO:0008134', 'GO:0000977', 'GO:0016817', 'GO:0016818', 'GO:0016462', 'GO:0005216', 'GO:0003682', 'GO:0019904', 'GO:0019901', 'GO:0035639', 'GO:0000987', 'GO:0016791', 'GO:0060090', 'GO:0016798', 'GO:0016747', 'GO:0017111', 'GO:0016758', 'GO:0008047', 'GO:0140678', 'GO:0016755', 'GO:0140297', 'GO:0032559', 'GO:0019787', 'GO:0004930', 'GO:0016741', 'GO:0000978', 'GO:0140097', 'GO:0001216', 'GO:0016874', 'GO:0016614', 'GO:0005261', 'GO:0022853', 'GO:0016853', 'GO:0016705', 'GO:0004175', 'GO:0008514', 'GO:0022836', 'GO:0004553', 'GO:0004842', 'GO:0060589', 'GO:0030695', 'GO:0030545', 'GO:0008168', 'GO:0015291', 'GO:0005198', 'GO:0005524', 'GO:0016616', 'GO:0030674', 'GO:0030546', 'GO:0003712', 'GO:0004518', 'GO:0016835', 'GO:0004857', 'GO:0048018', 'GO:0008270', 'GO:0005543', 'GO:0001228', 'GO:0016810', 'GO:0033218', 'GO:0005509', 'GO:0003779', 'GO:0050839', 'GO:0004497', 'GO:0015631', 'GO:0008757', 'GO:0008194', 'GO:0000287', 'GO:0016779', 'GO:0061629', 'GO:0005342', 'GO:0046943', 'GO:0052689', 'GO:0140993', 'GO:0001217', 'GO:0051020', 'GO:0016765', 'GO:0046982', 'GO:0015399', 'GO:0022834', 'GO:0015276', 'GO:0004721', 'GO:0015081', 'GO:0016887', 'GO:0017171', 'GO:0019207', 'GO:0042277', 'GO:0001664', 'GO:0061659', 'GO:0031267', 'GO:0003924', 'GO:0019001', 'GO:0015078', 'GO:0140101', 'GO:0032561', 'GO:0016298', 'GO:0043177', 'GO:0008017', 'GO:0061630', 'GO:0015103', 'GO:0015079', 'GO:0031406', 'GO:0016836', 'GO:0016830', 'GO:1901681', 'GO:0035091', 'GO:0019887', 'GO:0004519', 'GO:0005126', 'GO:0004540', 'GO:0030246', 'GO:0061134', 'GO:0008234', 'GO:0003713', 'GO:0016407', 'GO:0044389', 'GO:0015293', 'GO:0015294', 'GO:0016811', 'GO:0005085', 'GO:0016209', 'GO:0051213', 'GO:0031625', 'GO:0005096', 'GO:0022832', 'GO:0042626', 'GO:0005244', 'GO:0099094', 'GO:0008509', 'GO:0008236', 'GO:0042393', 'GO:0019902', 'GO:0001227', 'GO:0016410', 'GO:0008237', 'GO:0005525', 'GO:0051015', 'GO:0030414', 'GO:0016903', 'GO:0003735', 'GO:0061135', 'GO:0015171', 'GO:1901981', 'GO:0043021', 'GO:0005539', 'GO:0015085', 'GO:0003697', 'GO:0008081', 'GO:0016879', 'GO:0004620', 'GO:0016667', 'GO:0004866', 'GO:0004713', 'GO:0008094', 'GO:0003714', 'GO:0015108', 'GO:0016627', 'GO:0045182', 'GO:0005267', 'GO:0005516', 'GO:0005319', 'GO:0015297', 'GO:0045296', 'GO:0008238', 'GO:0009055', 'GO:0019903', 'GO:0019842', 'GO:0008170', 'GO:0004386', 'GO:0022843', 'GO:0003730', 'GO:0001653', 'GO:0016247', 'GO:0008276', 'GO:0140030', 'GO:0008080', 'GO:0016651', 'GO:0099106', 'GO:0046906', 'GO:0030594', 'GO:0004527', 'GO:0005506', 'GO:0008374', 'GO:0016620', 'GO:0016831', 'GO:0046527', 'GO:0016684', 'GO:0043621', 'GO:0090079', 'GO:0005230', 'GO:0019838', 'GO:0016877', 'GO:0004601', 'GO:0008528', 'GO:0015370', 'GO:0019955', 'GO:0005507', 'GO:0019783', 'GO:0016782', 'GO:0004521', 'GO:0003727', 'GO:0016838', 'GO:0005125', 'GO:0016866', 'GO:0030145', 'GO:0042562', 'GO:0008201', 'GO:0004252', 'GO:0015144', 'GO:0050660', 'GO:1901505', 'GO:0046915', 'GO:0020037', 'GO:0004536', 'GO:0016763', 'GO:0002020', 'GO:0005262', 'GO:0043022', 'GO:0015179', 'GO:0016860', 'GO:0019209', 'GO:0005253', 'GO:0003774', 'GO:0005179', 'GO:0051540', 'GO:0051536', 'GO:0140104', 'GO:0031072', 'GO:0042887', 'GO:0016709', 'GO:0033293', 'GO:0019199', 'GO:0030295', 'GO:0140375', 'GO:0051087', 'GO:0016706', 'GO:0016922', 'GO:0101005', 'GO:0004197', 'GO:0070851', 'GO:0004722', 'GO:0017022', 'GO:0035251', 'GO:0019213', 'GO:0005249', 'GO:0004867', 'GO:0019829', 'GO:0015932', 'GO:0008173', 'GO:0044325', 'GO:0016645', 'GO:0051082', 'GO:0031490', 'GO:0001221', 'GO:0032182', 'GO:0004659', 'GO:0016875', 'GO:0048029', 'GO:0004812', 'GO:0004725', 'GO:0005178', 'GO:0008135', 'GO:0016878', 'GO:0051287', 'GO:0016769', 'GO:0022835', 'GO:0098960', 'GO:0010333', 'GO:1902936', 'GO:0022824', 'GO:0005254', 'GO:0008483', 'GO:0003678', 'GO:0001727', 'GO:0019208', 'GO:0015036', 'GO:0016796', 'GO:0019200', 'GO:0015926', 'GO:0005496', 'GO:1990782', 'GO:0004222', 'GO:0016799', 'GO:0004364', 'GO:0042054', 'GO:0019205', 'GO:0000149', 'GO:0050661', 'GO:0008408', 'GO:0034212', 'GO:0004896', 'GO:0016405', 'GO:0003725', 'GO:0009975', 'GO:0019888', 'GO:0016893', 'GO:0016854', 'GO:0008028', 'GO:0016776', 'GO:0004843', 'GO:0043178', 'GO:0016701', 'GO:0004520', 'GO:0008146', 'GO:1901618', 'GO:1901682', 'GO:0016628', 'GO:0004714', 'GO:0043130', 'GO:0016278', 'GO:0016279', 'GO:0016597', 'GO:0015453', 'GO:0070279', 'GO:0016814', 'GO:0019843', 'GO:0140828', 'GO:0008083', 'GO:0042826', 'GO:0070566', 'GO:0015101', 'GO:0016638', 'GO:0030165', 'GO:0035591', 'GO:0008227', 'GO:0008013', 'GO:0051219', 'GO:0030170', 'GO:0019210', 'GO:0005231', 'GO:0016702', 'GO:0016881', 'GO:0016780', 'GO:0061733', 'GO:0030247', 'GO:0051119', 'GO:0051117', 'GO:0015175', 'GO:0004984', 'GO:0015145', 'GO:0005546', 'GO:1990841', 'GO:0032451', 'GO:0000030', 'GO:0004860', 'GO:0042379', 'GO:0008375', 'GO:0017124', 'GO:0016655', 'GO:0033558', 'GO:0016840', 'GO:0001046', 'GO:0008171', 'GO:0004532', 'GO:0097747', 'GO:0042162', 'GO:0005548', 'GO:0015605', 'GO:0140034', 'GO:0051539', 'GO:0004402', 'GO:0072341', 'GO:0034062', 'GO:0016891', 'GO:0003823', 'GO:0016849', 'GO:0035064', 'GO:0008186', 'GO:0003743', 'GO:0030276', 'GO:0019239', 'GO:0098531', 'GO:0001098', 'GO:0016790', 'GO:0140358', 'GO:0015662', 'GO:0016859', 'GO:0140296', 'GO:0140313', 'GO:0070063', 'GO:0071949', 'GO:0008378', 'GO:0046332', 'GO:0038024', 'GO:0019840', 'GO:0005518', 'GO:0140318', 'GO:0001099', 'GO:0008188', 'GO:0000049', 'GO:0120013', 'GO:0003724', 'GO:0004177', 'GO:0004879', 'GO:0016896', 'GO:0004407', 'GO:0015149', 'GO:0099529', 'GO:0004112', 'GO:0072349', 'GO:0015295', 'GO:0003777', 'GO:0034061', 'GO:0015459', 'GO:0003899', 'GO:0005372', 'GO:0005272', 'GO:0060590', 'GO:0140938', 'GO:1904315', 'GO:0019003', 'GO:0016248', 'GO:0061980', 'GO:0090729', 'GO:0003755', 'GO:0016646', 'GO:0016538', 'GO:0017046', 'GO:0008157', 'GO:0019203', 'GO:0008200', 'GO:0005343', 'GO:0032266', 'GO:0022829', 'GO:0052866', 'GO:0022884', 'GO:0140299', 'GO:0090482', 'GO:0140359', 'GO:0003684', 'GO:0004180', 'GO:0016846', 'GO:0097110', 'GO:0048306', 'GO:0031369', 'GO:0050840', 'GO:0051139', 'GO:0004683', 'GO:0030971', 'GO:0140223', 'GO:0008066', 'GO:0016229', 'GO:0016717', 'GO:0042910', 'GO:0004623', 'GO:0061783', 'GO:0015645', 'GO:0005504', 'GO:0004712', 'GO:0005501', 'GO:0008235', 'GO:0140597', 'GO:0032934', 'GO:0005200', 'GO:0005385', 'GO:0005355', 'GO:0005310', 'GO:0004806', 'GO:0015035', 'GO:0052742', 'GO:0008009', 'GO:0016712', 'GO:0000217', 'GO:0140767', 'GO:0008175', 'GO:0002039', 'GO:0001540', 'GO:0016832', 'GO:0030371', 'GO:0019894', 'GO:0016413', 'GO:0070001', 'GO:0043539', 'GO:0008320', 'GO:0031593', 'GO:0015020', 'GO:0001530', 'GO:0043495', 'GO:0004030', 'GO:0016775', 'GO:0019905', 'GO:0015296', 'GO:0035254', 'GO:0008198', 'GO:0008422', 'GO:0004190', 'GO:0000175', 'GO:0140457', 'GO:0015250', 'GO:0008395', 'GO:0001786', 'GO:0019104', 'GO:0032452', 'GO:0003955', 'GO:0030551', 'GO:0031491', 'GO:0050897', 'GO:0004114', 'GO:0005246', 'GO:0003954', 'GO:0070273', 'GO:0004033', 'GO:0016668', 'GO:0030332', 'GO:0016641', 'GO:0015377', 'GO:0140102', 'GO:0071855', 'GO:0010181', 'GO:0043175', 'GO:0016504', 'GO:1904680', 'GO:0015929', 'GO:0016857', 'GO:0050145', 'GO:0015215', 'GO:0015562', 'GO:0043531', 'GO:0005547', 'GO:0051537', 'GO:0003887', 'GO:0141052', 'GO:0016679', 'GO:0017069', 'GO:0005048', 'GO:0016409', 'GO:0004016', 'GO:0004707', 'GO:0015923', 'GO:0001223', 'GO:0022839', 'GO:0019212', 'GO:0019825', 'GO:0016411', 'GO:0051018', 'GO:0051879', 'GO:0004529', 'GO:0015651', 'GO:0070888', 'GO:0008376', 'GO:1990756', 'GO:0016251', 'GO:0019865', 'GO:0004029', 'GO:0015026', 'GO:0016661', 'GO:0045309', 'GO:0030291', 'GO:0009881', 'GO:0017080', 'GO:0000146', 'GO:0016861', 'GO:0016833', 'GO:0160041', 'GO:0005416', 'GO:0005184', 'GO:0008527', 'GO:0030544', 'GO:0033612', 'GO:0016868', 'GO:0008187', 'GO:0044183', 'GO:0033764', 'GO:0140303', 'GO:0005245', 'GO:0005080', 'GO:0015216', 'GO:0016722', 'GO:0052745', 'GO:0016895', 'GO:0098631', 'GO:0017147', 'GO:0043394', 'GO:0005217', 'GO:0043425', 'GO:0019956', 'GO:0001968', 'GO:0005537', 'GO:0046912', 'GO:0070569', 'GO:0001222', 'GO:0046961', 'GO:0042625', 'GO:0022848', 'GO:0016289', 'GO:0044769', 'GO:0004089', 'GO:0015485', 'GO:0019789', 'GO:0003916', 'GO:0004864', 'GO:0015205', 'GO:0016813', 'GO:0070402', 'GO:0005452', 'GO:0034593', 'GO:0001965', 'GO:0016812', 'GO:0051427', 'GO:0005201', 'GO:0015556', 'GO:0016725', 'GO:0016675', 'GO:0008649', 'GO:0015925', 'GO:0010334', 'GO:0080025', 'GO:0008409', 'GO:0001671', 'GO:0042301', 'GO:0016671', 'GO:0016783', 'GO:0004602', 'GO:0008301', 'GO:0050308', 'GO:0004467', 'GO:0043236', 'GO:0050136', 'GO:0043015', 'GO:0008106', 'GO:0005242', 'GO:0036002', 'GO:0061650', 'GO:0005227', 'GO:0015174', 'GO:0005154', 'GO:0070567', 'GO:0015172', 'GO:0008252', 'GO:0000295', 'GO:0004568', 'GO:0000993', 'GO:0004869', 'GO:0015373', 'GO:0000339', 'GO:0004629', 'GO:0008266', 'GO:0016841', 'GO:0004673', 'GO:0005283', 'GO:0016742', 'GO:0015106', 'GO:0004970', 'GO:0015464', 'GO:0005338', 'GO:0090599', 'GO:0004708', 'GO:0016721', 'GO:0015385', 'GO:0016408', 'GO:0035250', 'GO:0070717', 'GO:0004784', 'GO:0042805', 'GO:0008373', 'GO:0008137', 'GO:0004129', 'GO:0005484', 'GO:0030374', 'GO:0004396', 'GO:0043548', 'GO:0003785', 'GO:0051019', 'GO:0015378', 'GO:0070300', 'GO:0015252', 'GO:0008641', 'GO:0005544', 'GO:0097472', 'GO:0004693', 'GO:0004115', 'GO:0004675', 'GO:0035673', 'GO:0032813', 'GO:0043028', 'GO:0035198', 'GO:0004709', 'GO:0018455', 'GO:0005381', 'GO:0043138', 'GO:0005346', 'GO:0017025', 'GO:0046625', 'GO:0016863', 'GO:0140142', 'GO:0044390', 'GO:0030331', 'GO:0015368', 'GO:0016837', 'GO:0071617', 'GO:0005402', 'GO:0008417', 'GO:0048487', 'GO:0005243', 'GO:0015248', 'GO:0035255', 'GO:0070403', 'GO:0097718', 'GO:0003688', 'GO:0016682', 'GO:0051010', 'GO:0120014', 'GO:0030515', 'GO:0016273', 'GO:0038187', 'GO:0004022', 'GO:0016864', 'GO:0016274', 'GO:0003756', 'GO:0031420', 'GO:0004559', 'GO:0140311', 'GO:0042605', 'GO:0061631', 'GO:0051920', 'GO:0047617', 'GO:0000900', 'GO:0046875', 'GO:0005337', 'GO:0008061', 'GO:0009982', 'GO:0016417', 'GO:0140312', 'GO:0008172', 'GO:0050664', 'GO:0008484', 'GO:0140103', 'GO:0016500', 'GO:0008307', 'GO:0042287', 'GO:0015166', 'GO:0032036', 'GO:0016160', 'GO:0106389', 'GO:0004715', 'GO:0071889', 'GO:0016615', 'GO:0099604', 'GO:0098847', 'GO:0005109', 'GO:0017134', 'GO:0001091', 'GO:0005112', 'GO:0042056', 'GO:0098632', 'GO:0008143', 'GO:0042834', 'GO:0004549', 'GO:0001784', 'GO:0048027', 'GO:0003707', 'GO:0008138', 'GO:0051861', 'GO:0099589', 'GO:0001848', 'GO:0010485', 'GO:0010314', 'GO:0070492', 'GO:0042169', 'GO:0008556', 'GO:0015038', 'GO:0043014', 'GO:0005160', 'GO:0016894', 'GO:0140463']\n",
            "839\n"
          ]
        }
      ],
      "source": [
        "#This is a useless code block\n",
        "print(train_set['GO_term'].value_counts().index[:].tolist())\n",
        "print(len(train_set['GO_term'].value_counts().index[:].tolist()))\n",
        "train_set_mf=train_set[train_set['aspect']=='molecular_function']\n",
        "print(train_set_mf['GO_term'].value_counts().index[:].tolist())\n",
        "print(len(train_set_mf['GO_term'].value_counts().index[:].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "fNwtYqvUj3Ki",
        "outputId": "916d5b30-6711-44fd-94e2-821511ca0fb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(84638, 678)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GO:0005575</th>\n",
              "      <th>GO:0110165</th>\n",
              "      <th>GO:0005622</th>\n",
              "      <th>GO:0043226</th>\n",
              "      <th>GO:0043229</th>\n",
              "      <th>GO:0043227</th>\n",
              "      <th>GO:0005737</th>\n",
              "      <th>GO:0043231</th>\n",
              "      <th>GO:0005634</th>\n",
              "      <th>GO:0016020</th>\n",
              "      <th>...</th>\n",
              "      <th>GO:0005885</th>\n",
              "      <th>GO:0005751</th>\n",
              "      <th>GO:0032809</th>\n",
              "      <th>GO:0098688</th>\n",
              "      <th>GO:0090571</th>\n",
              "      <th>GO:0043194</th>\n",
              "      <th>GO:0031010</th>\n",
              "      <th>GO:0042470</th>\n",
              "      <th>GO:0000935</th>\n",
              "      <th>GO:0031941</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84633</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84634</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84635</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84636</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84637</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84638 rows × 678 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       GO:0005575  GO:0110165  GO:0005622  GO:0043226  GO:0043229  GO:0043227  \\\n",
              "0             1.0         1.0         1.0         1.0         1.0         1.0   \n",
              "1             1.0         1.0         0.0         0.0         0.0         0.0   \n",
              "2             1.0         1.0         1.0         0.0         0.0         0.0   \n",
              "3             1.0         1.0         0.0         0.0         0.0         0.0   \n",
              "4             1.0         1.0         1.0         1.0         1.0         1.0   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "84633         1.0         1.0         1.0         1.0         1.0         1.0   \n",
              "84634         1.0         1.0         1.0         0.0         0.0         0.0   \n",
              "84635         1.0         1.0         1.0         1.0         1.0         1.0   \n",
              "84636         1.0         1.0         1.0         1.0         1.0         1.0   \n",
              "84637         1.0         1.0         1.0         1.0         1.0         1.0   \n",
              "\n",
              "       GO:0005737  GO:0043231  GO:0005634  GO:0016020  ...  GO:0005885  \\\n",
              "0             0.0         1.0         1.0         0.0  ...         0.0   \n",
              "1             0.0         0.0         0.0         1.0  ...         0.0   \n",
              "2             1.0         0.0         0.0         0.0  ...         0.0   \n",
              "3             0.0         0.0         0.0         0.0  ...         0.0   \n",
              "4             1.0         1.0         0.0         1.0  ...         0.0   \n",
              "...           ...         ...         ...         ...  ...         ...   \n",
              "84633         1.0         1.0         1.0         0.0  ...         0.0   \n",
              "84634         1.0         0.0         0.0         0.0  ...         0.0   \n",
              "84635         0.0         1.0         1.0         0.0  ...         0.0   \n",
              "84636         1.0         1.0         1.0         0.0  ...         0.0   \n",
              "84637         1.0         1.0         1.0         0.0  ...         0.0   \n",
              "\n",
              "       GO:0005751  GO:0032809  GO:0098688  GO:0090571  GO:0043194  GO:0031010  \\\n",
              "0             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "1             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "2             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "3             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "4             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "84633         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "84634         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "84635         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "84636         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "84637         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "\n",
              "       GO:0042470  GO:0000935  GO:0031941  \n",
              "0             0.0         0.0         0.0  \n",
              "1             0.0         0.0         0.0  \n",
              "2             0.0         0.0         0.0  \n",
              "3             0.0         0.0         0.0  \n",
              "4             0.0         0.0         0.0  \n",
              "...           ...         ...         ...  \n",
              "84633         0.0         0.0         0.0  \n",
              "84634         0.0         0.0         0.0  \n",
              "84635         0.0         0.0         0.0  \n",
              "84636         0.0         0.0         0.0  \n",
              "84637         0.0         0.0         0.0  \n",
              "\n",
              "[84638 rows x 678 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set_cc=train_set[train_set['aspect']=='cellular_component']         ######################## Restricting to only the cellular component aspects\n",
        "\n",
        "labels_cc = train_set_cc['GO_term'].value_counts().index[:].tolist()      ######################## We take all the go terms used for cc aspects\n",
        "\n",
        "train_set_updated = train_set_cc                                         ####################### Here I copy train_set_cc only to keep the code after coherent\n",
        "\n",
        "train_size = cc_train.shape[0]                                            ########################## The n° of rows of cc_train is the number of rows of train_labels\n",
        "train_labels = np.zeros((train_size , len(labels_cc))) ########################## The length of labels_cc is how many go terms we need to predict\n",
        "\n",
        "series_train_protein_ids = cc_train['Protein_ID']   ###################### restrict to only cc\n",
        "\n",
        "for i in range(len(labels_cc)):   ##############################  we go go-term by go-term\n",
        "\n",
        "    n_train_terms = train_set_updated[train_set_updated['GO_term'] ==  labels_cc[i]]  ########################## I added _cc after labels for coherency\n",
        "\n",
        "    label_related_proteins = n_train_terms['Protein_ID'].unique()\n",
        "\n",
        "    train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n",
        "\n",
        "\n",
        "labels_cc_df = pd.DataFrame(data = train_labels, columns = labels_cc)\n",
        "print(labels_cc_df.shape)\n",
        "labels_cc_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "GCua__O3Myqd",
        "outputId": "df78eb6e-f71f-464d-8cb5-d5b0ab18ee97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(55698, 839)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GO:0003674</th>\n",
              "      <th>GO:0005488</th>\n",
              "      <th>GO:0005515</th>\n",
              "      <th>GO:0003824</th>\n",
              "      <th>GO:0097159</th>\n",
              "      <th>GO:0003676</th>\n",
              "      <th>GO:0016740</th>\n",
              "      <th>GO:0016787</th>\n",
              "      <th>GO:0140096</th>\n",
              "      <th>GO:0003677</th>\n",
              "      <th>...</th>\n",
              "      <th>GO:0010485</th>\n",
              "      <th>GO:0010314</th>\n",
              "      <th>GO:0070492</th>\n",
              "      <th>GO:0042169</th>\n",
              "      <th>GO:0008556</th>\n",
              "      <th>GO:0015038</th>\n",
              "      <th>GO:0043014</th>\n",
              "      <th>GO:0005160</th>\n",
              "      <th>GO:0016894</th>\n",
              "      <th>GO:0140463</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55693</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55694</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55695</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55696</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55697</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>55698 rows × 839 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       GO:0003674  GO:0005488  GO:0005515  GO:0003824  GO:0097159  GO:0003676  \\\n",
              "0             1.0         0.0         0.0         1.0         0.0         0.0   \n",
              "1             1.0         1.0         1.0         0.0         0.0         0.0   \n",
              "2             1.0         1.0         1.0         0.0         0.0         0.0   \n",
              "3             1.0         1.0         1.0         0.0         0.0         0.0   \n",
              "4             1.0         1.0         1.0         0.0         0.0         0.0   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "55693         1.0         0.0         0.0         1.0         0.0         0.0   \n",
              "55694         1.0         0.0         0.0         1.0         0.0         0.0   \n",
              "55695         1.0         0.0         0.0         1.0         0.0         0.0   \n",
              "55696         1.0         0.0         0.0         1.0         0.0         0.0   \n",
              "55697         1.0         1.0         1.0         0.0         0.0         0.0   \n",
              "\n",
              "       GO:0016740  GO:0016787  GO:0140096  GO:0003677  ...  GO:0010485  \\\n",
              "0             0.0         1.0         0.0         0.0  ...         0.0   \n",
              "1             0.0         0.0         0.0         0.0  ...         0.0   \n",
              "2             0.0         0.0         0.0         0.0  ...         0.0   \n",
              "3             0.0         0.0         0.0         0.0  ...         0.0   \n",
              "4             0.0         0.0         0.0         0.0  ...         0.0   \n",
              "...           ...         ...         ...         ...  ...         ...   \n",
              "55693         0.0         0.0         0.0         0.0  ...         0.0   \n",
              "55694         0.0         0.0         0.0         0.0  ...         0.0   \n",
              "55695         0.0         0.0         0.0         0.0  ...         0.0   \n",
              "55696         0.0         0.0         0.0         0.0  ...         0.0   \n",
              "55697         0.0         0.0         0.0         0.0  ...         0.0   \n",
              "\n",
              "       GO:0010314  GO:0070492  GO:0042169  GO:0008556  GO:0015038  GO:0043014  \\\n",
              "0             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "1             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "2             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "3             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "4             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "55693         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "55694         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "55695         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "55696         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "55697         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "\n",
              "       GO:0005160  GO:0016894  GO:0140463  \n",
              "0             0.0         0.0         0.0  \n",
              "1             0.0         0.0         0.0  \n",
              "2             0.0         0.0         0.0  \n",
              "3             0.0         0.0         0.0  \n",
              "4             0.0         0.0         0.0  \n",
              "...           ...         ...         ...  \n",
              "55693         0.0         0.0         0.0  \n",
              "55694         0.0         0.0         0.0  \n",
              "55695         0.0         0.0         0.0  \n",
              "55696         0.0         0.0         0.0  \n",
              "55697         0.0         0.0         0.0  \n",
              "\n",
              "[55698 rows x 839 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set_mf=train_set[train_set['aspect']=='molecular_function']         ######################## Restricting to only the cellular component aspects\n",
        "\n",
        "labels_mf = train_set_mf['GO_term'].value_counts().index[:].tolist()      ######################## We take all the go terms used for cc aspects\n",
        "\n",
        "train_set_updated = train_set_mf                                         ####################### Here I copy train_set_cc only to keep the code after coherent\n",
        "\n",
        "train_size = mf_train.shape[0]                                            ########################## The n° of rows of cc_train is the number of rows of train_labels\n",
        "train_labels = np.zeros((train_size , len(labels_mf))) ########################## The length of labels_cc is how many go terms we need to predict\n",
        "\n",
        "series_train_protein_ids = mf_train['Protein_ID']   ###################### restrict to only cc\n",
        "\n",
        "for i in range(len(labels_mf)):   ##############################  we go go-term by go-term\n",
        "\n",
        "    n_train_terms = train_set_updated[train_set_updated['GO_term'] ==  labels_mf[i]]  ########################## I added _cc after labels for coherency\n",
        "\n",
        "    label_related_proteins = n_train_terms['Protein_ID'].unique()\n",
        "\n",
        "    train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n",
        "\n",
        "\n",
        "labels_mf_df = pd.DataFrame(data = train_labels, columns = labels_mf)\n",
        "print(labels_mf_df.shape)\n",
        "labels_mf_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "W6ACxmE7M1_c",
        "outputId": "5e2eefee-af7b-40a3-8edf-571794546975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "(83064, 1487)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GO:0008150</th>\n",
              "      <th>GO:0009987</th>\n",
              "      <th>GO:0065007</th>\n",
              "      <th>GO:0050789</th>\n",
              "      <th>GO:0050794</th>\n",
              "      <th>GO:0008152</th>\n",
              "      <th>GO:0050896</th>\n",
              "      <th>GO:0071704</th>\n",
              "      <th>GO:0032501</th>\n",
              "      <th>GO:0044237</th>\n",
              "      <th>...</th>\n",
              "      <th>GO:0008356</th>\n",
              "      <th>GO:0032680</th>\n",
              "      <th>GO:0030522</th>\n",
              "      <th>GO:0048546</th>\n",
              "      <th>GO:0002702</th>\n",
              "      <th>GO:0072073</th>\n",
              "      <th>GO:1901989</th>\n",
              "      <th>GO:0061014</th>\n",
              "      <th>GO:0006814</th>\n",
              "      <th>GO:0019722</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83059</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83060</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83061</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83062</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83063</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>83064 rows × 1487 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       GO:0008150  GO:0009987  GO:0065007  GO:0050789  GO:0050794  GO:0008152  \\\n",
              "0             1.0         1.0         0.0         0.0         0.0         1.0   \n",
              "1             1.0         1.0         1.0         1.0         1.0         0.0   \n",
              "2             1.0         0.0         1.0         1.0         1.0         0.0   \n",
              "3             1.0         0.0         1.0         1.0         1.0         0.0   \n",
              "4             1.0         0.0         1.0         1.0         0.0         0.0   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "83059         1.0         1.0         1.0         0.0         0.0         1.0   \n",
              "83060         1.0         1.0         1.0         0.0         0.0         1.0   \n",
              "83061         1.0         1.0         1.0         0.0         0.0         1.0   \n",
              "83062         1.0         1.0         1.0         0.0         0.0         1.0   \n",
              "83063         1.0         1.0         1.0         1.0         1.0         0.0   \n",
              "\n",
              "       GO:0050896  GO:0071704  GO:0032501  GO:0044237  ...  GO:0008356  \\\n",
              "0             0.0         1.0         0.0         1.0  ...         0.0   \n",
              "1             0.0         0.0         1.0         0.0  ...         0.0   \n",
              "2             0.0         0.0         0.0         0.0  ...         0.0   \n",
              "3             0.0         0.0         0.0         0.0  ...         0.0   \n",
              "4             0.0         0.0         1.0         0.0  ...         0.0   \n",
              "...           ...         ...         ...         ...  ...         ...   \n",
              "83059         0.0         1.0         0.0         1.0  ...         0.0   \n",
              "83060         0.0         1.0         0.0         1.0  ...         0.0   \n",
              "83061         0.0         1.0         0.0         1.0  ...         0.0   \n",
              "83062         0.0         1.0         0.0         1.0  ...         0.0   \n",
              "83063         1.0         0.0         1.0         0.0  ...         0.0   \n",
              "\n",
              "       GO:0032680  GO:0030522  GO:0048546  GO:0002702  GO:0072073  GO:1901989  \\\n",
              "0             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "1             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "2             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "3             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "4             0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "83059         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "83060         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "83061         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "83062         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "83063         0.0         0.0         0.0         0.0         0.0         0.0   \n",
              "\n",
              "       GO:0061014  GO:0006814  GO:0019722  \n",
              "0             0.0         0.0         0.0  \n",
              "1             0.0         0.0         0.0  \n",
              "2             0.0         0.0         0.0  \n",
              "3             0.0         0.0         0.0  \n",
              "4             0.0         0.0         0.0  \n",
              "...           ...         ...         ...  \n",
              "83059         0.0         0.0         0.0  \n",
              "83060         0.0         0.0         0.0  \n",
              "83061         0.0         0.0         0.0  \n",
              "83062         0.0         0.0         0.0  \n",
              "83063         0.0         0.0         0.0  \n",
              "\n",
              "[83064 rows x 1487 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set_bp=train_set[train_set['aspect']=='biological_process']         ######################## Restricting to only the cellular component aspects\n",
        "\n",
        "labels_bp = train_set_bp['GO_term'].value_counts().index[:].tolist()      ######################## We take all the go terms used for cc aspects\n",
        "\n",
        "train_set_updated = train_set_bp                                         ####################### Here I copy train_set_cc only to keep the code after coherent\n",
        "\n",
        "train_size = bp_train.shape[0]                                            ########################## The n° of rows of cc_train is the number of rows of train_labels\n",
        "train_labels = np.zeros((train_size , len(labels_bp))) ########################## The length of labels_cc is how many go terms we need to predict\n",
        "\n",
        "series_train_protein_ids = bp_train['Protein_ID']   ###################### restrict to only cc\n",
        "\n",
        "for i in range(len(labels_bp)):   ##############################  we go go-term by go-term\n",
        "\n",
        "    n_train_terms = train_set_updated[train_set_updated['GO_term'] ==  labels_bp[i]]  ########################## I added _cc after labels for coherency\n",
        "\n",
        "    label_related_proteins = n_train_terms['Protein_ID'].unique()\n",
        "\n",
        "    train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n",
        "    if i%100==0:\n",
        "      print(i)\n",
        "\n",
        "labels_bp_df = pd.DataFrame(data = train_labels, columns = labels_bp)\n",
        "print(labels_bp_df.shape)\n",
        "labels_bp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgkE8BAVNvmT"
      },
      "source": [
        "Models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4NwnNZqWNxNC"
      },
      "outputs": [],
      "source": [
        "X_cc = cc_train\n",
        "y_cc = labels_cc_df\n",
        "X_train_cc, X_val_cc, y_train_cc, y_val_cc = train_test_split(X_cc,y_cc,test_size=0.2,random_state=42)\n",
        "\n",
        "X_mf = mf_train\n",
        "y_mf = labels_mf_df\n",
        "X_train_mf, X_val_mf, y_train_mf, y_val_mf = train_test_split(X_mf,y_mf,test_size=0.2,random_state=42)\n",
        "\n",
        "X_bp = bp_train\n",
        "y_bp = labels_bp_df\n",
        "X_train_bp, X_val_bp, y_train_bp, y_val_bp = train_test_split(X_bp,y_bp,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UforsLcECIDl"
      },
      "source": [
        "### CC models\n",
        "\n",
        "- Best val_avg_precision: 32%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4FTDMlBPwQR",
        "outputId": "0490469c-aaf0-4f73-c908-eaf86a892bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "14/14 [==============================] - 16s 982ms/step - loss: 0.3841 - binary_accuracy: 0.8507 - average_precision: 0.0197 - val_loss: 0.3953 - val_binary_accuracy: 0.9860 - val_average_precision: 0.0204\n",
            "Epoch 2/80\n",
            "14/14 [==============================] - 13s 915ms/step - loss: 0.0845 - binary_accuracy: 0.9845 - average_precision: 0.0205 - val_loss: 0.3933 - val_binary_accuracy: 0.9862 - val_average_precision: 0.0259\n",
            "Epoch 3/80\n",
            "14/14 [==============================] - 12s 861ms/step - loss: 0.0550 - binary_accuracy: 0.9865 - average_precision: 0.0254 - val_loss: 0.4968 - val_binary_accuracy: 0.9865 - val_average_precision: 0.0277\n",
            "Epoch 4/80\n",
            "14/14 [==============================] - 12s 870ms/step - loss: 0.0484 - binary_accuracy: 0.9867 - average_precision: 0.0296 - val_loss: 0.4921 - val_binary_accuracy: 0.9867 - val_average_precision: 0.0333\n",
            "Epoch 5/80\n",
            "14/14 [==============================] - 12s 824ms/step - loss: 0.0461 - binary_accuracy: 0.9870 - average_precision: 0.0372 - val_loss: 0.4834 - val_binary_accuracy: 0.9868 - val_average_precision: 0.0384\n",
            "Epoch 6/80\n",
            "14/14 [==============================] - 12s 896ms/step - loss: 0.0444 - binary_accuracy: 0.9875 - average_precision: 0.0438 - val_loss: 0.4682 - val_binary_accuracy: 0.9872 - val_average_precision: 0.0475\n",
            "Epoch 7/80\n",
            "14/14 [==============================] - 13s 975ms/step - loss: 0.0429 - binary_accuracy: 0.9878 - average_precision: 0.0517 - val_loss: 0.4548 - val_binary_accuracy: 0.9876 - val_average_precision: 0.0561\n",
            "Epoch 8/80\n",
            "14/14 [==============================] - 13s 917ms/step - loss: 0.0415 - binary_accuracy: 0.9880 - average_precision: 0.0598 - val_loss: 0.4366 - val_binary_accuracy: 0.9877 - val_average_precision: 0.0658\n",
            "Epoch 9/80\n",
            "14/14 [==============================] - 13s 911ms/step - loss: 0.0403 - binary_accuracy: 0.9881 - average_precision: 0.0696 - val_loss: 0.4180 - val_binary_accuracy: 0.9879 - val_average_precision: 0.0754\n",
            "Epoch 10/80\n",
            "14/14 [==============================] - 13s 919ms/step - loss: 0.0392 - binary_accuracy: 0.9883 - average_precision: 0.0797 - val_loss: 0.3983 - val_binary_accuracy: 0.9880 - val_average_precision: 0.0859\n",
            "Epoch 11/80\n",
            "14/14 [==============================] - 11s 785ms/step - loss: 0.0383 - binary_accuracy: 0.9884 - average_precision: 0.0896 - val_loss: 0.3775 - val_binary_accuracy: 0.9882 - val_average_precision: 0.0971\n",
            "Epoch 12/80\n",
            "14/14 [==============================] - 12s 810ms/step - loss: 0.0374 - binary_accuracy: 0.9885 - average_precision: 0.1002 - val_loss: 0.3577 - val_binary_accuracy: 0.9882 - val_average_precision: 0.1069\n",
            "Epoch 13/80\n",
            "14/14 [==============================] - 12s 877ms/step - loss: 0.0367 - binary_accuracy: 0.9886 - average_precision: 0.1103 - val_loss: 0.3343 - val_binary_accuracy: 0.9883 - val_average_precision: 0.1161\n",
            "Epoch 14/80\n",
            "14/14 [==============================] - 13s 905ms/step - loss: 0.0361 - binary_accuracy: 0.9887 - average_precision: 0.1194 - val_loss: 0.3144 - val_binary_accuracy: 0.9884 - val_average_precision: 0.1249\n",
            "Epoch 15/80\n",
            "14/14 [==============================] - 12s 903ms/step - loss: 0.0355 - binary_accuracy: 0.9888 - average_precision: 0.1290 - val_loss: 0.2932 - val_binary_accuracy: 0.9885 - val_average_precision: 0.1327\n",
            "Epoch 16/80\n",
            "14/14 [==============================] - 11s 806ms/step - loss: 0.0350 - binary_accuracy: 0.9889 - average_precision: 0.1377 - val_loss: 0.2740 - val_binary_accuracy: 0.9886 - val_average_precision: 0.1399\n",
            "Epoch 17/80\n",
            "14/14 [==============================] - 11s 749ms/step - loss: 0.0346 - binary_accuracy: 0.9889 - average_precision: 0.1464 - val_loss: 0.2529 - val_binary_accuracy: 0.9886 - val_average_precision: 0.1467\n",
            "Epoch 18/80\n",
            "14/14 [==============================] - 12s 868ms/step - loss: 0.0341 - binary_accuracy: 0.9890 - average_precision: 0.1557 - val_loss: 0.2345 - val_binary_accuracy: 0.9887 - val_average_precision: 0.1532\n",
            "Epoch 19/80\n",
            "14/14 [==============================] - 14s 992ms/step - loss: 0.0338 - binary_accuracy: 0.9891 - average_precision: 0.1637 - val_loss: 0.2168 - val_binary_accuracy: 0.9888 - val_average_precision: 0.1592\n",
            "Epoch 20/80\n",
            "14/14 [==============================] - 12s 879ms/step - loss: 0.0334 - binary_accuracy: 0.9891 - average_precision: 0.1724 - val_loss: 0.1989 - val_binary_accuracy: 0.9888 - val_average_precision: 0.1644\n",
            "Epoch 21/80\n",
            "14/14 [==============================] - 12s 856ms/step - loss: 0.0331 - binary_accuracy: 0.9892 - average_precision: 0.1797 - val_loss: 0.1834 - val_binary_accuracy: 0.9888 - val_average_precision: 0.1717\n",
            "Epoch 22/80\n",
            "14/14 [==============================] - 12s 841ms/step - loss: 0.0327 - binary_accuracy: 0.9893 - average_precision: 0.1889 - val_loss: 0.1670 - val_binary_accuracy: 0.9889 - val_average_precision: 0.1768\n",
            "Epoch 23/80\n",
            "14/14 [==============================] - 12s 807ms/step - loss: 0.0324 - binary_accuracy: 0.9893 - average_precision: 0.1956 - val_loss: 0.1523 - val_binary_accuracy: 0.9890 - val_average_precision: 0.1828\n",
            "Epoch 24/80\n",
            "14/14 [==============================] - 14s 986ms/step - loss: 0.0321 - binary_accuracy: 0.9894 - average_precision: 0.2040 - val_loss: 0.1385 - val_binary_accuracy: 0.9890 - val_average_precision: 0.1886\n",
            "Epoch 25/80\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0318 - binary_accuracy: 0.9895 - average_precision: 0.2117 - val_loss: 0.1277 - val_binary_accuracy: 0.9890 - val_average_precision: 0.1932\n",
            "Epoch 26/80\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0315 - binary_accuracy: 0.9895 - average_precision: 0.2191 - val_loss: 0.1153 - val_binary_accuracy: 0.9891 - val_average_precision: 0.1978\n",
            "Epoch 27/80\n",
            "14/14 [==============================] - 13s 921ms/step - loss: 0.0312 - binary_accuracy: 0.9896 - average_precision: 0.2267 - val_loss: 0.1048 - val_binary_accuracy: 0.9891 - val_average_precision: 0.2027\n",
            "Epoch 28/80\n",
            "14/14 [==============================] - 14s 992ms/step - loss: 0.0310 - binary_accuracy: 0.9897 - average_precision: 0.2335 - val_loss: 0.0962 - val_binary_accuracy: 0.9891 - val_average_precision: 0.2064\n",
            "Epoch 29/80\n",
            "14/14 [==============================] - 12s 861ms/step - loss: 0.0307 - binary_accuracy: 0.9897 - average_precision: 0.2399 - val_loss: 0.0869 - val_binary_accuracy: 0.9891 - val_average_precision: 0.2109\n",
            "Epoch 30/80\n",
            "14/14 [==============================] - 12s 858ms/step - loss: 0.0305 - binary_accuracy: 0.9898 - average_precision: 0.2471 - val_loss: 0.0796 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2154\n",
            "Epoch 31/80\n",
            "14/14 [==============================] - 10s 704ms/step - loss: 0.0302 - binary_accuracy: 0.9898 - average_precision: 0.2538 - val_loss: 0.0733 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2185\n",
            "Epoch 32/80\n",
            "14/14 [==============================] - 12s 839ms/step - loss: 0.0300 - binary_accuracy: 0.9899 - average_precision: 0.2601 - val_loss: 0.0666 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2216\n",
            "Epoch 33/80\n",
            "14/14 [==============================] - 12s 882ms/step - loss: 0.0297 - binary_accuracy: 0.9900 - average_precision: 0.2673 - val_loss: 0.0612 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2249\n",
            "Epoch 34/80\n",
            "14/14 [==============================] - 12s 859ms/step - loss: 0.0295 - binary_accuracy: 0.9901 - average_precision: 0.2741 - val_loss: 0.0559 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2272\n",
            "Epoch 35/80\n",
            "14/14 [==============================] - 11s 820ms/step - loss: 0.0292 - binary_accuracy: 0.9901 - average_precision: 0.2808 - val_loss: 0.0525 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2314\n",
            "Epoch 36/80\n",
            "14/14 [==============================] - 13s 895ms/step - loss: 0.0290 - binary_accuracy: 0.9902 - average_precision: 0.2859 - val_loss: 0.0492 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2354\n",
            "Epoch 37/80\n",
            "14/14 [==============================] - 11s 749ms/step - loss: 0.0288 - binary_accuracy: 0.9902 - average_precision: 0.2930 - val_loss: 0.0456 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2381\n",
            "Epoch 38/80\n",
            "14/14 [==============================] - 13s 901ms/step - loss: 0.0285 - binary_accuracy: 0.9903 - average_precision: 0.2979 - val_loss: 0.0436 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2395\n",
            "Epoch 39/80\n",
            "14/14 [==============================] - 13s 917ms/step - loss: 0.0283 - binary_accuracy: 0.9903 - average_precision: 0.3035 - val_loss: 0.0410 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2420\n",
            "Epoch 40/80\n",
            "14/14 [==============================] - 12s 891ms/step - loss: 0.0281 - binary_accuracy: 0.9904 - average_precision: 0.3110 - val_loss: 0.0390 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2462\n",
            "Epoch 41/80\n",
            "14/14 [==============================] - 13s 908ms/step - loss: 0.0279 - binary_accuracy: 0.9905 - average_precision: 0.3169 - val_loss: 0.0373 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2478\n",
            "Epoch 42/80\n",
            "14/14 [==============================] - 12s 880ms/step - loss: 0.0276 - binary_accuracy: 0.9905 - average_precision: 0.3229 - val_loss: 0.0362 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2498\n",
            "Epoch 43/80\n",
            "14/14 [==============================] - 11s 736ms/step - loss: 0.0274 - binary_accuracy: 0.9906 - average_precision: 0.3287 - val_loss: 0.0348 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2508\n",
            "Epoch 44/80\n",
            "14/14 [==============================] - 12s 864ms/step - loss: 0.0273 - binary_accuracy: 0.9906 - average_precision: 0.3329 - val_loss: 0.0342 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2529\n",
            "Epoch 45/80\n",
            "14/14 [==============================] - 12s 846ms/step - loss: 0.0271 - binary_accuracy: 0.9907 - average_precision: 0.3385 - val_loss: 0.0342 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2552\n",
            "Epoch 46/80\n",
            "14/14 [==============================] - 12s 870ms/step - loss: 0.0268 - binary_accuracy: 0.9908 - average_precision: 0.3433 - val_loss: 0.0333 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2572\n",
            "Epoch 47/80\n",
            "14/14 [==============================] - 13s 924ms/step - loss: 0.0266 - binary_accuracy: 0.9908 - average_precision: 0.3505 - val_loss: 0.0327 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2584\n",
            "Epoch 48/80\n",
            "14/14 [==============================] - 11s 789ms/step - loss: 0.0264 - binary_accuracy: 0.9909 - average_precision: 0.3566 - val_loss: 0.0326 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2609\n",
            "Epoch 49/80\n",
            "14/14 [==============================] - 11s 784ms/step - loss: 0.0262 - binary_accuracy: 0.9910 - average_precision: 0.3604 - val_loss: 0.0324 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2618\n",
            "Epoch 50/80\n",
            "14/14 [==============================] - 12s 847ms/step - loss: 0.0260 - binary_accuracy: 0.9910 - average_precision: 0.3656 - val_loss: 0.0323 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2645\n",
            "Epoch 51/80\n",
            "14/14 [==============================] - 12s 885ms/step - loss: 0.0259 - binary_accuracy: 0.9910 - average_precision: 0.3703 - val_loss: 0.0322 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2628\n",
            "Epoch 52/80\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0258 - binary_accuracy: 0.9911 - average_precision: 0.3734 - val_loss: 0.0322 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2650\n",
            "Epoch 53/80\n",
            "14/14 [==============================] - 13s 957ms/step - loss: 0.0256 - binary_accuracy: 0.9911 - average_precision: 0.3795 - val_loss: 0.0321 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2668\n",
            "Epoch 54/80\n",
            "14/14 [==============================] - 12s 872ms/step - loss: 0.0254 - binary_accuracy: 0.9912 - average_precision: 0.3844 - val_loss: 0.0321 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2657\n",
            "Epoch 55/80\n",
            "14/14 [==============================] - 13s 892ms/step - loss: 0.0252 - binary_accuracy: 0.9912 - average_precision: 0.3885 - val_loss: 0.0321 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2684\n",
            "Epoch 56/80\n",
            "14/14 [==============================] - 12s 833ms/step - loss: 0.0250 - binary_accuracy: 0.9913 - average_precision: 0.3941 - val_loss: 0.0322 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2686\n",
            "Epoch 57/80\n",
            "14/14 [==============================] - 13s 972ms/step - loss: 0.0248 - binary_accuracy: 0.9914 - average_precision: 0.3988 - val_loss: 0.0322 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2686\n",
            "Epoch 58/80\n",
            "14/14 [==============================] - 12s 903ms/step - loss: 0.0247 - binary_accuracy: 0.9914 - average_precision: 0.4018 - val_loss: 0.0323 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2705\n",
            "Epoch 59/80\n",
            "14/14 [==============================] - 13s 916ms/step - loss: 0.0246 - binary_accuracy: 0.9914 - average_precision: 0.4076 - val_loss: 0.0325 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2716\n",
            "Epoch 60/80\n",
            "14/14 [==============================] - 12s 906ms/step - loss: 0.0244 - binary_accuracy: 0.9915 - average_precision: 0.4103 - val_loss: 0.0325 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2721\n",
            "Epoch 61/80\n",
            "14/14 [==============================] - 13s 914ms/step - loss: 0.0243 - binary_accuracy: 0.9915 - average_precision: 0.4166 - val_loss: 0.0326 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2728\n",
            "Epoch 62/80\n",
            "14/14 [==============================] - 11s 822ms/step - loss: 0.0241 - binary_accuracy: 0.9916 - average_precision: 0.4196 - val_loss: 0.0328 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2719\n",
            "Epoch 63/80\n",
            "14/14 [==============================] - 11s 735ms/step - loss: 0.0240 - binary_accuracy: 0.9916 - average_precision: 0.4215 - val_loss: 0.0327 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2751\n",
            "Epoch 64/80\n",
            "14/14 [==============================] - 12s 875ms/step - loss: 0.0238 - binary_accuracy: 0.9917 - average_precision: 0.4287 - val_loss: 0.0328 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2770\n"
          ]
        }
      ],
      "source": [
        "INPUT_SHAPE = [train_df.shape[1]]\n",
        "BATCH_SIZE = 5120\n",
        "\n",
        "model_cc = tf.keras.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(input_shape=INPUT_SHAPE),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=len(labels_cc),activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model_cc.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy', tf.keras.metrics.AUC(multi_label=True, curve='PR', name='average_precision')]\n",
        ")\n",
        "\n",
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_auc',     # The performance metric to monitor\n",
        "    patience=10,            # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True # Restores model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "\n",
        "hist_cc = model_cc.fit(\n",
        "    X_train_cc.iloc[:,1:], y_train_cc,\n",
        "    validation_data=(X_val_cc.iloc[:,1:], y_val_cc),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=80,\n",
        "    callbacks=early_stopper\n",
        ")\n",
        "#val_loss: 10:0.40 min:0.0319 at 54"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COBKSUEICSA1"
      },
      "source": [
        "With Regularization and Dropout Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_average_precision',     # The performance metric to monitor\n",
        "    patience=10,            # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True, # Restores model weights from the epoch with the best value of the monitored quantity\n",
        "    min_delta=0.1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLlwXrrcm8Mo",
        "outputId": "9bc8d166-dc4b-4461-c7de-daadffc3a0f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "14/14 [==============================] - 64s 4s/step - loss: 0.2257 - binary_accuracy: 0.9189 - average_precision: 0.0200 - val_loss: 0.4990 - val_binary_accuracy: 0.9863 - val_average_precision: 0.0226\n",
            "Epoch 2/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0682 - binary_accuracy: 0.9864 - average_precision: 0.0234 - val_loss: 0.4233 - val_binary_accuracy: 0.9866 - val_average_precision: 0.0257\n",
            "Epoch 3/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0606 - binary_accuracy: 0.9870 - average_precision: 0.0296 - val_loss: 0.4068 - val_binary_accuracy: 0.9868 - val_average_precision: 0.0288\n",
            "Epoch 4/600\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.0571 - binary_accuracy: 0.9875 - average_precision: 0.0374 - val_loss: 0.3943 - val_binary_accuracy: 0.9871 - val_average_precision: 0.0368\n",
            "Epoch 5/600\n",
            "14/14 [==============================] - 21s 1s/step - loss: 0.0543 - binary_accuracy: 0.9878 - average_precision: 0.0474 - val_loss: 0.3848 - val_binary_accuracy: 0.9871 - val_average_precision: 0.0456\n",
            "Epoch 6/600\n",
            "14/14 [==============================] - 21s 1s/step - loss: 0.0519 - binary_accuracy: 0.9880 - average_precision: 0.0584 - val_loss: 0.3694 - val_binary_accuracy: 0.9871 - val_average_precision: 0.0554\n",
            "Epoch 7/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0497 - binary_accuracy: 0.9882 - average_precision: 0.0701 - val_loss: 0.3520 - val_binary_accuracy: 0.9873 - val_average_precision: 0.0681\n",
            "Epoch 8/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0479 - binary_accuracy: 0.9883 - average_precision: 0.0811 - val_loss: 0.3327 - val_binary_accuracy: 0.9874 - val_average_precision: 0.0808\n",
            "Epoch 9/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0464 - binary_accuracy: 0.9884 - average_precision: 0.0890 - val_loss: 0.3063 - val_binary_accuracy: 0.9873 - val_average_precision: 0.0911\n",
            "Epoch 10/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0452 - binary_accuracy: 0.9885 - average_precision: 0.0988 - val_loss: 0.2787 - val_binary_accuracy: 0.9874 - val_average_precision: 0.1009\n",
            "Epoch 11/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0441 - binary_accuracy: 0.9886 - average_precision: 0.1052 - val_loss: 0.2681 - val_binary_accuracy: 0.9876 - val_average_precision: 0.1093\n",
            "Epoch 12/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0432 - binary_accuracy: 0.9887 - average_precision: 0.1136 - val_loss: 0.2366 - val_binary_accuracy: 0.9875 - val_average_precision: 0.1137\n",
            "Epoch 13/600\n",
            "14/14 [==============================] - 14s 985ms/step - loss: 0.0424 - binary_accuracy: 0.9887 - average_precision: 0.1195 - val_loss: 0.2214 - val_binary_accuracy: 0.9876 - val_average_precision: 0.1214\n",
            "Epoch 14/600\n",
            "14/14 [==============================] - 13s 915ms/step - loss: 0.0417 - binary_accuracy: 0.9888 - average_precision: 0.1263 - val_loss: 0.2054 - val_binary_accuracy: 0.9876 - val_average_precision: 0.1259\n",
            "Epoch 15/600\n",
            "14/14 [==============================] - 13s 934ms/step - loss: 0.0410 - binary_accuracy: 0.9888 - average_precision: 0.1312 - val_loss: 0.1891 - val_binary_accuracy: 0.9879 - val_average_precision: 0.1307\n",
            "Epoch 16/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0405 - binary_accuracy: 0.9888 - average_precision: 0.1371 - val_loss: 0.1692 - val_binary_accuracy: 0.9878 - val_average_precision: 0.1384\n",
            "Epoch 17/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0400 - binary_accuracy: 0.9889 - average_precision: 0.1424 - val_loss: 0.1626 - val_binary_accuracy: 0.9878 - val_average_precision: 0.1442\n",
            "Epoch 18/600\n",
            "14/14 [==============================] - 21s 2s/step - loss: 0.0395 - binary_accuracy: 0.9889 - average_precision: 0.1476 - val_loss: 0.1460 - val_binary_accuracy: 0.9878 - val_average_precision: 0.1459\n",
            "Epoch 19/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0392 - binary_accuracy: 0.9889 - average_precision: 0.1512 - val_loss: 0.1354 - val_binary_accuracy: 0.9880 - val_average_precision: 0.1512\n",
            "Epoch 20/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0387 - binary_accuracy: 0.9890 - average_precision: 0.1544 - val_loss: 0.1250 - val_binary_accuracy: 0.9880 - val_average_precision: 0.1590\n",
            "Epoch 21/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0384 - binary_accuracy: 0.9890 - average_precision: 0.1594 - val_loss: 0.1153 - val_binary_accuracy: 0.9879 - val_average_precision: 0.1562\n",
            "Epoch 22/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0380 - binary_accuracy: 0.9890 - average_precision: 0.1660 - val_loss: 0.1067 - val_binary_accuracy: 0.9881 - val_average_precision: 0.1622\n",
            "Epoch 23/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0377 - binary_accuracy: 0.9891 - average_precision: 0.1706 - val_loss: 0.1005 - val_binary_accuracy: 0.9882 - val_average_precision: 0.1712\n",
            "Epoch 24/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0374 - binary_accuracy: 0.9891 - average_precision: 0.1740 - val_loss: 0.0934 - val_binary_accuracy: 0.9882 - val_average_precision: 0.1708\n",
            "Epoch 25/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0372 - binary_accuracy: 0.9891 - average_precision: 0.1777 - val_loss: 0.0876 - val_binary_accuracy: 0.9883 - val_average_precision: 0.1767\n",
            "Epoch 26/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0370 - binary_accuracy: 0.9891 - average_precision: 0.1787 - val_loss: 0.0801 - val_binary_accuracy: 0.9884 - val_average_precision: 0.1790\n",
            "Epoch 27/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0368 - binary_accuracy: 0.9892 - average_precision: 0.1826 - val_loss: 0.0775 - val_binary_accuracy: 0.9886 - val_average_precision: 0.1879\n",
            "Epoch 28/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0367 - binary_accuracy: 0.9892 - average_precision: 0.1837 - val_loss: 0.0732 - val_binary_accuracy: 0.9885 - val_average_precision: 0.1881\n",
            "Epoch 29/600\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.0365 - binary_accuracy: 0.9892 - average_precision: 0.1883 - val_loss: 0.0678 - val_binary_accuracy: 0.9886 - val_average_precision: 0.1880\n",
            "Epoch 30/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0362 - binary_accuracy: 0.9892 - average_precision: 0.1922 - val_loss: 0.0614 - val_binary_accuracy: 0.9887 - val_average_precision: 0.1927\n",
            "Epoch 31/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0360 - binary_accuracy: 0.9893 - average_precision: 0.1945 - val_loss: 0.0580 - val_binary_accuracy: 0.9886 - val_average_precision: 0.1902\n",
            "Epoch 32/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0361 - binary_accuracy: 0.9892 - average_precision: 0.1925 - val_loss: 0.0575 - val_binary_accuracy: 0.9886 - val_average_precision: 0.1941\n",
            "Epoch 33/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0359 - binary_accuracy: 0.9893 - average_precision: 0.1968 - val_loss: 0.0557 - val_binary_accuracy: 0.9889 - val_average_precision: 0.2000\n",
            "Epoch 34/600\n",
            "14/14 [==============================] - 21s 2s/step - loss: 0.0356 - binary_accuracy: 0.9893 - average_precision: 0.2001 - val_loss: 0.0505 - val_binary_accuracy: 0.9889 - val_average_precision: 0.2023\n",
            "Epoch 35/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0354 - binary_accuracy: 0.9894 - average_precision: 0.2050 - val_loss: 0.0471 - val_binary_accuracy: 0.9889 - val_average_precision: 0.2071\n",
            "Epoch 36/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0354 - binary_accuracy: 0.9893 - average_precision: 0.2068 - val_loss: 0.0453 - val_binary_accuracy: 0.9889 - val_average_precision: 0.2069\n",
            "Epoch 37/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0354 - binary_accuracy: 0.9893 - average_precision: 0.2078 - val_loss: 0.0454 - val_binary_accuracy: 0.9890 - val_average_precision: 0.2146\n",
            "Epoch 38/600\n",
            "14/14 [==============================] - 12s 884ms/step - loss: 0.0352 - binary_accuracy: 0.9894 - average_precision: 0.2106 - val_loss: 0.0431 - val_binary_accuracy: 0.9891 - val_average_precision: 0.2181\n",
            "Epoch 39/600\n",
            "14/14 [==============================] - 12s 901ms/step - loss: 0.0350 - binary_accuracy: 0.9894 - average_precision: 0.2137 - val_loss: 0.0405 - val_binary_accuracy: 0.9891 - val_average_precision: 0.2193\n",
            "Epoch 40/600\n",
            "14/14 [==============================] - 13s 907ms/step - loss: 0.0350 - binary_accuracy: 0.9894 - average_precision: 0.2153 - val_loss: 0.0405 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2243\n",
            "Epoch 41/600\n",
            "14/14 [==============================] - 12s 864ms/step - loss: 0.0348 - binary_accuracy: 0.9894 - average_precision: 0.2188 - val_loss: 0.0385 - val_binary_accuracy: 0.9891 - val_average_precision: 0.2187\n",
            "Epoch 42/600\n",
            "14/14 [==============================] - 12s 876ms/step - loss: 0.0348 - binary_accuracy: 0.9894 - average_precision: 0.2180 - val_loss: 0.0389 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2235\n",
            "Epoch 43/600\n",
            "14/14 [==============================] - 13s 906ms/step - loss: 0.0347 - binary_accuracy: 0.9894 - average_precision: 0.2187 - val_loss: 0.0387 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2286\n",
            "Epoch 44/600\n",
            "14/14 [==============================] - 12s 872ms/step - loss: 0.0346 - binary_accuracy: 0.9895 - average_precision: 0.2196 - val_loss: 0.0385 - val_binary_accuracy: 0.9891 - val_average_precision: 0.2311\n",
            "Epoch 45/600\n",
            "14/14 [==============================] - 13s 971ms/step - loss: 0.0346 - binary_accuracy: 0.9895 - average_precision: 0.2233 - val_loss: 0.0370 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2283\n",
            "Epoch 46/600\n",
            "14/14 [==============================] - 12s 903ms/step - loss: 0.0344 - binary_accuracy: 0.9895 - average_precision: 0.2279 - val_loss: 0.0365 - val_binary_accuracy: 0.9892 - val_average_precision: 0.2295\n",
            "Epoch 47/600\n",
            "14/14 [==============================] - 13s 952ms/step - loss: 0.0343 - binary_accuracy: 0.9895 - average_precision: 0.2290 - val_loss: 0.0365 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2295\n",
            "Epoch 48/600\n",
            "14/14 [==============================] - 12s 896ms/step - loss: 0.0344 - binary_accuracy: 0.9895 - average_precision: 0.2298 - val_loss: 0.0363 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2310\n",
            "Epoch 49/600\n",
            "14/14 [==============================] - 13s 940ms/step - loss: 0.0343 - binary_accuracy: 0.9895 - average_precision: 0.2311 - val_loss: 0.0364 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2388\n",
            "Epoch 50/600\n",
            "14/14 [==============================] - 13s 913ms/step - loss: 0.0343 - binary_accuracy: 0.9895 - average_precision: 0.2289 - val_loss: 0.0357 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2373\n",
            "Epoch 51/600\n",
            "14/14 [==============================] - 13s 957ms/step - loss: 0.0341 - binary_accuracy: 0.9895 - average_precision: 0.2343 - val_loss: 0.0355 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2392\n",
            "Epoch 52/600\n",
            "14/14 [==============================] - 13s 927ms/step - loss: 0.0341 - binary_accuracy: 0.9896 - average_precision: 0.2345 - val_loss: 0.0355 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2373\n",
            "Epoch 53/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0341 - binary_accuracy: 0.9896 - average_precision: 0.2359 - val_loss: 0.0355 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2378\n",
            "Epoch 54/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0340 - binary_accuracy: 0.9896 - average_precision: 0.2380 - val_loss: 0.0357 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2429\n",
            "Epoch 55/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0339 - binary_accuracy: 0.9896 - average_precision: 0.2385 - val_loss: 0.0355 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2431\n",
            "Epoch 56/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0339 - binary_accuracy: 0.9896 - average_precision: 0.2406 - val_loss: 0.0352 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2431\n",
            "Epoch 57/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0338 - binary_accuracy: 0.9896 - average_precision: 0.2458 - val_loss: 0.0351 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2454\n",
            "Epoch 58/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0337 - binary_accuracy: 0.9896 - average_precision: 0.2448 - val_loss: 0.0350 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2476\n",
            "Epoch 59/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0336 - binary_accuracy: 0.9896 - average_precision: 0.2475 - val_loss: 0.0351 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2449\n",
            "Epoch 60/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0336 - binary_accuracy: 0.9897 - average_precision: 0.2445 - val_loss: 0.0351 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2462\n",
            "Epoch 61/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0336 - binary_accuracy: 0.9896 - average_precision: 0.2471 - val_loss: 0.0350 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2464\n",
            "Epoch 62/600\n",
            "14/14 [==============================] - 14s 957ms/step - loss: 0.0336 - binary_accuracy: 0.9896 - average_precision: 0.2461 - val_loss: 0.0350 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2465\n",
            "Epoch 63/600\n",
            "14/14 [==============================] - 13s 974ms/step - loss: 0.0336 - binary_accuracy: 0.9897 - average_precision: 0.2482 - val_loss: 0.0350 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2488\n",
            "Epoch 64/600\n",
            "14/14 [==============================] - 13s 966ms/step - loss: 0.0334 - binary_accuracy: 0.9897 - average_precision: 0.2527 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2511\n",
            "Epoch 65/600\n",
            "14/14 [==============================] - 13s 952ms/step - loss: 0.0334 - binary_accuracy: 0.9897 - average_precision: 0.2514 - val_loss: 0.0351 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2464\n",
            "Epoch 66/600\n",
            "14/14 [==============================] - 13s 919ms/step - loss: 0.0335 - binary_accuracy: 0.9897 - average_precision: 0.2516 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2469\n",
            "Epoch 67/600\n",
            "14/14 [==============================] - 13s 935ms/step - loss: 0.0335 - binary_accuracy: 0.9897 - average_precision: 0.2505 - val_loss: 0.0351 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2456\n",
            "Epoch 68/600\n",
            "14/14 [==============================] - 14s 993ms/step - loss: 0.0333 - binary_accuracy: 0.9897 - average_precision: 0.2565 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2525\n",
            "Epoch 69/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0333 - binary_accuracy: 0.9897 - average_precision: 0.2535 - val_loss: 0.0350 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2480\n",
            "Epoch 70/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0333 - binary_accuracy: 0.9897 - average_precision: 0.2528 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2525\n",
            "Epoch 71/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0333 - binary_accuracy: 0.9897 - average_precision: 0.2556 - val_loss: 0.0350 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2521\n",
            "Epoch 72/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0333 - binary_accuracy: 0.9897 - average_precision: 0.2578 - val_loss: 0.0350 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2491\n",
            "Epoch 73/600\n",
            "14/14 [==============================] - 13s 906ms/step - loss: 0.0332 - binary_accuracy: 0.9897 - average_precision: 0.2602 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2538\n",
            "Epoch 74/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0332 - binary_accuracy: 0.9897 - average_precision: 0.2586 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2557\n",
            "Epoch 75/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0331 - binary_accuracy: 0.9898 - average_precision: 0.2609 - val_loss: 0.0347 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2557\n",
            "Epoch 76/600\n",
            "14/14 [==============================] - 13s 940ms/step - loss: 0.0330 - binary_accuracy: 0.9898 - average_precision: 0.2635 - val_loss: 0.0348 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2578\n",
            "Epoch 77/600\n",
            "14/14 [==============================] - 13s 972ms/step - loss: 0.0330 - binary_accuracy: 0.9898 - average_precision: 0.2651 - val_loss: 0.0348 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2584\n",
            "Epoch 78/600\n",
            "14/14 [==============================] - 13s 922ms/step - loss: 0.0330 - binary_accuracy: 0.9898 - average_precision: 0.2637 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2560\n",
            "Epoch 79/600\n",
            "14/14 [==============================] - 12s 896ms/step - loss: 0.0329 - binary_accuracy: 0.9898 - average_precision: 0.2667 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2578\n",
            "Epoch 80/600\n",
            "14/14 [==============================] - 13s 911ms/step - loss: 0.0329 - binary_accuracy: 0.9898 - average_precision: 0.2649 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2556\n",
            "Epoch 81/600\n",
            "14/14 [==============================] - 13s 958ms/step - loss: 0.0329 - binary_accuracy: 0.9898 - average_precision: 0.2673 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2585\n",
            "Epoch 82/600\n",
            "14/14 [==============================] - 13s 957ms/step - loss: 0.0328 - binary_accuracy: 0.9898 - average_precision: 0.2698 - val_loss: 0.0347 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2580\n",
            "Epoch 83/600\n",
            "14/14 [==============================] - 13s 941ms/step - loss: 0.0329 - binary_accuracy: 0.9898 - average_precision: 0.2676 - val_loss: 0.0348 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2577\n",
            "Epoch 84/600\n",
            "14/14 [==============================] - 13s 905ms/step - loss: 0.0328 - binary_accuracy: 0.9898 - average_precision: 0.2699 - val_loss: 0.0347 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2600\n",
            "Epoch 85/600\n",
            "14/14 [==============================] - 13s 902ms/step - loss: 0.0328 - binary_accuracy: 0.9899 - average_precision: 0.2703 - val_loss: 0.0351 - val_binary_accuracy: 0.9893 - val_average_precision: 0.2585\n",
            "Epoch 86/600\n",
            "14/14 [==============================] - 13s 954ms/step - loss: 0.0329 - binary_accuracy: 0.9898 - average_precision: 0.2666 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2586\n",
            "Epoch 87/600\n",
            "14/14 [==============================] - 13s 906ms/step - loss: 0.0329 - binary_accuracy: 0.9898 - average_precision: 0.2708 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2590\n",
            "Epoch 88/600\n",
            "14/14 [==============================] - 13s 924ms/step - loss: 0.0328 - binary_accuracy: 0.9898 - average_precision: 0.2713 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2599\n",
            "Epoch 89/600\n",
            "14/14 [==============================] - 13s 907ms/step - loss: 0.0328 - binary_accuracy: 0.9899 - average_precision: 0.2747 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2607\n",
            "Epoch 90/600\n",
            "14/14 [==============================] - 12s 899ms/step - loss: 0.0328 - binary_accuracy: 0.9899 - average_precision: 0.2695 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2618\n",
            "Epoch 91/600\n",
            "14/14 [==============================] - 13s 906ms/step - loss: 0.0327 - binary_accuracy: 0.9899 - average_precision: 0.2756 - val_loss: 0.0348 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2620\n",
            "Epoch 92/600\n",
            "14/14 [==============================] - 13s 955ms/step - loss: 0.0326 - binary_accuracy: 0.9899 - average_precision: 0.2771 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2661\n",
            "Epoch 93/600\n",
            "14/14 [==============================] - 13s 903ms/step - loss: 0.0327 - binary_accuracy: 0.9899 - average_precision: 0.2756 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2621\n",
            "Epoch 94/600\n",
            "14/14 [==============================] - 14s 998ms/step - loss: 0.0326 - binary_accuracy: 0.9899 - average_precision: 0.2746 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2661\n",
            "Epoch 95/600\n",
            "14/14 [==============================] - 13s 949ms/step - loss: 0.0326 - binary_accuracy: 0.9899 - average_precision: 0.2770 - val_loss: 0.0348 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2640\n",
            "Epoch 96/600\n",
            "14/14 [==============================] - 13s 950ms/step - loss: 0.0326 - binary_accuracy: 0.9899 - average_precision: 0.2751 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2672\n",
            "Epoch 97/600\n",
            "14/14 [==============================] - 13s 906ms/step - loss: 0.0326 - binary_accuracy: 0.9899 - average_precision: 0.2774 - val_loss: 0.0350 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2610\n",
            "Epoch 98/600\n",
            "14/14 [==============================] - 12s 901ms/step - loss: 0.0327 - binary_accuracy: 0.9899 - average_precision: 0.2756 - val_loss: 0.0349 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2654\n",
            "Epoch 99/600\n",
            "14/14 [==============================] - 13s 911ms/step - loss: 0.0326 - binary_accuracy: 0.9899 - average_precision: 0.2808 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2660\n",
            "Epoch 100/600\n",
            "14/14 [==============================] - 13s 921ms/step - loss: 0.0326 - binary_accuracy: 0.9899 - average_precision: 0.2784 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2681\n",
            "Epoch 101/600\n",
            "14/14 [==============================] - 14s 995ms/step - loss: 0.0324 - binary_accuracy: 0.9899 - average_precision: 0.2818 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2664\n",
            "Epoch 102/600\n",
            "14/14 [==============================] - 13s 932ms/step - loss: 0.0325 - binary_accuracy: 0.9899 - average_precision: 0.2813 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2674\n",
            "Epoch 103/600\n",
            "14/14 [==============================] - 13s 920ms/step - loss: 0.0325 - binary_accuracy: 0.9899 - average_precision: 0.2796 - val_loss: 0.0347 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2635\n",
            "Epoch 104/600\n",
            "14/14 [==============================] - 13s 910ms/step - loss: 0.0325 - binary_accuracy: 0.9899 - average_precision: 0.2794 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2665\n",
            "Epoch 105/600\n",
            "14/14 [==============================] - 13s 910ms/step - loss: 0.0324 - binary_accuracy: 0.9900 - average_precision: 0.2815 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2698\n",
            "Epoch 106/600\n",
            "14/14 [==============================] - 13s 909ms/step - loss: 0.0324 - binary_accuracy: 0.9900 - average_precision: 0.2860 - val_loss: 0.0350 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2655\n",
            "Epoch 107/600\n",
            "14/14 [==============================] - 12s 900ms/step - loss: 0.0325 - binary_accuracy: 0.9899 - average_precision: 0.2792 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2695\n",
            "Epoch 108/600\n",
            "14/14 [==============================] - 12s 895ms/step - loss: 0.0323 - binary_accuracy: 0.9900 - average_precision: 0.2860 - val_loss: 0.0347 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2650\n",
            "Epoch 109/600\n",
            "14/14 [==============================] - 13s 913ms/step - loss: 0.0324 - binary_accuracy: 0.9900 - average_precision: 0.2819 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2686\n",
            "Epoch 110/600\n",
            "14/14 [==============================] - 13s 901ms/step - loss: 0.0324 - binary_accuracy: 0.9900 - average_precision: 0.2848 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2676\n",
            "Epoch 111/600\n",
            "14/14 [==============================] - 12s 902ms/step - loss: 0.0323 - binary_accuracy: 0.9900 - average_precision: 0.2849 - val_loss: 0.0346 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2667\n",
            "Epoch 112/600\n",
            "14/14 [==============================] - 13s 915ms/step - loss: 0.0324 - binary_accuracy: 0.9899 - average_precision: 0.2849 - val_loss: 0.0348 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2722\n",
            "Epoch 113/600\n",
            "14/14 [==============================] - 13s 905ms/step - loss: 0.0325 - binary_accuracy: 0.9899 - average_precision: 0.2821 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2679\n",
            "Epoch 114/600\n",
            "14/14 [==============================] - 13s 926ms/step - loss: 0.0323 - binary_accuracy: 0.9900 - average_precision: 0.2863 - val_loss: 0.0350 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2665\n",
            "Epoch 115/600\n",
            "14/14 [==============================] - 13s 920ms/step - loss: 0.0323 - binary_accuracy: 0.9900 - average_precision: 0.2868 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2708\n",
            "Epoch 116/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0322 - binary_accuracy: 0.9900 - average_precision: 0.2886 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2725\n",
            "Epoch 117/600\n",
            "14/14 [==============================] - 13s 961ms/step - loss: 0.0322 - binary_accuracy: 0.9900 - average_precision: 0.2891 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2726\n",
            "Epoch 118/600\n",
            "14/14 [==============================] - 13s 905ms/step - loss: 0.0321 - binary_accuracy: 0.9900 - average_precision: 0.2899 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2694\n",
            "Epoch 119/600\n",
            "14/14 [==============================] - 13s 908ms/step - loss: 0.0322 - binary_accuracy: 0.9900 - average_precision: 0.2915 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2678\n",
            "Epoch 120/600\n",
            "14/14 [==============================] - 13s 901ms/step - loss: 0.0322 - binary_accuracy: 0.9900 - average_precision: 0.2899 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2727\n",
            "Epoch 121/600\n",
            "14/14 [==============================] - 12s 895ms/step - loss: 0.0322 - binary_accuracy: 0.9900 - average_precision: 0.2891 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2718\n",
            "Epoch 122/600\n",
            "14/14 [==============================] - 13s 938ms/step - loss: 0.0322 - binary_accuracy: 0.9900 - average_precision: 0.2906 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2706\n",
            "Epoch 123/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0321 - binary_accuracy: 0.9900 - average_precision: 0.2895 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2734\n",
            "Epoch 124/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0320 - binary_accuracy: 0.9901 - average_precision: 0.2936 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2698\n",
            "Epoch 125/600\n",
            "14/14 [==============================] - 13s 948ms/step - loss: 0.0321 - binary_accuracy: 0.9900 - average_precision: 0.2925 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2711\n",
            "Epoch 126/600\n",
            "14/14 [==============================] - 13s 904ms/step - loss: 0.0321 - binary_accuracy: 0.9900 - average_precision: 0.2945 - val_loss: 0.0346 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2709\n",
            "Epoch 127/600\n",
            "14/14 [==============================] - 13s 920ms/step - loss: 0.0322 - binary_accuracy: 0.9900 - average_precision: 0.2924 - val_loss: 0.0348 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2721\n",
            "Epoch 128/600\n",
            "14/14 [==============================] - 13s 900ms/step - loss: 0.0321 - binary_accuracy: 0.9900 - average_precision: 0.2946 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2733\n",
            "Epoch 129/600\n",
            "14/14 [==============================] - 12s 900ms/step - loss: 0.0320 - binary_accuracy: 0.9901 - average_precision: 0.2953 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2749\n",
            "Epoch 130/600\n",
            "14/14 [==============================] - 13s 919ms/step - loss: 0.0320 - binary_accuracy: 0.9901 - average_precision: 0.2935 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2765\n",
            "Epoch 131/600\n",
            "14/14 [==============================] - 13s 940ms/step - loss: 0.0320 - binary_accuracy: 0.9901 - average_precision: 0.2972 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2740\n",
            "Epoch 132/600\n",
            "14/14 [==============================] - 13s 939ms/step - loss: 0.0319 - binary_accuracy: 0.9901 - average_precision: 0.2973 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2762\n",
            "Epoch 133/600\n",
            "14/14 [==============================] - 13s 918ms/step - loss: 0.0320 - binary_accuracy: 0.9901 - average_precision: 0.2966 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2716\n",
            "Epoch 134/600\n",
            "14/14 [==============================] - 12s 891ms/step - loss: 0.0320 - binary_accuracy: 0.9901 - average_precision: 0.2974 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2754\n",
            "Epoch 135/600\n",
            "14/14 [==============================] - 13s 902ms/step - loss: 0.0320 - binary_accuracy: 0.9900 - average_precision: 0.2969 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2750\n",
            "Epoch 136/600\n",
            "14/14 [==============================] - 13s 952ms/step - loss: 0.0320 - binary_accuracy: 0.9901 - average_precision: 0.3006 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2764\n",
            "Epoch 137/600\n",
            "14/14 [==============================] - 13s 905ms/step - loss: 0.0320 - binary_accuracy: 0.9901 - average_precision: 0.2981 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2769\n",
            "Epoch 138/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0319 - binary_accuracy: 0.9901 - average_precision: 0.3000 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2766\n",
            "Epoch 139/600\n",
            "14/14 [==============================] - 13s 961ms/step - loss: 0.0319 - binary_accuracy: 0.9901 - average_precision: 0.3004 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2730\n",
            "Epoch 140/600\n",
            "14/14 [==============================] - 13s 967ms/step - loss: 0.0319 - binary_accuracy: 0.9901 - average_precision: 0.3009 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2770\n",
            "Epoch 141/600\n",
            "14/14 [==============================] - 14s 974ms/step - loss: 0.0319 - binary_accuracy: 0.9901 - average_precision: 0.2984 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2760\n",
            "Epoch 142/600\n",
            "14/14 [==============================] - 13s 959ms/step - loss: 0.0321 - binary_accuracy: 0.9900 - average_precision: 0.2956 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2767\n",
            "Epoch 143/600\n",
            "14/14 [==============================] - 13s 920ms/step - loss: 0.0319 - binary_accuracy: 0.9901 - average_precision: 0.2995 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2724\n",
            "Epoch 144/600\n",
            "14/14 [==============================] - 12s 888ms/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3015 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2780\n",
            "Epoch 145/600\n",
            "14/14 [==============================] - 12s 899ms/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3036 - val_loss: 0.0347 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2766\n",
            "Epoch 146/600\n",
            "14/14 [==============================] - 12s 890ms/step - loss: 0.0319 - binary_accuracy: 0.9901 - average_precision: 0.3002 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2788\n",
            "Epoch 147/600\n",
            "14/14 [==============================] - 13s 937ms/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3024 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2790\n",
            "Epoch 148/600\n",
            "14/14 [==============================] - 13s 926ms/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3011 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2739\n",
            "Epoch 149/600\n",
            "14/14 [==============================] - 13s 902ms/step - loss: 0.0319 - binary_accuracy: 0.9901 - average_precision: 0.3025 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2779\n",
            "Epoch 150/600\n",
            "14/14 [==============================] - 13s 912ms/step - loss: 0.0317 - binary_accuracy: 0.9901 - average_precision: 0.3066 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2770\n",
            "Epoch 151/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3002 - val_loss: 0.0346 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2798\n",
            "Epoch 152/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3034 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2802\n",
            "Epoch 153/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3040 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2766\n",
            "Epoch 154/600\n",
            "14/14 [==============================] - 14s 990ms/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3044 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2780\n",
            "Epoch 155/600\n",
            "14/14 [==============================] - 13s 930ms/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3033 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2768\n",
            "Epoch 156/600\n",
            "14/14 [==============================] - 13s 934ms/step - loss: 0.0318 - binary_accuracy: 0.9901 - average_precision: 0.3015 - val_loss: 0.0348 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2781\n",
            "Epoch 157/600\n",
            "14/14 [==============================] - 13s 912ms/step - loss: 0.0319 - binary_accuracy: 0.9901 - average_precision: 0.3004 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2802\n",
            "Epoch 158/600\n",
            "14/14 [==============================] - 12s 888ms/step - loss: 0.0317 - binary_accuracy: 0.9901 - average_precision: 0.3072 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2808\n",
            "Epoch 159/600\n",
            "14/14 [==============================] - 12s 898ms/step - loss: 0.0317 - binary_accuracy: 0.9902 - average_precision: 0.3074 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2799\n",
            "Epoch 160/600\n",
            "14/14 [==============================] - 12s 892ms/step - loss: 0.0317 - binary_accuracy: 0.9901 - average_precision: 0.3032 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2795\n",
            "Epoch 161/600\n",
            "14/14 [==============================] - 12s 903ms/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3085 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2831\n",
            "Epoch 162/600\n",
            "14/14 [==============================] - 13s 922ms/step - loss: 0.0317 - binary_accuracy: 0.9902 - average_precision: 0.3059 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2821\n",
            "Epoch 163/600\n",
            "14/14 [==============================] - 13s 955ms/step - loss: 0.0317 - binary_accuracy: 0.9902 - average_precision: 0.3108 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2808\n",
            "Epoch 164/600\n",
            "14/14 [==============================] - 12s 888ms/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3074 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2833\n",
            "Epoch 165/600\n",
            "14/14 [==============================] - 13s 906ms/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3109 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2788\n",
            "Epoch 166/600\n",
            "14/14 [==============================] - 13s 908ms/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3076 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2797\n",
            "Epoch 167/600\n",
            "14/14 [==============================] - 13s 919ms/step - loss: 0.0317 - binary_accuracy: 0.9902 - average_precision: 0.3068 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2820\n",
            "Epoch 168/600\n",
            "14/14 [==============================] - 12s 889ms/step - loss: 0.0317 - binary_accuracy: 0.9901 - average_precision: 0.3063 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2837\n",
            "Epoch 169/600\n",
            "14/14 [==============================] - 13s 932ms/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3076 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2819\n",
            "Epoch 170/600\n",
            "14/14 [==============================] - 13s 936ms/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3108 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2801\n",
            "Epoch 171/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3083 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2875\n",
            "Epoch 172/600\n",
            "14/14 [==============================] - 13s 913ms/step - loss: 0.0317 - binary_accuracy: 0.9902 - average_precision: 0.3076 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2841\n",
            "Epoch 173/600\n",
            "14/14 [==============================] - 13s 932ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3110 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2843\n",
            "Epoch 174/600\n",
            "14/14 [==============================] - 12s 895ms/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3118 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2820\n",
            "Epoch 175/600\n",
            "14/14 [==============================] - 12s 896ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3151 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2797\n",
            "Epoch 176/600\n",
            "14/14 [==============================] - 12s 899ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3122 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2817\n",
            "Epoch 177/600\n",
            "14/14 [==============================] - 13s 918ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3139 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2841\n",
            "Epoch 178/600\n",
            "14/14 [==============================] - 13s 902ms/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3092 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2850\n",
            "Epoch 179/600\n",
            "14/14 [==============================] - 13s 905ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3132 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2850\n",
            "Epoch 180/600\n",
            "14/14 [==============================] - 12s 881ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3136 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2862\n",
            "Epoch 181/600\n",
            "14/14 [==============================] - 12s 882ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3116 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2838\n",
            "Epoch 182/600\n",
            "14/14 [==============================] - 12s 889ms/step - loss: 0.0316 - binary_accuracy: 0.9902 - average_precision: 0.3121 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2822\n",
            "Epoch 183/600\n",
            "14/14 [==============================] - 13s 917ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3124 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2819\n",
            "Epoch 184/600\n",
            "14/14 [==============================] - 12s 883ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3155 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2821\n",
            "Epoch 185/600\n",
            "14/14 [==============================] - 12s 901ms/step - loss: 0.0314 - binary_accuracy: 0.9903 - average_precision: 0.3152 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2859\n",
            "Epoch 186/600\n",
            "14/14 [==============================] - 13s 919ms/step - loss: 0.0313 - binary_accuracy: 0.9903 - average_precision: 0.3187 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2856\n",
            "Epoch 187/600\n",
            "14/14 [==============================] - 13s 964ms/step - loss: 0.0314 - binary_accuracy: 0.9903 - average_precision: 0.3186 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2829\n",
            "Epoch 188/600\n",
            "14/14 [==============================] - 13s 930ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3174 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2836\n",
            "Epoch 189/600\n",
            "14/14 [==============================] - 13s 915ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3149 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2885\n",
            "Epoch 190/600\n",
            "14/14 [==============================] - 13s 913ms/step - loss: 0.0313 - binary_accuracy: 0.9903 - average_precision: 0.3173 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2866\n",
            "Epoch 191/600\n",
            "14/14 [==============================] - 12s 894ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3135 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2854\n",
            "Epoch 192/600\n",
            "14/14 [==============================] - 12s 877ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3169 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2835\n",
            "Epoch 193/600\n",
            "14/14 [==============================] - 13s 977ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3144 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2853\n",
            "Epoch 194/600\n",
            "14/14 [==============================] - 12s 900ms/step - loss: 0.0313 - binary_accuracy: 0.9903 - average_precision: 0.3188 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2885\n",
            "Epoch 195/600\n",
            "14/14 [==============================] - 14s 997ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3185 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2859\n",
            "Epoch 196/600\n",
            "14/14 [==============================] - 13s 898ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3142 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2857\n",
            "Epoch 197/600\n",
            "14/14 [==============================] - 13s 941ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3152 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2860\n",
            "Epoch 198/600\n",
            "14/14 [==============================] - 13s 915ms/step - loss: 0.0314 - binary_accuracy: 0.9903 - average_precision: 0.3175 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2875\n",
            "Epoch 199/600\n",
            "14/14 [==============================] - 12s 881ms/step - loss: 0.0315 - binary_accuracy: 0.9902 - average_precision: 0.3156 - val_loss: 0.0347 - val_binary_accuracy: 0.9894 - val_average_precision: 0.2836\n",
            "Epoch 200/600\n",
            "14/14 [==============================] - 12s 886ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3148 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2845\n",
            "Epoch 201/600\n",
            "14/14 [==============================] - 13s 906ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3151 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2883\n",
            "Epoch 202/600\n",
            "14/14 [==============================] - 13s 951ms/step - loss: 0.0313 - binary_accuracy: 0.9902 - average_precision: 0.3164 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2868\n",
            "Epoch 203/600\n",
            "14/14 [==============================] - 12s 883ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3203 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2853\n",
            "Epoch 204/600\n",
            "14/14 [==============================] - 12s 878ms/step - loss: 0.0314 - binary_accuracy: 0.9902 - average_precision: 0.3195 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2877\n",
            "Epoch 205/600\n",
            "14/14 [==============================] - 12s 891ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3228 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2871\n",
            "Epoch 206/600\n",
            "14/14 [==============================] - 13s 906ms/step - loss: 0.0313 - binary_accuracy: 0.9903 - average_precision: 0.3196 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2836\n",
            "Epoch 207/600\n",
            "14/14 [==============================] - 12s 884ms/step - loss: 0.0313 - binary_accuracy: 0.9903 - average_precision: 0.3221 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2901\n",
            "Epoch 208/600\n",
            "14/14 [==============================] - 12s 872ms/step - loss: 0.0313 - binary_accuracy: 0.9903 - average_precision: 0.3210 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2895\n",
            "Epoch 209/600\n",
            "14/14 [==============================] - 12s 878ms/step - loss: 0.0313 - binary_accuracy: 0.9903 - average_precision: 0.3245 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2895\n",
            "Epoch 210/600\n",
            "14/14 [==============================] - 12s 878ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3258 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2846\n",
            "Epoch 211/600\n",
            "14/14 [==============================] - 13s 910ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3222 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2907\n",
            "Epoch 212/600\n",
            "14/14 [==============================] - 13s 949ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3253 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2906\n",
            "Epoch 213/600\n",
            "14/14 [==============================] - 13s 916ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3238 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2893\n",
            "Epoch 214/600\n",
            "14/14 [==============================] - 12s 876ms/step - loss: 0.0313 - binary_accuracy: 0.9903 - average_precision: 0.3236 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2870\n",
            "Epoch 215/600\n",
            "14/14 [==============================] - 12s 883ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3215 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2873\n",
            "Epoch 216/600\n",
            "14/14 [==============================] - 13s 908ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3239 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2849\n",
            "Epoch 217/600\n",
            "14/14 [==============================] - 13s 939ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3222 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2846\n",
            "Epoch 218/600\n",
            "14/14 [==============================] - 12s 884ms/step - loss: 0.0313 - binary_accuracy: 0.9903 - average_precision: 0.3228 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2901\n",
            "Epoch 219/600\n",
            "14/14 [==============================] - 13s 932ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3218 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2869\n",
            "Epoch 220/600\n",
            "14/14 [==============================] - 13s 897ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3244 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2880\n",
            "Epoch 221/600\n",
            "14/14 [==============================] - 13s 904ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3263 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2915\n",
            "Epoch 222/600\n",
            "14/14 [==============================] - 13s 938ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3271 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2881\n",
            "Epoch 223/600\n",
            "14/14 [==============================] - 13s 903ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3236 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2920\n",
            "Epoch 224/600\n",
            "14/14 [==============================] - 12s 902ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3305 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2894\n",
            "Epoch 225/600\n",
            "14/14 [==============================] - 13s 924ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3209 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2897\n",
            "Epoch 226/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3207 - val_loss: 0.0343 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2865\n",
            "Epoch 227/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3251 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2933\n",
            "Epoch 228/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3289 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2885\n",
            "Epoch 229/600\n",
            "14/14 [==============================] - 14s 993ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3236 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2892\n",
            "Epoch 230/600\n",
            "14/14 [==============================] - 14s 996ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3294 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2870\n",
            "Epoch 231/600\n",
            "14/14 [==============================] - 12s 885ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3220 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2911\n",
            "Epoch 232/600\n",
            "14/14 [==============================] - 13s 922ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3296 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2924\n",
            "Epoch 233/600\n",
            "14/14 [==============================] - 13s 928ms/step - loss: 0.0311 - binary_accuracy: 0.9904 - average_precision: 0.3247 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2939\n",
            "Epoch 234/600\n",
            "14/14 [==============================] - 13s 955ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3300 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2930\n",
            "Epoch 235/600\n",
            "14/14 [==============================] - 13s 938ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3240 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2901\n",
            "Epoch 236/600\n",
            "14/14 [==============================] - 13s 928ms/step - loss: 0.0312 - binary_accuracy: 0.9903 - average_precision: 0.3262 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2940\n",
            "Epoch 237/600\n",
            "14/14 [==============================] - 12s 901ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3290 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2922\n",
            "Epoch 238/600\n",
            "14/14 [==============================] - 12s 884ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3289 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2936\n",
            "Epoch 239/600\n",
            "14/14 [==============================] - 12s 873ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3265 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2931\n",
            "Epoch 240/600\n",
            "14/14 [==============================] - 13s 920ms/step - loss: 0.0311 - binary_accuracy: 0.9904 - average_precision: 0.3300 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2942\n",
            "Epoch 241/600\n",
            "14/14 [==============================] - 12s 875ms/step - loss: 0.0311 - binary_accuracy: 0.9904 - average_precision: 0.3292 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2918\n",
            "Epoch 242/600\n",
            "14/14 [==============================] - 13s 905ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3321 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2874\n",
            "Epoch 243/600\n",
            "14/14 [==============================] - 12s 897ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3306 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2901\n",
            "Epoch 244/600\n",
            "14/14 [==============================] - 13s 903ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3323 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2931\n",
            "Epoch 245/600\n",
            "14/14 [==============================] - 12s 882ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3304 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2913\n",
            "Epoch 246/600\n",
            "14/14 [==============================] - 13s 922ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3287 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2931\n",
            "Epoch 247/600\n",
            "14/14 [==============================] - 12s 902ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3310 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2939\n",
            "Epoch 248/600\n",
            "14/14 [==============================] - 13s 911ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3294 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2926\n",
            "Epoch 249/600\n",
            "14/14 [==============================] - 12s 902ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3358 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2905\n",
            "Epoch 250/600\n",
            "14/14 [==============================] - 12s 896ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3301 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2902\n",
            "Epoch 251/600\n",
            "14/14 [==============================] - 13s 933ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3332 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2975\n",
            "Epoch 252/600\n",
            "14/14 [==============================] - 12s 892ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3324 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2933\n",
            "Epoch 253/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3347 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2931\n",
            "Epoch 254/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3320 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2971\n",
            "Epoch 255/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3358 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2929\n",
            "Epoch 256/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3324 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2926\n",
            "Epoch 257/600\n",
            "14/14 [==============================] - 13s 951ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3350 - val_loss: 0.0347 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2904\n",
            "Epoch 258/600\n",
            "14/14 [==============================] - 13s 927ms/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3296 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2997\n",
            "Epoch 259/600\n",
            "14/14 [==============================] - 13s 910ms/step - loss: 0.0311 - binary_accuracy: 0.9904 - average_precision: 0.3334 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2958\n",
            "Epoch 260/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0311 - binary_accuracy: 0.9903 - average_precision: 0.3286 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2936\n",
            "Epoch 261/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3326 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2971\n",
            "Epoch 262/600\n",
            "14/14 [==============================] - 13s 937ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3359 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2900\n",
            "Epoch 263/600\n",
            "14/14 [==============================] - 13s 904ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3343 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2942\n",
            "Epoch 264/600\n",
            "14/14 [==============================] - 13s 921ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3322 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2967\n",
            "Epoch 265/600\n",
            "14/14 [==============================] - 13s 931ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3314 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2982\n",
            "Epoch 266/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3390 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2925\n",
            "Epoch 267/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3347 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2954\n",
            "Epoch 268/600\n",
            "14/14 [==============================] - 13s 948ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3351 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2961\n",
            "Epoch 269/600\n",
            "14/14 [==============================] - 13s 965ms/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3334 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2984\n",
            "Epoch 270/600\n",
            "14/14 [==============================] - 12s 899ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3370 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2921\n",
            "Epoch 271/600\n",
            "14/14 [==============================] - 13s 922ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3334 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2992\n",
            "Epoch 272/600\n",
            "14/14 [==============================] - 13s 924ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3379 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2988\n",
            "Epoch 273/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0310 - binary_accuracy: 0.9904 - average_precision: 0.3314 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2975\n",
            "Epoch 274/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3370 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2968\n",
            "Epoch 275/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3354 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2967\n",
            "Epoch 276/600\n",
            "14/14 [==============================] - 13s 932ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3347 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2951\n",
            "Epoch 277/600\n",
            "14/14 [==============================] - 13s 913ms/step - loss: 0.0308 - binary_accuracy: 0.9905 - average_precision: 0.3409 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2927\n",
            "Epoch 278/600\n",
            "14/14 [==============================] - 13s 974ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3368 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2953\n",
            "Epoch 279/600\n",
            "14/14 [==============================] - 14s 983ms/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3401 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2952\n",
            "Epoch 280/600\n",
            "14/14 [==============================] - 14s 986ms/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3415 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2961\n",
            "Epoch 281/600\n",
            "14/14 [==============================] - 13s 970ms/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3343 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3001\n",
            "Epoch 282/600\n",
            "14/14 [==============================] - 13s 925ms/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3389 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2989\n",
            "Epoch 283/600\n",
            "14/14 [==============================] - 13s 911ms/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3406 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.2950\n",
            "Epoch 284/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3336 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2956\n",
            "Epoch 285/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3390 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2959\n",
            "Epoch 286/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3332 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2934\n",
            "Epoch 287/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3392 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2968\n",
            "Epoch 288/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3407 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2990\n",
            "Epoch 289/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3427 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2951\n",
            "Epoch 290/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3379 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2990\n",
            "Epoch 291/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3464 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2984\n",
            "Epoch 292/600\n",
            "14/14 [==============================] - 21s 1s/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3417 - val_loss: 0.0344 - val_binary_accuracy: 0.9895 - val_average_precision: 0.3001\n",
            "Epoch 293/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3378 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2952\n",
            "Epoch 294/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0309 - binary_accuracy: 0.9904 - average_precision: 0.3380 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3002\n",
            "Epoch 295/600\n",
            "14/14 [==============================] - 14s 979ms/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3420 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2987\n",
            "Epoch 296/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3455 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2974\n",
            "Epoch 297/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3429 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2982\n",
            "Epoch 298/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3405 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3014\n",
            "Epoch 299/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3424 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3004\n",
            "Epoch 300/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3432 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2966\n",
            "Epoch 301/600\n",
            "14/14 [==============================] - 13s 968ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3451 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2989\n",
            "Epoch 302/600\n",
            "14/14 [==============================] - 13s 973ms/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3439 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3000\n",
            "Epoch 303/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3436 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2987\n",
            "Epoch 304/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3436 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2986\n",
            "Epoch 305/600\n",
            "14/14 [==============================] - 13s 960ms/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3416 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2960\n",
            "Epoch 306/600\n",
            "14/14 [==============================] - 13s 957ms/step - loss: 0.0308 - binary_accuracy: 0.9905 - average_precision: 0.3396 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2963\n",
            "Epoch 307/600\n",
            "14/14 [==============================] - 14s 996ms/step - loss: 0.0308 - binary_accuracy: 0.9904 - average_precision: 0.3372 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3011\n",
            "Epoch 308/600\n",
            "14/14 [==============================] - 13s 941ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3451 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3001\n",
            "Epoch 309/600\n",
            "14/14 [==============================] - 14s 982ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3439 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3012\n",
            "Epoch 310/600\n",
            "14/14 [==============================] - 13s 947ms/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3505 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2996\n",
            "Epoch 311/600\n",
            "14/14 [==============================] - 13s 942ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3436 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2985\n",
            "Epoch 312/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3487 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2990\n",
            "Epoch 313/600\n",
            "14/14 [==============================] - 14s 978ms/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3454 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2943\n",
            "Epoch 314/600\n",
            "14/14 [==============================] - 13s 960ms/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3433 - val_loss: 0.0347 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2950\n",
            "Epoch 315/600\n",
            "14/14 [==============================] - 14s 979ms/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3439 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3006\n",
            "Epoch 316/600\n",
            "14/14 [==============================] - 14s 977ms/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3431 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2968\n",
            "Epoch 317/600\n",
            "14/14 [==============================] - 14s 989ms/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3459 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2997\n",
            "Epoch 318/600\n",
            "14/14 [==============================] - 14s 987ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3474 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2998\n",
            "Epoch 319/600\n",
            "14/14 [==============================] - 14s 995ms/step - loss: 0.0307 - binary_accuracy: 0.9904 - average_precision: 0.3418 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2999\n",
            "Epoch 320/600\n",
            "14/14 [==============================] - 13s 969ms/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3455 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3049\n",
            "Epoch 321/600\n",
            "14/14 [==============================] - 14s 992ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3448 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2979\n",
            "Epoch 322/600\n",
            "14/14 [==============================] - 14s 990ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3449 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3003\n",
            "Epoch 323/600\n",
            "14/14 [==============================] - 13s 966ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3479 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3013\n",
            "Epoch 324/600\n",
            "14/14 [==============================] - 14s 975ms/step - loss: 0.0307 - binary_accuracy: 0.9904 - average_precision: 0.3441 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3008\n",
            "Epoch 325/600\n",
            "14/14 [==============================] - 14s 985ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3471 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3018\n",
            "Epoch 326/600\n",
            "14/14 [==============================] - 13s 959ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3458 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2993\n",
            "Epoch 327/600\n",
            "14/14 [==============================] - 13s 960ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3490 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2997\n",
            "Epoch 328/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3487 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3060\n",
            "Epoch 329/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3469 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3011\n",
            "Epoch 330/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3515 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3004\n",
            "Epoch 331/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3481 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2992\n",
            "Epoch 332/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3499 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2986\n",
            "Epoch 333/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3458 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3043\n",
            "Epoch 334/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3495 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2979\n",
            "Epoch 335/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3498 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3037\n",
            "Epoch 336/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3513 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3042\n",
            "Epoch 337/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3507 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2969\n",
            "Epoch 338/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3495 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3058\n",
            "Epoch 339/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3488 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.3001\n",
            "Epoch 340/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0307 - binary_accuracy: 0.9905 - average_precision: 0.3490 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3002\n",
            "Epoch 341/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3501 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3034\n",
            "Epoch 342/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3514 - val_loss: 0.0347 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2986\n",
            "Epoch 343/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3494 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3019\n",
            "Epoch 344/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3508 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3045\n",
            "Epoch 345/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3544 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3018\n",
            "Epoch 346/600\n",
            "14/14 [==============================] - 13s 963ms/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3586 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3042\n",
            "Epoch 347/600\n",
            "14/14 [==============================] - 13s 944ms/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3536 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3010\n",
            "Epoch 348/600\n",
            "14/14 [==============================] - 13s 963ms/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3490 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3053\n",
            "Epoch 349/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3493 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3031\n",
            "Epoch 350/600\n",
            "14/14 [==============================] - 13s 955ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3509 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3065\n",
            "Epoch 351/600\n",
            "14/14 [==============================] - 13s 950ms/step - loss: 0.0306 - binary_accuracy: 0.9905 - average_precision: 0.3460 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3010\n",
            "Epoch 352/600\n",
            "14/14 [==============================] - 13s 962ms/step - loss: 0.0305 - binary_accuracy: 0.9906 - average_precision: 0.3497 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3057\n",
            "Epoch 353/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3535 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3054\n",
            "Epoch 354/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3546 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.2994\n",
            "Epoch 355/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3498 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3021\n",
            "Epoch 356/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0305 - binary_accuracy: 0.9906 - average_precision: 0.3505 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3023\n",
            "Epoch 357/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3505 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3046\n",
            "Epoch 358/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3555 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.3014\n",
            "Epoch 359/600\n",
            "14/14 [==============================] - 22s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3500 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3068\n",
            "Epoch 360/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0304 - binary_accuracy: 0.9905 - average_precision: 0.3541 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3060\n",
            "Epoch 361/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3522 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3053\n",
            "Epoch 362/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3517 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3053\n",
            "Epoch 363/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0304 - binary_accuracy: 0.9905 - average_precision: 0.3521 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.2996\n",
            "Epoch 364/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3510 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3075\n",
            "Epoch 365/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0304 - binary_accuracy: 0.9905 - average_precision: 0.3540 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3038\n",
            "Epoch 366/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3553 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3033\n",
            "Epoch 367/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3538 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3060\n",
            "Epoch 368/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3525 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3020\n",
            "Epoch 369/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3522 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3034\n",
            "Epoch 370/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3501 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3066\n",
            "Epoch 371/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3539 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3040\n",
            "Epoch 372/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3571 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3055\n",
            "Epoch 373/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3593 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3091\n",
            "Epoch 374/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3607 - val_loss: 0.0346 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3011\n",
            "Epoch 375/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3497 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3060\n",
            "Epoch 376/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3550 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3053\n",
            "Epoch 377/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3563 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3051\n",
            "Epoch 378/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3567 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3026\n",
            "Epoch 379/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3512 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3091\n",
            "Epoch 380/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3601 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3063\n",
            "Epoch 381/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3601 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3055\n",
            "Epoch 382/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3561 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3037\n",
            "Epoch 383/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3529 - val_loss: 0.0342 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3108\n",
            "Epoch 384/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3604 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.3056\n",
            "Epoch 385/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3564 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3088\n",
            "Epoch 386/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0305 - binary_accuracy: 0.9905 - average_precision: 0.3558 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3081\n",
            "Epoch 387/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3593 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3100\n",
            "Epoch 388/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3610 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3042\n",
            "Epoch 389/600\n",
            "14/14 [==============================] - 21s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3567 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3055\n",
            "Epoch 390/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3574 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3079\n",
            "Epoch 391/600\n",
            "14/14 [==============================] - 21s 2s/step - loss: 0.0304 - binary_accuracy: 0.9905 - average_precision: 0.3546 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3065\n",
            "Epoch 392/600\n",
            "14/14 [==============================] - 50s 4s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3554 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3047\n",
            "Epoch 393/600\n",
            "14/14 [==============================] - 43s 3s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3586 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3054\n",
            "Epoch 394/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3574 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3093\n",
            "Epoch 395/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3587 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3110\n",
            "Epoch 396/600\n",
            "14/14 [==============================] - 28s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3596 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3067\n",
            "Epoch 397/600\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3586 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3093\n",
            "Epoch 398/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3532 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3079\n",
            "Epoch 399/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3560 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3066\n",
            "Epoch 400/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3594 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3068\n",
            "Epoch 401/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3602 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3085\n",
            "Epoch 402/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3557 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3032\n",
            "Epoch 403/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3592 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3093\n",
            "Epoch 404/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3584 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3063\n",
            "Epoch 405/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0304 - binary_accuracy: 0.9906 - average_precision: 0.3608 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3075\n",
            "Epoch 406/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3602 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3074\n",
            "Epoch 407/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3630 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3103\n",
            "Epoch 408/600\n",
            "14/14 [==============================] - 17s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3611 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3091\n",
            "Epoch 409/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3606 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3069\n",
            "Epoch 410/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3624 - val_loss: 0.0348 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3043\n",
            "Epoch 411/600\n",
            "14/14 [==============================] - 14s 999ms/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3591 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3108\n",
            "Epoch 412/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3604 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3077\n",
            "Epoch 413/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3611 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3039\n",
            "Epoch 414/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3617 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3119\n",
            "Epoch 415/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3632 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3050\n",
            "Epoch 416/600\n",
            "14/14 [==============================] - 13s 940ms/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3641 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3084\n",
            "Epoch 417/600\n",
            "14/14 [==============================] - 13s 948ms/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3671 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3085\n",
            "Epoch 418/600\n",
            "14/14 [==============================] - 13s 947ms/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3617 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3069\n",
            "Epoch 419/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3632 - val_loss: 0.0341 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3121\n",
            "Epoch 420/600\n",
            "14/14 [==============================] - 21s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3605 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3100\n",
            "Epoch 421/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3609 - val_loss: 0.0342 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3092\n",
            "Epoch 422/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3609 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3058\n",
            "Epoch 423/600\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3571 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3068\n",
            "Epoch 424/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3610 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3072\n",
            "Epoch 425/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3613 - val_loss: 0.0346 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3034\n",
            "Epoch 426/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3610 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3112\n",
            "Epoch 427/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3679 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3121\n",
            "Epoch 428/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3671 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3090\n",
            "Epoch 429/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3644 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3061\n",
            "Epoch 430/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3633 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3100\n",
            "Epoch 431/600\n",
            "14/14 [==============================] - 12s 906ms/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3621 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3059\n",
            "Epoch 432/600\n",
            "14/14 [==============================] - 11s 819ms/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3620 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3074\n",
            "Epoch 433/600\n",
            "14/14 [==============================] - 11s 806ms/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3665 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3130\n",
            "Epoch 434/600\n",
            "14/14 [==============================] - 11s 796ms/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3676 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3084\n",
            "Epoch 435/600\n",
            "14/14 [==============================] - 11s 790ms/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3636 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3103\n",
            "Epoch 436/600\n",
            "14/14 [==============================] - 13s 961ms/step - loss: 0.0303 - binary_accuracy: 0.9906 - average_precision: 0.3617 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3129\n",
            "Epoch 437/600\n",
            "14/14 [==============================] - 21s 2s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3662 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3100\n",
            "Epoch 438/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3620 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3132\n",
            "Epoch 439/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3677 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3093\n",
            "Epoch 440/600\n",
            "14/14 [==============================] - 16s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3642 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3131\n",
            "Epoch 441/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3675 - val_loss: 0.0346 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3071\n",
            "Epoch 442/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3641 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3086\n",
            "Epoch 443/600\n",
            "14/14 [==============================] - 14s 997ms/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3647 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3120\n",
            "Epoch 444/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3647 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3116\n",
            "Epoch 445/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3635 - val_loss: 0.0347 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3097\n",
            "Epoch 446/600\n",
            "14/14 [==============================] - 14s 983ms/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3658 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3153\n",
            "Epoch 447/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3692 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3140\n",
            "Epoch 448/600\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3686 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3127\n",
            "Epoch 449/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3676 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3156\n",
            "Epoch 450/600\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3708 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3138\n",
            "Epoch 451/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3678 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3153\n",
            "Epoch 452/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3674 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3098\n",
            "Epoch 453/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3662 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3125\n",
            "Epoch 454/600\n",
            "14/14 [==============================] - 28s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3668 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3119\n",
            "Epoch 455/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3672 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3128\n",
            "Epoch 456/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3691 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3106\n",
            "Epoch 457/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3699 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3173\n",
            "Epoch 458/600\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3695 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3116\n",
            "Epoch 459/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3687 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3116\n",
            "Epoch 460/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3677 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3145\n",
            "Epoch 461/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3684 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3160\n",
            "Epoch 462/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3702 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3151\n",
            "Epoch 463/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3695 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3144\n",
            "Epoch 464/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3688 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3146\n",
            "Epoch 465/600\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3692 - val_loss: 0.0348 - val_binary_accuracy: 0.9895 - val_average_precision: 0.3095\n",
            "Epoch 466/600\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3699 - val_loss: 0.0341 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3177\n",
            "Epoch 467/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3728 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3125\n",
            "Epoch 468/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3697 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3135\n",
            "Epoch 469/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3682 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3113\n",
            "Epoch 470/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3719 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3142\n",
            "Epoch 471/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3693 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3090\n",
            "Epoch 472/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3669 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3178\n",
            "Epoch 473/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3731 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3105\n",
            "Epoch 474/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3690 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3132\n",
            "Epoch 475/600\n",
            "14/14 [==============================] - 28s 2s/step - loss: 0.0302 - binary_accuracy: 0.9906 - average_precision: 0.3677 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3133\n",
            "Epoch 476/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3678 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3148\n",
            "Epoch 477/600\n",
            "14/14 [==============================] - 28s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3732 - val_loss: 0.0341 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3146\n",
            "Epoch 478/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3740 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3167\n",
            "Epoch 479/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3726 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3113\n",
            "Epoch 480/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3683 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3153\n",
            "Epoch 481/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3714 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3146\n",
            "Epoch 482/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3704 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3137\n",
            "Epoch 483/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3731 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3105\n",
            "Epoch 484/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3730 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3151\n",
            "Epoch 485/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3735 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3176\n",
            "Epoch 486/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3743 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3139\n",
            "Epoch 487/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3718 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3131\n",
            "Epoch 488/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3722 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3132\n",
            "Epoch 489/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3699 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3127\n",
            "Epoch 490/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3756 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3142\n",
            "Epoch 491/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3708 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3119\n",
            "Epoch 492/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3707 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3150\n",
            "Epoch 493/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3704 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3193\n",
            "Epoch 494/600\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3740 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3149\n",
            "Epoch 495/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0301 - binary_accuracy: 0.9906 - average_precision: 0.3731 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3144\n",
            "Epoch 496/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3735 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3176\n",
            "Epoch 497/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3743 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3143\n",
            "Epoch 498/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3727 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3106\n",
            "Epoch 499/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3737 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3166\n",
            "Epoch 500/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3730 - val_loss: 0.0346 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3119\n",
            "Epoch 501/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0301 - binary_accuracy: 0.9907 - average_precision: 0.3696 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3140\n",
            "Epoch 502/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3741 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3155\n",
            "Epoch 503/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3748 - val_loss: 0.0342 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3186\n",
            "Epoch 504/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3767 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3120\n",
            "Epoch 505/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3739 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3150\n",
            "Epoch 506/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3746 - val_loss: 0.0346 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3119\n",
            "Epoch 507/600\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3744 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3157\n",
            "Epoch 508/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3770 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3184\n",
            "Epoch 509/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3763 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3157\n",
            "Epoch 510/600\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3772 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3187\n",
            "Epoch 511/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3742 - val_loss: 0.0341 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3182\n",
            "Epoch 512/600\n",
            "14/14 [==============================] - 43s 3s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3763 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3151\n",
            "Epoch 513/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3723 - val_loss: 0.0341 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3173\n",
            "Epoch 514/600\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3781 - val_loss: 0.0342 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3161\n",
            "Epoch 515/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3784 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3164\n",
            "Epoch 516/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3756 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3133\n",
            "Epoch 517/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3763 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3177\n",
            "Epoch 518/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3751 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3147\n",
            "Epoch 519/600\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3767 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3170\n",
            "Epoch 520/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3805 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3154\n",
            "Epoch 521/600\n",
            "14/14 [==============================] - 34s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3766 - val_loss: 0.0343 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3184\n",
            "Epoch 522/600\n",
            "14/14 [==============================] - 35s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3739 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3169\n",
            "Epoch 523/600\n",
            "14/14 [==============================] - 37s 3s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3734 - val_loss: 0.0345 - val_binary_accuracy: 0.9895 - val_average_precision: 0.3129\n",
            "Epoch 524/600\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3756 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3129\n",
            "Epoch 525/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3746 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3177\n",
            "Epoch 526/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3755 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3165\n",
            "Epoch 527/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3759 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3124\n",
            "Epoch 528/600\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3781 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3148\n",
            "Epoch 529/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3748 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3163\n",
            "Epoch 530/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3800 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3166\n",
            "Epoch 531/600\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3799 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3181\n",
            "Epoch 532/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3793 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3140\n",
            "Epoch 533/600\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3778 - val_loss: 0.0341 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3161\n",
            "Epoch 534/600\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3782 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3167\n",
            "Epoch 535/600\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3788 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3164\n",
            "Epoch 536/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3789 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3160\n",
            "Epoch 537/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3773 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.3137\n",
            "Epoch 538/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3781 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3179\n",
            "Epoch 539/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3780 - val_loss: 0.0341 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3196\n",
            "Epoch 540/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3802 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3176\n",
            "Epoch 541/600\n",
            "14/14 [==============================] - 36s 3s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3788 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3156\n",
            "Epoch 542/600\n",
            "14/14 [==============================] - 28s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3768 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3159\n",
            "Epoch 543/600\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3760 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3139\n",
            "Epoch 544/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3801 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3195\n",
            "Epoch 545/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3770 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3147\n",
            "Epoch 546/600\n",
            "14/14 [==============================] - 28s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3800 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3181\n",
            "Epoch 547/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3793 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3187\n",
            "Epoch 548/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3753 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3137\n",
            "Epoch 549/600\n",
            "14/14 [==============================] - 28s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3745 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3159\n",
            "Epoch 550/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3795 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3161\n",
            "Epoch 551/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3776 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3198\n",
            "Epoch 552/600\n",
            "14/14 [==============================] - 28s 2s/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3830 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3199\n",
            "Epoch 553/600\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3821 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3213\n",
            "Epoch 554/600\n",
            "14/14 [==============================] - 21s 2s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3841 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3155\n",
            "Epoch 555/600\n",
            "14/14 [==============================] - 21s 2s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3851 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3198\n",
            "Epoch 556/600\n",
            "14/14 [==============================] - 26s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3784 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3188\n",
            "Epoch 557/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3832 - val_loss: 0.0346 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3156\n",
            "Epoch 558/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3776 - val_loss: 0.0342 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3197\n",
            "Epoch 559/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3841 - val_loss: 0.0346 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3169\n",
            "Epoch 560/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3836 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3196\n",
            "Epoch 561/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3774 - val_loss: 0.0341 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3229\n",
            "Epoch 562/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3855 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3188\n",
            "Epoch 563/600\n",
            "14/14 [==============================] - 34s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3814 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3198\n",
            "Epoch 564/600\n",
            "14/14 [==============================] - 35s 2s/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3817 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3186\n",
            "Epoch 565/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3857 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3185\n",
            "Epoch 566/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3806 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3216\n",
            "Epoch 567/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3874 - val_loss: 0.0343 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3231\n",
            "Epoch 568/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3795 - val_loss: 0.0346 - val_binary_accuracy: 0.9895 - val_average_precision: 0.3160\n",
            "Epoch 569/600\n",
            "14/14 [==============================] - 29s 2s/step - loss: 0.0300 - binary_accuracy: 0.9907 - average_precision: 0.3730 - val_loss: 0.0342 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3196\n",
            "Epoch 570/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3838 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3210\n",
            "Epoch 571/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3798 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3197\n",
            "Epoch 572/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3791 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3188\n",
            "Epoch 573/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3841 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3220\n",
            "Epoch 574/600\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3890 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3195\n",
            "Epoch 575/600\n",
            "14/14 [==============================] - 33s 2s/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3828 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3189\n",
            "Epoch 576/600\n",
            "14/14 [==============================] - 31s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3797 - val_loss: 0.0342 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3203\n",
            "Epoch 577/600\n",
            "14/14 [==============================] - 30s 2s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3845 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3229\n",
            "Epoch 578/600\n",
            "14/14 [==============================] - 32s 2s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3853 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3141\n",
            "Epoch 579/600\n",
            "14/14 [==============================] - 47s 3s/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3825 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3201\n",
            "Epoch 580/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0296 - binary_accuracy: 0.9908 - average_precision: 0.3848 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3205\n",
            "Epoch 581/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3885 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3180\n",
            "Epoch 582/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3880 - val_loss: 0.0342 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3218\n",
            "Epoch 583/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3828 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3211\n",
            "Epoch 584/600\n",
            "14/14 [==============================] - 13s 966ms/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3781 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3215\n",
            "Epoch 585/600\n",
            "14/14 [==============================] - 13s 970ms/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3835 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3210\n",
            "Epoch 586/600\n",
            "14/14 [==============================] - 13s 966ms/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3844 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3188\n",
            "Epoch 587/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3826 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3221\n",
            "Epoch 588/600\n",
            "14/14 [==============================] - 14s 988ms/step - loss: 0.0296 - binary_accuracy: 0.9908 - average_precision: 0.3883 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3213\n",
            "Epoch 589/600\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3833 - val_loss: 0.0345 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3173\n",
            "Epoch 590/600\n",
            "14/14 [==============================] - 14s 993ms/step - loss: 0.0299 - binary_accuracy: 0.9907 - average_precision: 0.3815 - val_loss: 0.0345 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3202\n",
            "Epoch 591/600\n",
            "14/14 [==============================] - 14s 990ms/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3819 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3207\n",
            "Epoch 592/600\n",
            "14/14 [==============================] - 13s 965ms/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3864 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3179\n",
            "Epoch 593/600\n",
            "14/14 [==============================] - 14s 997ms/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3859 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3211\n",
            "Epoch 594/600\n",
            "14/14 [==============================] - 14s 989ms/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3844 - val_loss: 0.0344 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3189\n",
            "Epoch 595/600\n",
            "14/14 [==============================] - 13s 968ms/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3843 - val_loss: 0.0343 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3197\n",
            "Epoch 596/600\n",
            "14/14 [==============================] - 14s 999ms/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3852 - val_loss: 0.0344 - val_binary_accuracy: 0.9896 - val_average_precision: 0.3179\n",
            "Epoch 597/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3814 - val_loss: 0.0341 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3259\n",
            "Epoch 598/600\n",
            "14/14 [==============================] - 13s 966ms/step - loss: 0.0298 - binary_accuracy: 0.9907 - average_precision: 0.3869 - val_loss: 0.0341 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3257\n",
            "Epoch 599/600\n",
            "14/14 [==============================] - 14s 978ms/step - loss: 0.0297 - binary_accuracy: 0.9908 - average_precision: 0.3874 - val_loss: 0.0343 - val_binary_accuracy: 0.9897 - val_average_precision: 0.3200\n",
            "Epoch 600/600\n",
            "14/14 [==============================] - 14s 1s/step - loss: 0.0298 - binary_accuracy: 0.9908 - average_precision: 0.3828 - val_loss: 0.0344 - val_binary_accuracy: 0.9898 - val_average_precision: 0.3222\n"
          ]
        }
      ],
      "source": [
        "INPUT_SHAPE = [train_df.shape[1]]\n",
        "BATCH_SIZE = 5120\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model_cc= tf.keras.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(input_shape=INPUT_SHAPE),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dense(units=len(labels_cc),activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model_cc.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy', tf.keras.metrics.AUC(multi_label=True, curve='PR', name='average_precision')]\n",
        ")\n",
        "\n",
        "hist_cc= model_cc.fit(\n",
        "    X_train_cc.iloc[:,1:], y_train_cc,\n",
        "    validation_data=(X_val_cc.iloc[:,1:], y_val_cc),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=600\n",
        ")\n",
        "#val_loss: 10:0.40 min:0.0319 at 54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cc_l1.save('model_cc.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_SHAPE = [train_df.shape[1]]\n",
        "BATCH_SIZE = 5120\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Dropout\n",
        "\n",
        "\n",
        "# doesn't really learn anymore after 0.20 precision\n",
        "\n",
        "model_cc_l1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(input_shape=INPUT_SHAPE),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dense(units=len(labels_cc),activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model_cc_l1.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy', tf.keras.metrics.AUC(multi_label=True, curve='PR', name='average_precision')]\n",
        ")\n",
        "\n",
        "hist_cc_l1 = model_cc_l1.fit(\n",
        "    X_train_cc.iloc[:,1:], y_train_cc,\n",
        "    validation_data=(X_val_cc.iloc[:,1:], y_val_cc),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=600\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "-I9xPUtk3E0B",
        "outputId": "01373378-7f96-4230-90f8-bd7f0459decf"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-417fb863a99f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_l2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mINPUT_SHAPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5120\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model_cc_l1 = tf.keras.Sequential([\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ],
      "source": [
        "#no errors here, just colab ram crashing. To be re tested, from what I remember it has a similar rate to the original model non reg\n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "INPUT_SHAPE = [train_df.shape[1]]\n",
        "BATCH_SIZE = 5120\n",
        "\n",
        "model_cc_l1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(input_shape=INPUT_SHAPE),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu',kernel_regularizer=l1(0.0000001)),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu',kernel_regularizer=l1(0.0000001)),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu',kernel_regularizer=l1(0.0000001)),\n",
        "    tf.keras.layers.Dense(units=len(labels_cc),activation='sigmoid',kernel_regularizer=l1(0.0000001))\n",
        "])\n",
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_auc',     # The performance metric to monitor\n",
        "    patience=10,            # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True # Restores model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model_cc_l1.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy', tf.keras.metrics.AUC(multi_label=True, curve='PR', name='average_precision')]\n",
        ")\n",
        "\n",
        "hist_cc_l1 = model_cc_l1.fit(\n",
        "    X_train_cc.iloc[:,1:], y_train_cc,\n",
        "    validation_data=(X_val_cc.iloc[:,1:], y_val_cc),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=80,callbacks=early_stopper\n",
        ")\n",
        "#val_loss: 10:0.40 min:0.0319 at 54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGri8kbcm8jR"
      },
      "outputs": [],
      "source": [
        "#never tried, probably doesn't work unless we put a small constant\n",
        "INPUT_SHAPE = [train_df.shape[1]]\n",
        "BATCH_SIZE = 5120\n",
        "\n",
        "model_cc_l2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(input_shape=INPUT_SHAPE),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu',kernel_regularizer='l2'),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu',kernel_regularizer='l2'),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu',kernel_regularizer='l2'),\n",
        "    tf.keras.layers.Dense(units=len(labels_cc),activation='sigmoid',kernel_regularizer='l2')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model_cc_l2.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy', tf.keras.metrics.AUC(multi_label=True, curve='PR', name='average_precision')]\n",
        ")\n",
        "\n",
        "hist_cc_l2 = model_cc_l2.fit(\n",
        "    X_train_cc.iloc[:,1:], y_train_cc,\n",
        "    validation_data=(X_val_cc.iloc[:,1:], y_val_cc),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=80,callbacks=early_stopper\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avf-RoegC4pa"
      },
      "source": [
        "### MF Models\n",
        "\n",
        "- Least common aspect between GO term Ids\n",
        "- Best val_avg_precision: 46% (better than baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "9FV32ZzNRS6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "9/9 [==============================] - 22s 2s/step - loss: 0.2872 - binary_accuracy: 0.8984 - average_precision: 0.0117 - val_loss: 0.3817 - val_binary_accuracy: 0.9869 - val_average_precision: 0.0217\n",
            "Epoch 2/200\n",
            "9/9 [==============================] - 14s 2s/step - loss: 0.0748 - binary_accuracy: 0.9884 - average_precision: 0.0141 - val_loss: 0.4119 - val_binary_accuracy: 0.9903 - val_average_precision: 0.0145\n",
            "Epoch 3/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0600 - binary_accuracy: 0.9904 - average_precision: 0.0156 - val_loss: 0.4317 - val_binary_accuracy: 0.9905 - val_average_precision: 0.0168\n",
            "Epoch 4/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0555 - binary_accuracy: 0.9907 - average_precision: 0.0206 - val_loss: 0.4200 - val_binary_accuracy: 0.9907 - val_average_precision: 0.0205\n",
            "Epoch 5/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0527 - binary_accuracy: 0.9909 - average_precision: 0.0261 - val_loss: 0.4084 - val_binary_accuracy: 0.9908 - val_average_precision: 0.0245\n",
            "Epoch 6/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0506 - binary_accuracy: 0.9910 - average_precision: 0.0345 - val_loss: 0.4039 - val_binary_accuracy: 0.9909 - val_average_precision: 0.0307\n",
            "Epoch 7/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0483 - binary_accuracy: 0.9912 - average_precision: 0.0411 - val_loss: 0.3997 - val_binary_accuracy: 0.9910 - val_average_precision: 0.0430\n",
            "Epoch 8/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0461 - binary_accuracy: 0.9913 - average_precision: 0.0517 - val_loss: 0.3943 - val_binary_accuracy: 0.9910 - val_average_precision: 0.0568\n",
            "Epoch 9/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0441 - binary_accuracy: 0.9915 - average_precision: 0.0627 - val_loss: 0.3869 - val_binary_accuracy: 0.9911 - val_average_precision: 0.0688\n",
            "Epoch 10/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0423 - binary_accuracy: 0.9916 - average_precision: 0.0734 - val_loss: 0.3701 - val_binary_accuracy: 0.9912 - val_average_precision: 0.0814\n",
            "Epoch 11/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0408 - binary_accuracy: 0.9918 - average_precision: 0.0850 - val_loss: 0.3580 - val_binary_accuracy: 0.9913 - val_average_precision: 0.0958\n",
            "Epoch 12/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0395 - binary_accuracy: 0.9919 - average_precision: 0.0967 - val_loss: 0.3369 - val_binary_accuracy: 0.9913 - val_average_precision: 0.1102\n",
            "Epoch 13/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0383 - binary_accuracy: 0.9919 - average_precision: 0.1076 - val_loss: 0.3197 - val_binary_accuracy: 0.9913 - val_average_precision: 0.1224\n",
            "Epoch 14/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0373 - binary_accuracy: 0.9920 - average_precision: 0.1200 - val_loss: 0.3031 - val_binary_accuracy: 0.9914 - val_average_precision: 0.1406\n",
            "Epoch 15/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0363 - binary_accuracy: 0.9921 - average_precision: 0.1318 - val_loss: 0.2831 - val_binary_accuracy: 0.9914 - val_average_precision: 0.1518\n",
            "Epoch 16/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0354 - binary_accuracy: 0.9922 - average_precision: 0.1434 - val_loss: 0.2676 - val_binary_accuracy: 0.9915 - val_average_precision: 0.1650\n",
            "Epoch 17/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0346 - binary_accuracy: 0.9923 - average_precision: 0.1557 - val_loss: 0.2507 - val_binary_accuracy: 0.9915 - val_average_precision: 0.1799\n",
            "Epoch 18/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0339 - binary_accuracy: 0.9924 - average_precision: 0.1660 - val_loss: 0.2345 - val_binary_accuracy: 0.9916 - val_average_precision: 0.1894\n",
            "Epoch 19/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0333 - binary_accuracy: 0.9924 - average_precision: 0.1784 - val_loss: 0.2232 - val_binary_accuracy: 0.9917 - val_average_precision: 0.1999\n",
            "Epoch 20/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0327 - binary_accuracy: 0.9925 - average_precision: 0.1881 - val_loss: 0.2087 - val_binary_accuracy: 0.9916 - val_average_precision: 0.2087\n",
            "Epoch 21/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0321 - binary_accuracy: 0.9925 - average_precision: 0.1980 - val_loss: 0.1950 - val_binary_accuracy: 0.9917 - val_average_precision: 0.2156\n",
            "Epoch 22/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0316 - binary_accuracy: 0.9926 - average_precision: 0.2043 - val_loss: 0.1824 - val_binary_accuracy: 0.9918 - val_average_precision: 0.2285\n",
            "Epoch 23/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0312 - binary_accuracy: 0.9926 - average_precision: 0.2130 - val_loss: 0.1749 - val_binary_accuracy: 0.9918 - val_average_precision: 0.2322\n",
            "Epoch 24/200\n",
            "9/9 [==============================] - 14s 2s/step - loss: 0.0307 - binary_accuracy: 0.9927 - average_precision: 0.2221 - val_loss: 0.1650 - val_binary_accuracy: 0.9918 - val_average_precision: 0.2383\n",
            "Epoch 25/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0303 - binary_accuracy: 0.9927 - average_precision: 0.2292 - val_loss: 0.1550 - val_binary_accuracy: 0.9918 - val_average_precision: 0.2419\n",
            "Epoch 26/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0299 - binary_accuracy: 0.9928 - average_precision: 0.2364 - val_loss: 0.1467 - val_binary_accuracy: 0.9920 - val_average_precision: 0.2537\n",
            "Epoch 27/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0296 - binary_accuracy: 0.9928 - average_precision: 0.2450 - val_loss: 0.1412 - val_binary_accuracy: 0.9920 - val_average_precision: 0.2630\n",
            "Epoch 28/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0293 - binary_accuracy: 0.9928 - average_precision: 0.2488 - val_loss: 0.1323 - val_binary_accuracy: 0.9920 - val_average_precision: 0.2649\n",
            "Epoch 29/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0289 - binary_accuracy: 0.9929 - average_precision: 0.2591 - val_loss: 0.1279 - val_binary_accuracy: 0.9920 - val_average_precision: 0.2712\n",
            "Epoch 30/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0287 - binary_accuracy: 0.9929 - average_precision: 0.2638 - val_loss: 0.1214 - val_binary_accuracy: 0.9921 - val_average_precision: 0.2765\n",
            "Epoch 31/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0284 - binary_accuracy: 0.9930 - average_precision: 0.2713 - val_loss: 0.1153 - val_binary_accuracy: 0.9922 - val_average_precision: 0.2831\n",
            "Epoch 32/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0281 - binary_accuracy: 0.9930 - average_precision: 0.2786 - val_loss: 0.1085 - val_binary_accuracy: 0.9922 - val_average_precision: 0.2860\n",
            "Epoch 33/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0279 - binary_accuracy: 0.9930 - average_precision: 0.2824 - val_loss: 0.1034 - val_binary_accuracy: 0.9922 - val_average_precision: 0.2948\n",
            "Epoch 34/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0277 - binary_accuracy: 0.9930 - average_precision: 0.2841 - val_loss: 0.0990 - val_binary_accuracy: 0.9922 - val_average_precision: 0.2970\n",
            "Epoch 35/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0274 - binary_accuracy: 0.9930 - average_precision: 0.2933 - val_loss: 0.0948 - val_binary_accuracy: 0.9922 - val_average_precision: 0.3028\n",
            "Epoch 36/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0272 - binary_accuracy: 0.9931 - average_precision: 0.3000 - val_loss: 0.0863 - val_binary_accuracy: 0.9923 - val_average_precision: 0.3102\n",
            "Epoch 37/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0270 - binary_accuracy: 0.9931 - average_precision: 0.3007 - val_loss: 0.0823 - val_binary_accuracy: 0.9923 - val_average_precision: 0.3119\n",
            "Epoch 38/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0268 - binary_accuracy: 0.9931 - average_precision: 0.3078 - val_loss: 0.0837 - val_binary_accuracy: 0.9924 - val_average_precision: 0.3143\n",
            "Epoch 39/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0266 - binary_accuracy: 0.9931 - average_precision: 0.3107 - val_loss: 0.0766 - val_binary_accuracy: 0.9925 - val_average_precision: 0.3230\n",
            "Epoch 40/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0265 - binary_accuracy: 0.9932 - average_precision: 0.3181 - val_loss: 0.0702 - val_binary_accuracy: 0.9925 - val_average_precision: 0.3274\n",
            "Epoch 41/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0263 - binary_accuracy: 0.9932 - average_precision: 0.3199 - val_loss: 0.0672 - val_binary_accuracy: 0.9925 - val_average_precision: 0.3286\n",
            "Epoch 42/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0262 - binary_accuracy: 0.9932 - average_precision: 0.3237 - val_loss: 0.0645 - val_binary_accuracy: 0.9925 - val_average_precision: 0.3347\n",
            "Epoch 43/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0260 - binary_accuracy: 0.9932 - average_precision: 0.3312 - val_loss: 0.0618 - val_binary_accuracy: 0.9927 - val_average_precision: 0.3411\n",
            "Epoch 44/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0259 - binary_accuracy: 0.9932 - average_precision: 0.3325 - val_loss: 0.0614 - val_binary_accuracy: 0.9927 - val_average_precision: 0.3404\n",
            "Epoch 45/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0257 - binary_accuracy: 0.9933 - average_precision: 0.3358 - val_loss: 0.0565 - val_binary_accuracy: 0.9927 - val_average_precision: 0.3469\n",
            "Epoch 46/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0256 - binary_accuracy: 0.9933 - average_precision: 0.3385 - val_loss: 0.0556 - val_binary_accuracy: 0.9928 - val_average_precision: 0.3486\n",
            "Epoch 47/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0255 - binary_accuracy: 0.9933 - average_precision: 0.3412 - val_loss: 0.0525 - val_binary_accuracy: 0.9929 - val_average_precision: 0.3529\n",
            "Epoch 48/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0254 - binary_accuracy: 0.9933 - average_precision: 0.3450 - val_loss: 0.0501 - val_binary_accuracy: 0.9929 - val_average_precision: 0.3590\n",
            "Epoch 49/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0252 - binary_accuracy: 0.9933 - average_precision: 0.3480 - val_loss: 0.0486 - val_binary_accuracy: 0.9929 - val_average_precision: 0.3577\n",
            "Epoch 50/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0252 - binary_accuracy: 0.9933 - average_precision: 0.3481 - val_loss: 0.0455 - val_binary_accuracy: 0.9929 - val_average_precision: 0.3584\n",
            "Epoch 51/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0251 - binary_accuracy: 0.9933 - average_precision: 0.3513 - val_loss: 0.0444 - val_binary_accuracy: 0.9930 - val_average_precision: 0.3674\n",
            "Epoch 52/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0249 - binary_accuracy: 0.9933 - average_precision: 0.3551 - val_loss: 0.0422 - val_binary_accuracy: 0.9931 - val_average_precision: 0.3693\n",
            "Epoch 53/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0248 - binary_accuracy: 0.9934 - average_precision: 0.3604 - val_loss: 0.0390 - val_binary_accuracy: 0.9930 - val_average_precision: 0.3699\n",
            "Epoch 54/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0247 - binary_accuracy: 0.9934 - average_precision: 0.3618 - val_loss: 0.0392 - val_binary_accuracy: 0.9931 - val_average_precision: 0.3743\n",
            "Epoch 55/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0246 - binary_accuracy: 0.9934 - average_precision: 0.3623 - val_loss: 0.0366 - val_binary_accuracy: 0.9932 - val_average_precision: 0.3797\n",
            "Epoch 56/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0246 - binary_accuracy: 0.9934 - average_precision: 0.3629 - val_loss: 0.0357 - val_binary_accuracy: 0.9931 - val_average_precision: 0.3749\n",
            "Epoch 57/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0245 - binary_accuracy: 0.9934 - average_precision: 0.3694 - val_loss: 0.0349 - val_binary_accuracy: 0.9933 - val_average_precision: 0.3838\n",
            "Epoch 58/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0244 - binary_accuracy: 0.9934 - average_precision: 0.3695 - val_loss: 0.0330 - val_binary_accuracy: 0.9932 - val_average_precision: 0.3859\n",
            "Epoch 59/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0243 - binary_accuracy: 0.9934 - average_precision: 0.3709 - val_loss: 0.0329 - val_binary_accuracy: 0.9933 - val_average_precision: 0.3879\n",
            "Epoch 60/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0242 - binary_accuracy: 0.9935 - average_precision: 0.3754 - val_loss: 0.0327 - val_binary_accuracy: 0.9933 - val_average_precision: 0.3860\n",
            "Epoch 61/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0242 - binary_accuracy: 0.9935 - average_precision: 0.3738 - val_loss: 0.0313 - val_binary_accuracy: 0.9933 - val_average_precision: 0.3830\n",
            "Epoch 62/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0241 - binary_accuracy: 0.9935 - average_precision: 0.3766 - val_loss: 0.0303 - val_binary_accuracy: 0.9933 - val_average_precision: 0.3892\n",
            "Epoch 63/200\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0240 - binary_accuracy: 0.9935 - average_precision: 0.3810 - val_loss: 0.0293 - val_binary_accuracy: 0.9934 - val_average_precision: 0.3967\n",
            "Epoch 64/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0239 - binary_accuracy: 0.9935 - average_precision: 0.3814 - val_loss: 0.0286 - val_binary_accuracy: 0.9933 - val_average_precision: 0.3900\n",
            "Epoch 65/200\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0239 - binary_accuracy: 0.9935 - average_precision: 0.3869 - val_loss: 0.0284 - val_binary_accuracy: 0.9934 - val_average_precision: 0.3996\n",
            "Epoch 66/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0238 - binary_accuracy: 0.9935 - average_precision: 0.3864 - val_loss: 0.0277 - val_binary_accuracy: 0.9934 - val_average_precision: 0.4011\n",
            "Epoch 67/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0237 - binary_accuracy: 0.9936 - average_precision: 0.3864 - val_loss: 0.0276 - val_binary_accuracy: 0.9933 - val_average_precision: 0.3941\n",
            "Epoch 68/200\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0238 - binary_accuracy: 0.9935 - average_precision: 0.3860 - val_loss: 0.0270 - val_binary_accuracy: 0.9934 - val_average_precision: 0.3994\n",
            "Epoch 69/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0237 - binary_accuracy: 0.9936 - average_precision: 0.3910 - val_loss: 0.0265 - val_binary_accuracy: 0.9934 - val_average_precision: 0.4043\n",
            "Epoch 70/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0236 - binary_accuracy: 0.9936 - average_precision: 0.3944 - val_loss: 0.0264 - val_binary_accuracy: 0.9935 - val_average_precision: 0.4021\n",
            "Epoch 71/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0235 - binary_accuracy: 0.9936 - average_precision: 0.3951 - val_loss: 0.0261 - val_binary_accuracy: 0.9935 - val_average_precision: 0.4043\n",
            "Epoch 72/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0235 - binary_accuracy: 0.9936 - average_precision: 0.3939 - val_loss: 0.0258 - val_binary_accuracy: 0.9935 - val_average_precision: 0.4073\n",
            "Epoch 73/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0234 - binary_accuracy: 0.9936 - average_precision: 0.3946 - val_loss: 0.0258 - val_binary_accuracy: 0.9935 - val_average_precision: 0.4115\n",
            "Epoch 74/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0234 - binary_accuracy: 0.9936 - average_precision: 0.3943 - val_loss: 0.0252 - val_binary_accuracy: 0.9935 - val_average_precision: 0.4085\n",
            "Epoch 75/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0234 - binary_accuracy: 0.9936 - average_precision: 0.3956 - val_loss: 0.0249 - val_binary_accuracy: 0.9935 - val_average_precision: 0.4085\n",
            "Epoch 76/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0233 - binary_accuracy: 0.9936 - average_precision: 0.4022 - val_loss: 0.0249 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4133\n",
            "Epoch 77/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0232 - binary_accuracy: 0.9937 - average_precision: 0.4054 - val_loss: 0.0247 - val_binary_accuracy: 0.9935 - val_average_precision: 0.4157\n",
            "Epoch 78/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0232 - binary_accuracy: 0.9936 - average_precision: 0.4027 - val_loss: 0.0250 - val_binary_accuracy: 0.9935 - val_average_precision: 0.4159\n",
            "Epoch 79/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0231 - binary_accuracy: 0.9937 - average_precision: 0.4044 - val_loss: 0.0247 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4150\n",
            "Epoch 80/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0230 - binary_accuracy: 0.9937 - average_precision: 0.4101 - val_loss: 0.0247 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4168\n",
            "Epoch 81/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0229 - binary_accuracy: 0.9937 - average_precision: 0.4086 - val_loss: 0.0244 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4216\n",
            "Epoch 82/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0229 - binary_accuracy: 0.9937 - average_precision: 0.4145 - val_loss: 0.0243 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4195\n",
            "Epoch 83/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0229 - binary_accuracy: 0.9937 - average_precision: 0.4113 - val_loss: 0.0242 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4197\n",
            "Epoch 84/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0229 - binary_accuracy: 0.9937 - average_precision: 0.4130 - val_loss: 0.0242 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4184\n",
            "Epoch 85/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0228 - binary_accuracy: 0.9937 - average_precision: 0.4154 - val_loss: 0.0244 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4165\n",
            "Epoch 86/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0228 - binary_accuracy: 0.9937 - average_precision: 0.4123 - val_loss: 0.0241 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4223\n",
            "Epoch 87/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0228 - binary_accuracy: 0.9937 - average_precision: 0.4163 - val_loss: 0.0241 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4195\n",
            "Epoch 88/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0227 - binary_accuracy: 0.9937 - average_precision: 0.4170 - val_loss: 0.0241 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4232\n",
            "Epoch 89/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0226 - binary_accuracy: 0.9938 - average_precision: 0.4201 - val_loss: 0.0241 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4256\n",
            "Epoch 90/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0226 - binary_accuracy: 0.9938 - average_precision: 0.4217 - val_loss: 0.0241 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4235\n",
            "Epoch 91/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0226 - binary_accuracy: 0.9937 - average_precision: 0.4210 - val_loss: 0.0241 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4224\n",
            "Epoch 92/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0226 - binary_accuracy: 0.9938 - average_precision: 0.4219 - val_loss: 0.0240 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4251\n",
            "Epoch 93/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0225 - binary_accuracy: 0.9938 - average_precision: 0.4249 - val_loss: 0.0240 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4272\n",
            "Epoch 94/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0225 - binary_accuracy: 0.9938 - average_precision: 0.4238 - val_loss: 0.0240 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4258\n",
            "Epoch 95/200\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0225 - binary_accuracy: 0.9938 - average_precision: 0.4261 - val_loss: 0.0240 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4262\n",
            "Epoch 96/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0225 - binary_accuracy: 0.9938 - average_precision: 0.4246 - val_loss: 0.0241 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4232\n",
            "Epoch 97/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0225 - binary_accuracy: 0.9938 - average_precision: 0.4259 - val_loss: 0.0239 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4282\n",
            "Epoch 98/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0224 - binary_accuracy: 0.9938 - average_precision: 0.4287 - val_loss: 0.0240 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4302\n",
            "Epoch 99/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0223 - binary_accuracy: 0.9938 - average_precision: 0.4292 - val_loss: 0.0239 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4268\n",
            "Epoch 100/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0223 - binary_accuracy: 0.9938 - average_precision: 0.4298 - val_loss: 0.0239 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4299\n",
            "Epoch 101/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0222 - binary_accuracy: 0.9938 - average_precision: 0.4353 - val_loss: 0.0238 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4297\n",
            "Epoch 102/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0222 - binary_accuracy: 0.9938 - average_precision: 0.4357 - val_loss: 0.0238 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4322\n",
            "Epoch 103/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0222 - binary_accuracy: 0.9938 - average_precision: 0.4316 - val_loss: 0.0238 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4340\n",
            "Epoch 104/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0221 - binary_accuracy: 0.9939 - average_precision: 0.4345 - val_loss: 0.0239 - val_binary_accuracy: 0.9936 - val_average_precision: 0.4296\n",
            "Epoch 105/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0222 - binary_accuracy: 0.9938 - average_precision: 0.4351 - val_loss: 0.0238 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4306\n",
            "Epoch 106/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0222 - binary_accuracy: 0.9938 - average_precision: 0.4349 - val_loss: 0.0238 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4338\n",
            "Epoch 107/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0221 - binary_accuracy: 0.9939 - average_precision: 0.4384 - val_loss: 0.0238 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4310\n",
            "Epoch 108/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0221 - binary_accuracy: 0.9939 - average_precision: 0.4375 - val_loss: 0.0238 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4348\n",
            "Epoch 109/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0221 - binary_accuracy: 0.9938 - average_precision: 0.4365 - val_loss: 0.0238 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4319\n",
            "Epoch 110/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0220 - binary_accuracy: 0.9939 - average_precision: 0.4404 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4369\n",
            "Epoch 111/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0219 - binary_accuracy: 0.9939 - average_precision: 0.4431 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4351\n",
            "Epoch 112/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0220 - binary_accuracy: 0.9939 - average_precision: 0.4427 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4357\n",
            "Epoch 113/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0219 - binary_accuracy: 0.9939 - average_precision: 0.4423 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4326\n",
            "Epoch 114/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0219 - binary_accuracy: 0.9939 - average_precision: 0.4462 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4377\n",
            "Epoch 115/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0219 - binary_accuracy: 0.9939 - average_precision: 0.4452 - val_loss: 0.0239 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4363\n",
            "Epoch 116/200\n",
            "9/9 [==============================] - 13s 2s/step - loss: 0.0219 - binary_accuracy: 0.9939 - average_precision: 0.4442 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4361\n",
            "Epoch 117/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0219 - binary_accuracy: 0.9939 - average_precision: 0.4431 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4381\n",
            "Epoch 118/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0218 - binary_accuracy: 0.9939 - average_precision: 0.4472 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4337\n",
            "Epoch 119/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0218 - binary_accuracy: 0.9939 - average_precision: 0.4448 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4422\n",
            "Epoch 120/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0218 - binary_accuracy: 0.9939 - average_precision: 0.4451 - val_loss: 0.0238 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4373\n",
            "Epoch 121/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0218 - binary_accuracy: 0.9939 - average_precision: 0.4497 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4386\n",
            "Epoch 122/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0218 - binary_accuracy: 0.9939 - average_precision: 0.4495 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4377\n",
            "Epoch 123/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0218 - binary_accuracy: 0.9939 - average_precision: 0.4481 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4402\n",
            "Epoch 124/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0217 - binary_accuracy: 0.9940 - average_precision: 0.4499 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4397\n",
            "Epoch 125/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0217 - binary_accuracy: 0.9940 - average_precision: 0.4524 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4389\n",
            "Epoch 126/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0217 - binary_accuracy: 0.9939 - average_precision: 0.4517 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4386\n",
            "Epoch 127/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0217 - binary_accuracy: 0.9939 - average_precision: 0.4517 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4413\n",
            "Epoch 128/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0217 - binary_accuracy: 0.9940 - average_precision: 0.4530 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4432\n",
            "Epoch 129/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0216 - binary_accuracy: 0.9940 - average_precision: 0.4535 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4418\n",
            "Epoch 130/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0216 - binary_accuracy: 0.9940 - average_precision: 0.4510 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4396\n",
            "Epoch 131/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0216 - binary_accuracy: 0.9940 - average_precision: 0.4552 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4402\n",
            "Epoch 132/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0215 - binary_accuracy: 0.9940 - average_precision: 0.4569 - val_loss: 0.0236 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4447\n",
            "Epoch 133/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0215 - binary_accuracy: 0.9940 - average_precision: 0.4592 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4458\n",
            "Epoch 134/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0215 - binary_accuracy: 0.9940 - average_precision: 0.4579 - val_loss: 0.0236 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4441\n",
            "Epoch 135/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0215 - binary_accuracy: 0.9940 - average_precision: 0.4583 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4442\n",
            "Epoch 136/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0216 - binary_accuracy: 0.9940 - average_precision: 0.4565 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4441\n",
            "Epoch 137/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0216 - binary_accuracy: 0.9940 - average_precision: 0.4562 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4455\n",
            "Epoch 138/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0216 - binary_accuracy: 0.9940 - average_precision: 0.4562 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4455\n",
            "Epoch 139/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0216 - binary_accuracy: 0.9940 - average_precision: 0.4549 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4442\n",
            "Epoch 140/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0214 - binary_accuracy: 0.9940 - average_precision: 0.4595 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4442\n",
            "Epoch 141/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0215 - binary_accuracy: 0.9940 - average_precision: 0.4592 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4458\n",
            "Epoch 142/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0215 - binary_accuracy: 0.9940 - average_precision: 0.4592 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4467\n",
            "Epoch 143/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0214 - binary_accuracy: 0.9940 - average_precision: 0.4605 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4464\n",
            "Epoch 144/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0214 - binary_accuracy: 0.9940 - average_precision: 0.4624 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4478\n",
            "Epoch 145/200\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0213 - binary_accuracy: 0.9940 - average_precision: 0.4635 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4455\n",
            "Epoch 146/200\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0214 - binary_accuracy: 0.9940 - average_precision: 0.4601 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4446\n",
            "Epoch 147/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0214 - binary_accuracy: 0.9940 - average_precision: 0.4640 - val_loss: 0.0236 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4478\n",
            "Epoch 148/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0214 - binary_accuracy: 0.9940 - average_precision: 0.4628 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4472\n",
            "Epoch 149/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0214 - binary_accuracy: 0.9940 - average_precision: 0.4643 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4466\n",
            "Epoch 150/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0214 - binary_accuracy: 0.9940 - average_precision: 0.4652 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4480\n",
            "Epoch 151/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0213 - binary_accuracy: 0.9940 - average_precision: 0.4681 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4491\n",
            "Epoch 152/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0213 - binary_accuracy: 0.9940 - average_precision: 0.4642 - val_loss: 0.0235 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4521\n",
            "Epoch 153/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0212 - binary_accuracy: 0.9940 - average_precision: 0.4653 - val_loss: 0.0237 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4455\n",
            "Epoch 154/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0213 - binary_accuracy: 0.9941 - average_precision: 0.4668 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4531\n",
            "Epoch 155/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0212 - binary_accuracy: 0.9941 - average_precision: 0.4695 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4541\n",
            "Epoch 156/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0212 - binary_accuracy: 0.9941 - average_precision: 0.4707 - val_loss: 0.0236 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4495\n",
            "Epoch 157/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0212 - binary_accuracy: 0.9940 - average_precision: 0.4695 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4512\n",
            "Epoch 158/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0212 - binary_accuracy: 0.9940 - average_precision: 0.4708 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4540\n",
            "Epoch 159/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0211 - binary_accuracy: 0.9941 - average_precision: 0.4715 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4555\n",
            "Epoch 160/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0211 - binary_accuracy: 0.9941 - average_precision: 0.4719 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4509\n",
            "Epoch 161/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0211 - binary_accuracy: 0.9941 - average_precision: 0.4711 - val_loss: 0.0234 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4515\n",
            "Epoch 162/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0212 - binary_accuracy: 0.9941 - average_precision: 0.4700 - val_loss: 0.0235 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4504\n",
            "Epoch 163/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0212 - binary_accuracy: 0.9941 - average_precision: 0.4690 - val_loss: 0.0235 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4550\n",
            "Epoch 164/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0212 - binary_accuracy: 0.9941 - average_precision: 0.4720 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4515\n",
            "Epoch 165/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0212 - binary_accuracy: 0.9941 - average_precision: 0.4722 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4539\n",
            "Epoch 166/200\n",
            "9/9 [==============================] - 13s 1s/step - loss: 0.0211 - binary_accuracy: 0.9941 - average_precision: 0.4735 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4516\n",
            "Epoch 167/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0211 - binary_accuracy: 0.9941 - average_precision: 0.4730 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4511\n",
            "Epoch 168/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0211 - binary_accuracy: 0.9941 - average_precision: 0.4692 - val_loss: 0.0233 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4548\n",
            "Epoch 169/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0211 - binary_accuracy: 0.9941 - average_precision: 0.4730 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4543\n",
            "Epoch 170/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0210 - binary_accuracy: 0.9941 - average_precision: 0.4745 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4557\n",
            "Epoch 171/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0210 - binary_accuracy: 0.9941 - average_precision: 0.4754 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4572\n",
            "Epoch 172/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0210 - binary_accuracy: 0.9941 - average_precision: 0.4739 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4542\n",
            "Epoch 173/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0210 - binary_accuracy: 0.9941 - average_precision: 0.4765 - val_loss: 0.0235 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4539\n",
            "Epoch 174/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4758 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4509\n",
            "Epoch 175/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9942 - average_precision: 0.4769 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4574\n",
            "Epoch 176/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4794 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4572\n",
            "Epoch 177/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4781 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4563\n",
            "Epoch 178/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4774 - val_loss: 0.0233 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4605\n",
            "Epoch 179/200\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.0210 - binary_accuracy: 0.9941 - average_precision: 0.4785 - val_loss: 0.0235 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4528\n",
            "Epoch 180/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0210 - binary_accuracy: 0.9941 - average_precision: 0.4775 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4553\n",
            "Epoch 181/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4799 - val_loss: 0.0236 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4524\n",
            "Epoch 182/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4776 - val_loss: 0.0235 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4554\n",
            "Epoch 183/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4797 - val_loss: 0.0233 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4581\n",
            "Epoch 184/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0208 - binary_accuracy: 0.9942 - average_precision: 0.4833 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4550\n",
            "Epoch 185/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4785 - val_loss: 0.0236 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4557\n",
            "Epoch 186/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4800 - val_loss: 0.0235 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4566\n",
            "Epoch 187/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4769 - val_loss: 0.0234 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4592\n",
            "Epoch 188/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0210 - binary_accuracy: 0.9941 - average_precision: 0.4799 - val_loss: 0.0233 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4573\n",
            "Epoch 189/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0209 - binary_accuracy: 0.9941 - average_precision: 0.4801 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4602\n",
            "Epoch 190/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0208 - binary_accuracy: 0.9941 - average_precision: 0.4840 - val_loss: 0.0233 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4586\n",
            "Epoch 191/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0208 - binary_accuracy: 0.9942 - average_precision: 0.4843 - val_loss: 0.0233 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4596\n",
            "Epoch 192/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0207 - binary_accuracy: 0.9942 - average_precision: 0.4861 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4591\n",
            "Epoch 193/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0207 - binary_accuracy: 0.9942 - average_precision: 0.4851 - val_loss: 0.0235 - val_binary_accuracy: 0.9937 - val_average_precision: 0.4584\n",
            "Epoch 194/200\n",
            "9/9 [==============================] - 11s 1s/step - loss: 0.0208 - binary_accuracy: 0.9941 - average_precision: 0.4830 - val_loss: 0.0233 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4607\n",
            "Epoch 195/200\n",
            "9/9 [==============================] - 12s 1s/step - loss: 0.0208 - binary_accuracy: 0.9941 - average_precision: 0.4832 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4576\n",
            "Epoch 196/200\n",
            "9/9 [==============================] - 16s 2s/step - loss: 0.0207 - binary_accuracy: 0.9942 - average_precision: 0.4853 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4579\n",
            "Epoch 197/200\n",
            "9/9 [==============================] - 15s 2s/step - loss: 0.0207 - binary_accuracy: 0.9942 - average_precision: 0.4867 - val_loss: 0.0233 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4618\n",
            "Epoch 198/200\n",
            "9/9 [==============================] - 14s 2s/step - loss: 0.0208 - binary_accuracy: 0.9942 - average_precision: 0.4848 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4593\n",
            "Epoch 199/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0207 - binary_accuracy: 0.9942 - average_precision: 0.4843 - val_loss: 0.0235 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4612\n",
            "Epoch 200/200\n",
            "9/9 [==============================] - 10s 1s/step - loss: 0.0207 - binary_accuracy: 0.9942 - average_precision: 0.4871 - val_loss: 0.0234 - val_binary_accuracy: 0.9938 - val_average_precision: 0.4644\n"
          ]
        }
      ],
      "source": [
        "INPUT_SHAPE = [train_df.shape[1]]\n",
        "BATCH_SIZE = 5120\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model_mf = tf.keras.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(input_shape=INPUT_SHAPE),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dense(units=len(labels_mf),activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model_mf.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy', tf.keras.metrics.AUC(multi_label=True, curve='PR', name='average_precision')]\n",
        ")\n",
        "\n",
        "hist_mf = model_mf.fit(\n",
        "    X_train_mf.iloc[:,1:], y_train_mf,\n",
        "    validation_data=(X_val_mf.iloc[:,1:], y_val_mf),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_mf.save('model_mf.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BP Models\n",
        "\n",
        "- The majority of the GO term Ids have BPO(Biological Process Ontology) as their aspect.\n",
        "- In the baseline results, this aspect has the lowest scores\n",
        "- Best val_avg_precision: 21%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "JbNwbjcwRhCZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "13/13 [==============================] - 59s 4s/step - loss: 0.2323 - binary_accuracy: 0.9319 - average_precision: 0.0223 - val_loss: 0.4421 - val_binary_accuracy: 0.9795 - val_average_precision: 0.0206\n",
            "Epoch 2/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.1020 - binary_accuracy: 0.9796 - average_precision: 0.0269 - val_loss: 0.4767 - val_binary_accuracy: 0.9796 - val_average_precision: 0.0253\n",
            "Epoch 3/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0936 - binary_accuracy: 0.9799 - average_precision: 0.0348 - val_loss: 0.4408 - val_binary_accuracy: 0.9794 - val_average_precision: 0.0321\n",
            "Epoch 4/200\n",
            "13/13 [==============================] - 38s 3s/step - loss: 0.0892 - binary_accuracy: 0.9801 - average_precision: 0.0440 - val_loss: 0.4359 - val_binary_accuracy: 0.9796 - val_average_precision: 0.0416\n",
            "Epoch 5/200\n",
            "13/13 [==============================] - 50s 4s/step - loss: 0.0856 - binary_accuracy: 0.9803 - average_precision: 0.0550 - val_loss: 0.4164 - val_binary_accuracy: 0.9795 - val_average_precision: 0.0497\n",
            "Epoch 6/200\n",
            "13/13 [==============================] - 29s 2s/step - loss: 0.0828 - binary_accuracy: 0.9804 - average_precision: 0.0656 - val_loss: 0.3990 - val_binary_accuracy: 0.9796 - val_average_precision: 0.0594\n",
            "Epoch 7/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0806 - binary_accuracy: 0.9805 - average_precision: 0.0755 - val_loss: 0.3838 - val_binary_accuracy: 0.9798 - val_average_precision: 0.0707\n",
            "Epoch 8/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0787 - binary_accuracy: 0.9806 - average_precision: 0.0852 - val_loss: 0.3598 - val_binary_accuracy: 0.9798 - val_average_precision: 0.0772\n",
            "Epoch 9/200\n",
            "13/13 [==============================] - 24s 2s/step - loss: 0.0772 - binary_accuracy: 0.9807 - average_precision: 0.0929 - val_loss: 0.3412 - val_binary_accuracy: 0.9798 - val_average_precision: 0.0836\n",
            "Epoch 10/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0758 - binary_accuracy: 0.9808 - average_precision: 0.1003 - val_loss: 0.3214 - val_binary_accuracy: 0.9799 - val_average_precision: 0.0939\n",
            "Epoch 11/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0746 - binary_accuracy: 0.9809 - average_precision: 0.1070 - val_loss: 0.3072 - val_binary_accuracy: 0.9799 - val_average_precision: 0.0999\n",
            "Epoch 12/200\n",
            "13/13 [==============================] - 24s 2s/step - loss: 0.0736 - binary_accuracy: 0.9809 - average_precision: 0.1122 - val_loss: 0.2924 - val_binary_accuracy: 0.9800 - val_average_precision: 0.1053\n",
            "Epoch 13/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0726 - binary_accuracy: 0.9810 - average_precision: 0.1171 - val_loss: 0.2693 - val_binary_accuracy: 0.9800 - val_average_precision: 0.1109\n",
            "Epoch 14/200\n",
            "13/13 [==============================] - 29s 2s/step - loss: 0.0718 - binary_accuracy: 0.9810 - average_precision: 0.1223 - val_loss: 0.2530 - val_binary_accuracy: 0.9801 - val_average_precision: 0.1127\n",
            "Epoch 15/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0711 - binary_accuracy: 0.9811 - average_precision: 0.1273 - val_loss: 0.2427 - val_binary_accuracy: 0.9801 - val_average_precision: 0.1155\n",
            "Epoch 16/200\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.0703 - binary_accuracy: 0.9811 - average_precision: 0.1315 - val_loss: 0.2297 - val_binary_accuracy: 0.9801 - val_average_precision: 0.1219\n",
            "Epoch 17/200\n",
            "13/13 [==============================] - 36s 3s/step - loss: 0.0697 - binary_accuracy: 0.9812 - average_precision: 0.1349 - val_loss: 0.2204 - val_binary_accuracy: 0.9802 - val_average_precision: 0.1235\n",
            "Epoch 18/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0692 - binary_accuracy: 0.9812 - average_precision: 0.1377 - val_loss: 0.2086 - val_binary_accuracy: 0.9803 - val_average_precision: 0.1286\n",
            "Epoch 19/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0688 - binary_accuracy: 0.9812 - average_precision: 0.1400 - val_loss: 0.1964 - val_binary_accuracy: 0.9803 - val_average_precision: 0.1310\n",
            "Epoch 20/200\n",
            "13/13 [==============================] - 31s 2s/step - loss: 0.0682 - binary_accuracy: 0.9813 - average_precision: 0.1429 - val_loss: 0.1866 - val_binary_accuracy: 0.9803 - val_average_precision: 0.1319\n",
            "Epoch 21/200\n",
            "13/13 [==============================] - 38s 3s/step - loss: 0.0678 - binary_accuracy: 0.9813 - average_precision: 0.1468 - val_loss: 0.1775 - val_binary_accuracy: 0.9803 - val_average_precision: 0.1342\n",
            "Epoch 22/200\n",
            "13/13 [==============================] - 29s 2s/step - loss: 0.0673 - binary_accuracy: 0.9813 - average_precision: 0.1489 - val_loss: 0.1701 - val_binary_accuracy: 0.9804 - val_average_precision: 0.1371\n",
            "Epoch 23/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0669 - binary_accuracy: 0.9814 - average_precision: 0.1528 - val_loss: 0.1607 - val_binary_accuracy: 0.9805 - val_average_precision: 0.1377\n",
            "Epoch 24/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0666 - binary_accuracy: 0.9814 - average_precision: 0.1544 - val_loss: 0.1571 - val_binary_accuracy: 0.9805 - val_average_precision: 0.1400\n",
            "Epoch 25/200\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.0662 - binary_accuracy: 0.9814 - average_precision: 0.1583 - val_loss: 0.1497 - val_binary_accuracy: 0.9805 - val_average_precision: 0.1416\n",
            "Epoch 26/200\n",
            "13/13 [==============================] - 35s 3s/step - loss: 0.0659 - binary_accuracy: 0.9815 - average_precision: 0.1599 - val_loss: 0.1405 - val_binary_accuracy: 0.9805 - val_average_precision: 0.1439\n",
            "Epoch 27/200\n",
            "13/13 [==============================] - 31s 2s/step - loss: 0.0656 - binary_accuracy: 0.9815 - average_precision: 0.1621 - val_loss: 0.1341 - val_binary_accuracy: 0.9806 - val_average_precision: 0.1454\n",
            "Epoch 28/200\n",
            "13/13 [==============================] - 35s 3s/step - loss: 0.0654 - binary_accuracy: 0.9815 - average_precision: 0.1632 - val_loss: 0.1290 - val_binary_accuracy: 0.9806 - val_average_precision: 0.1470\n",
            "Epoch 29/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0650 - binary_accuracy: 0.9816 - average_precision: 0.1682 - val_loss: 0.1215 - val_binary_accuracy: 0.9807 - val_average_precision: 0.1497\n",
            "Epoch 30/200\n",
            "13/13 [==============================] - 32s 3s/step - loss: 0.0647 - binary_accuracy: 0.9816 - average_precision: 0.1705 - val_loss: 0.1160 - val_binary_accuracy: 0.9806 - val_average_precision: 0.1498\n",
            "Epoch 31/200\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.0645 - binary_accuracy: 0.9816 - average_precision: 0.1712 - val_loss: 0.1126 - val_binary_accuracy: 0.9806 - val_average_precision: 0.1530\n",
            "Epoch 32/200\n",
            "13/13 [==============================] - 32s 3s/step - loss: 0.0642 - binary_accuracy: 0.9817 - average_precision: 0.1740 - val_loss: 0.1050 - val_binary_accuracy: 0.9807 - val_average_precision: 0.1548\n",
            "Epoch 33/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0640 - binary_accuracy: 0.9817 - average_precision: 0.1766 - val_loss: 0.1056 - val_binary_accuracy: 0.9807 - val_average_precision: 0.1581\n",
            "Epoch 34/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0641 - binary_accuracy: 0.9817 - average_precision: 0.1752 - val_loss: 0.0969 - val_binary_accuracy: 0.9809 - val_average_precision: 0.1542\n",
            "Epoch 35/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0637 - binary_accuracy: 0.9817 - average_precision: 0.1786 - val_loss: 0.0953 - val_binary_accuracy: 0.9809 - val_average_precision: 0.1600\n",
            "Epoch 36/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0635 - binary_accuracy: 0.9818 - average_precision: 0.1808 - val_loss: 0.0883 - val_binary_accuracy: 0.9810 - val_average_precision: 0.1636\n",
            "Epoch 37/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0633 - binary_accuracy: 0.9818 - average_precision: 0.1823 - val_loss: 0.0873 - val_binary_accuracy: 0.9810 - val_average_precision: 0.1664\n",
            "Epoch 38/200\n",
            "13/13 [==============================] - 32s 3s/step - loss: 0.0630 - binary_accuracy: 0.9818 - average_precision: 0.1858 - val_loss: 0.0871 - val_binary_accuracy: 0.9808 - val_average_precision: 0.1640\n",
            "Epoch 39/200\n",
            "13/13 [==============================] - 31s 2s/step - loss: 0.0630 - binary_accuracy: 0.9818 - average_precision: 0.1860 - val_loss: 0.0833 - val_binary_accuracy: 0.9810 - val_average_precision: 0.1665\n",
            "Epoch 40/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0628 - binary_accuracy: 0.9819 - average_precision: 0.1882 - val_loss: 0.0819 - val_binary_accuracy: 0.9809 - val_average_precision: 0.1669\n",
            "Epoch 41/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0628 - binary_accuracy: 0.9819 - average_precision: 0.1876 - val_loss: 0.0767 - val_binary_accuracy: 0.9811 - val_average_precision: 0.1685\n",
            "Epoch 42/200\n",
            "13/13 [==============================] - 23s 2s/step - loss: 0.0624 - binary_accuracy: 0.9819 - average_precision: 0.1923 - val_loss: 0.0782 - val_binary_accuracy: 0.9810 - val_average_precision: 0.1726\n",
            "Epoch 43/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0625 - binary_accuracy: 0.9819 - average_precision: 0.1920 - val_loss: 0.0741 - val_binary_accuracy: 0.9811 - val_average_precision: 0.1721\n",
            "Epoch 44/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0622 - binary_accuracy: 0.9820 - average_precision: 0.1939 - val_loss: 0.0732 - val_binary_accuracy: 0.9812 - val_average_precision: 0.1734\n",
            "Epoch 45/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0620 - binary_accuracy: 0.9820 - average_precision: 0.1946 - val_loss: 0.0721 - val_binary_accuracy: 0.9812 - val_average_precision: 0.1764\n",
            "Epoch 46/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0618 - binary_accuracy: 0.9820 - average_precision: 0.1980 - val_loss: 0.0700 - val_binary_accuracy: 0.9812 - val_average_precision: 0.1768\n",
            "Epoch 47/200\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0616 - binary_accuracy: 0.9821 - average_precision: 0.2010 - val_loss: 0.0696 - val_binary_accuracy: 0.9813 - val_average_precision: 0.1783\n",
            "Epoch 48/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0616 - binary_accuracy: 0.9821 - average_precision: 0.2018 - val_loss: 0.0685 - val_binary_accuracy: 0.9813 - val_average_precision: 0.1793\n",
            "Epoch 49/200\n",
            "13/13 [==============================] - 22s 2s/step - loss: 0.0615 - binary_accuracy: 0.9821 - average_precision: 0.2010 - val_loss: 0.0677 - val_binary_accuracy: 0.9814 - val_average_precision: 0.1797\n",
            "Epoch 50/200\n",
            "13/13 [==============================] - 22s 2s/step - loss: 0.0615 - binary_accuracy: 0.9821 - average_precision: 0.2019 - val_loss: 0.0674 - val_binary_accuracy: 0.9813 - val_average_precision: 0.1784\n",
            "Epoch 51/200\n",
            "13/13 [==============================] - 23s 2s/step - loss: 0.0614 - binary_accuracy: 0.9821 - average_precision: 0.2033 - val_loss: 0.0663 - val_binary_accuracy: 0.9814 - val_average_precision: 0.1816\n",
            "Epoch 52/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0614 - binary_accuracy: 0.9821 - average_precision: 0.2030 - val_loss: 0.0670 - val_binary_accuracy: 0.9814 - val_average_precision: 0.1810\n",
            "Epoch 53/200\n",
            "13/13 [==============================] - 29s 2s/step - loss: 0.0612 - binary_accuracy: 0.9822 - average_precision: 0.2066 - val_loss: 0.0662 - val_binary_accuracy: 0.9814 - val_average_precision: 0.1838\n",
            "Epoch 54/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0611 - binary_accuracy: 0.9822 - average_precision: 0.2076 - val_loss: 0.0662 - val_binary_accuracy: 0.9814 - val_average_precision: 0.1838\n",
            "Epoch 55/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0609 - binary_accuracy: 0.9822 - average_precision: 0.2091 - val_loss: 0.0653 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1847\n",
            "Epoch 56/200\n",
            "13/13 [==============================] - 24s 2s/step - loss: 0.0609 - binary_accuracy: 0.9822 - average_precision: 0.2107 - val_loss: 0.0661 - val_binary_accuracy: 0.9814 - val_average_precision: 0.1839\n",
            "Epoch 57/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0608 - binary_accuracy: 0.9822 - average_precision: 0.2109 - val_loss: 0.0655 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1864\n",
            "Epoch 58/200\n",
            "13/13 [==============================] - 32s 3s/step - loss: 0.0607 - binary_accuracy: 0.9823 - average_precision: 0.2124 - val_loss: 0.0650 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1856\n",
            "Epoch 59/200\n",
            "13/13 [==============================] - 32s 3s/step - loss: 0.0605 - binary_accuracy: 0.9823 - average_precision: 0.2146 - val_loss: 0.0650 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1854\n",
            "Epoch 60/200\n",
            "13/13 [==============================] - 29s 2s/step - loss: 0.0605 - binary_accuracy: 0.9823 - average_precision: 0.2139 - val_loss: 0.0652 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1863\n",
            "Epoch 61/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0605 - binary_accuracy: 0.9823 - average_precision: 0.2137 - val_loss: 0.0650 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1864\n",
            "Epoch 62/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0603 - binary_accuracy: 0.9824 - average_precision: 0.2169 - val_loss: 0.0653 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1842\n",
            "Epoch 63/200\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0601 - binary_accuracy: 0.9824 - average_precision: 0.2192 - val_loss: 0.0647 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1891\n",
            "Epoch 64/200\n",
            "13/13 [==============================] - 29s 2s/step - loss: 0.0599 - binary_accuracy: 0.9824 - average_precision: 0.2211 - val_loss: 0.0653 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1883\n",
            "Epoch 65/200\n",
            "13/13 [==============================] - 29s 2s/step - loss: 0.0602 - binary_accuracy: 0.9824 - average_precision: 0.2197 - val_loss: 0.0649 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1896\n",
            "Epoch 66/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0600 - binary_accuracy: 0.9824 - average_precision: 0.2219 - val_loss: 0.0650 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1889\n",
            "Epoch 67/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0599 - binary_accuracy: 0.9825 - average_precision: 0.2211 - val_loss: 0.0647 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1895\n",
            "Epoch 68/200\n",
            "13/13 [==============================] - 39s 3s/step - loss: 0.0598 - binary_accuracy: 0.9825 - average_precision: 0.2233 - val_loss: 0.0650 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1897\n",
            "Epoch 69/200\n",
            "13/13 [==============================] - 37s 3s/step - loss: 0.0598 - binary_accuracy: 0.9825 - average_precision: 0.2220 - val_loss: 0.0649 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1900\n",
            "Epoch 70/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0598 - binary_accuracy: 0.9825 - average_precision: 0.2237 - val_loss: 0.0651 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1904\n",
            "Epoch 71/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0597 - binary_accuracy: 0.9825 - average_precision: 0.2242 - val_loss: 0.0649 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1921\n",
            "Epoch 72/200\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.0595 - binary_accuracy: 0.9825 - average_precision: 0.2272 - val_loss: 0.0649 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1916\n",
            "Epoch 73/200\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.0595 - binary_accuracy: 0.9826 - average_precision: 0.2277 - val_loss: 0.0649 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1924\n",
            "Epoch 74/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0594 - binary_accuracy: 0.9826 - average_precision: 0.2298 - val_loss: 0.0648 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1933\n",
            "Epoch 75/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0594 - binary_accuracy: 0.9826 - average_precision: 0.2294 - val_loss: 0.0649 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1920\n",
            "Epoch 76/200\n",
            "13/13 [==============================] - 29s 2s/step - loss: 0.0594 - binary_accuracy: 0.9826 - average_precision: 0.2298 - val_loss: 0.0649 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1927\n",
            "Epoch 77/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0593 - binary_accuracy: 0.9826 - average_precision: 0.2308 - val_loss: 0.0647 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1944\n",
            "Epoch 78/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0594 - binary_accuracy: 0.9826 - average_precision: 0.2308 - val_loss: 0.0647 - val_binary_accuracy: 0.9817 - val_average_precision: 0.1955\n",
            "Epoch 79/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0591 - binary_accuracy: 0.9826 - average_precision: 0.2333 - val_loss: 0.0648 - val_binary_accuracy: 0.9817 - val_average_precision: 0.1951\n",
            "Epoch 80/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0589 - binary_accuracy: 0.9827 - average_precision: 0.2345 - val_loss: 0.0649 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1945\n",
            "Epoch 81/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0590 - binary_accuracy: 0.9827 - average_precision: 0.2353 - val_loss: 0.0648 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1963\n",
            "Epoch 82/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0590 - binary_accuracy: 0.9827 - average_precision: 0.2345 - val_loss: 0.0645 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1968\n",
            "Epoch 83/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0590 - binary_accuracy: 0.9827 - average_precision: 0.2348 - val_loss: 0.0648 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1948\n",
            "Epoch 84/200\n",
            "13/13 [==============================] - 29s 2s/step - loss: 0.0590 - binary_accuracy: 0.9826 - average_precision: 0.2339 - val_loss: 0.0651 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1952\n",
            "Epoch 85/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0590 - binary_accuracy: 0.9827 - average_precision: 0.2354 - val_loss: 0.0646 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1982\n",
            "Epoch 86/200\n",
            "13/13 [==============================] - 31s 2s/step - loss: 0.0588 - binary_accuracy: 0.9827 - average_precision: 0.2386 - val_loss: 0.0646 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1962\n",
            "Epoch 87/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0586 - binary_accuracy: 0.9827 - average_precision: 0.2389 - val_loss: 0.0647 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1969\n",
            "Epoch 88/200\n",
            "13/13 [==============================] - 42s 3s/step - loss: 0.0586 - binary_accuracy: 0.9827 - average_precision: 0.2401 - val_loss: 0.0648 - val_binary_accuracy: 0.9815 - val_average_precision: 0.1971\n",
            "Epoch 89/200\n",
            "13/13 [==============================] - 38s 3s/step - loss: 0.0587 - binary_accuracy: 0.9827 - average_precision: 0.2395 - val_loss: 0.0644 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2003\n",
            "Epoch 90/200\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.0585 - binary_accuracy: 0.9828 - average_precision: 0.2425 - val_loss: 0.0645 - val_binary_accuracy: 0.9817 - val_average_precision: 0.1988\n",
            "Epoch 91/200\n",
            "13/13 [==============================] - 32s 3s/step - loss: 0.0585 - binary_accuracy: 0.9828 - average_precision: 0.2419 - val_loss: 0.0648 - val_binary_accuracy: 0.9817 - val_average_precision: 0.1991\n",
            "Epoch 92/200\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.0586 - binary_accuracy: 0.9827 - average_precision: 0.2406 - val_loss: 0.0649 - val_binary_accuracy: 0.9816 - val_average_precision: 0.1983\n",
            "Epoch 93/200\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.0588 - binary_accuracy: 0.9827 - average_precision: 0.2370 - val_loss: 0.0646 - val_binary_accuracy: 0.9817 - val_average_precision: 0.1994\n",
            "Epoch 94/200\n",
            "13/13 [==============================] - 41s 3s/step - loss: 0.0585 - binary_accuracy: 0.9828 - average_precision: 0.2429 - val_loss: 0.0647 - val_binary_accuracy: 0.9817 - val_average_precision: 0.1994\n",
            "Epoch 95/200\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.0583 - binary_accuracy: 0.9828 - average_precision: 0.2458 - val_loss: 0.0644 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2028\n",
            "Epoch 96/200\n",
            "13/13 [==============================] - 32s 3s/step - loss: 0.0582 - binary_accuracy: 0.9828 - average_precision: 0.2475 - val_loss: 0.0649 - val_binary_accuracy: 0.9816 - val_average_precision: 0.2013\n",
            "Epoch 97/200\n",
            "13/13 [==============================] - 47s 4s/step - loss: 0.0583 - binary_accuracy: 0.9828 - average_precision: 0.2440 - val_loss: 0.0647 - val_binary_accuracy: 0.9817 - val_average_precision: 0.1999\n",
            "Epoch 98/200\n",
            "13/13 [==============================] - 31s 2s/step - loss: 0.0584 - binary_accuracy: 0.9828 - average_precision: 0.2443 - val_loss: 0.0644 - val_binary_accuracy: 0.9816 - val_average_precision: 0.2020\n",
            "Epoch 99/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0583 - binary_accuracy: 0.9828 - average_precision: 0.2451 - val_loss: 0.0647 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2024\n",
            "Epoch 100/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0582 - binary_accuracy: 0.9828 - average_precision: 0.2471 - val_loss: 0.0646 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2021\n",
            "Epoch 101/200\n",
            "13/13 [==============================] - 40s 3s/step - loss: 0.0585 - binary_accuracy: 0.9828 - average_precision: 0.2435 - val_loss: 0.0646 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2010\n",
            "Epoch 102/200\n",
            "13/13 [==============================] - 35s 3s/step - loss: 0.0582 - binary_accuracy: 0.9828 - average_precision: 0.2465 - val_loss: 0.0648 - val_binary_accuracy: 0.9816 - val_average_precision: 0.2027\n",
            "Epoch 103/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0580 - binary_accuracy: 0.9829 - average_precision: 0.2499 - val_loss: 0.0649 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2025\n",
            "Epoch 104/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0580 - binary_accuracy: 0.9829 - average_precision: 0.2496 - val_loss: 0.0649 - val_binary_accuracy: 0.9816 - val_average_precision: 0.2010\n",
            "Epoch 105/200\n",
            "13/13 [==============================] - 27s 2s/step - loss: 0.0581 - binary_accuracy: 0.9829 - average_precision: 0.2481 - val_loss: 0.0648 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2027\n",
            "Epoch 106/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0581 - binary_accuracy: 0.9829 - average_precision: 0.2472 - val_loss: 0.0647 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2034\n",
            "Epoch 107/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0581 - binary_accuracy: 0.9829 - average_precision: 0.2492 - val_loss: 0.0646 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2026\n",
            "Epoch 108/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0580 - binary_accuracy: 0.9829 - average_precision: 0.2507 - val_loss: 0.0648 - val_binary_accuracy: 0.9816 - val_average_precision: 0.2039\n",
            "Epoch 109/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0579 - binary_accuracy: 0.9829 - average_precision: 0.2517 - val_loss: 0.0648 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2039\n",
            "Epoch 110/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0579 - binary_accuracy: 0.9829 - average_precision: 0.2521 - val_loss: 0.0646 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2051\n",
            "Epoch 111/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0576 - binary_accuracy: 0.9830 - average_precision: 0.2544 - val_loss: 0.0648 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2034\n",
            "Epoch 112/200\n",
            "13/13 [==============================] - 24s 2s/step - loss: 0.0577 - binary_accuracy: 0.9830 - average_precision: 0.2541 - val_loss: 0.0647 - val_binary_accuracy: 0.9816 - val_average_precision: 0.2057\n",
            "Epoch 113/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0578 - binary_accuracy: 0.9830 - average_precision: 0.2544 - val_loss: 0.0645 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2074\n",
            "Epoch 114/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0578 - binary_accuracy: 0.9830 - average_precision: 0.2536 - val_loss: 0.0645 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2066\n",
            "Epoch 115/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0578 - binary_accuracy: 0.9830 - average_precision: 0.2546 - val_loss: 0.0645 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2043\n",
            "Epoch 116/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0577 - binary_accuracy: 0.9830 - average_precision: 0.2544 - val_loss: 0.0649 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2031\n",
            "Epoch 117/200\n",
            "13/13 [==============================] - 25s 2s/step - loss: 0.0577 - binary_accuracy: 0.9830 - average_precision: 0.2551 - val_loss: 0.0647 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2064\n",
            "Epoch 118/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0575 - binary_accuracy: 0.9830 - average_precision: 0.2583 - val_loss: 0.0645 - val_binary_accuracy: 0.9818 - val_average_precision: 0.2073\n",
            "Epoch 119/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0576 - binary_accuracy: 0.9830 - average_precision: 0.2564 - val_loss: 0.0650 - val_binary_accuracy: 0.9816 - val_average_precision: 0.2052\n",
            "Epoch 120/200\n",
            "13/13 [==============================] - 28s 2s/step - loss: 0.0576 - binary_accuracy: 0.9830 - average_precision: 0.2556 - val_loss: 0.0648 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2083\n",
            "Epoch 121/200\n",
            "13/13 [==============================] - 26s 2s/step - loss: 0.0575 - binary_accuracy: 0.9830 - average_precision: 0.2586 - val_loss: 0.0649 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2061\n",
            "Epoch 122/200\n",
            "13/13 [==============================] - 30s 2s/step - loss: 0.0576 - binary_accuracy: 0.9830 - average_precision: 0.2565 - val_loss: 0.0647 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2075\n",
            "Epoch 123/200\n",
            "13/13 [==============================] - 39s 3s/step - loss: 0.0575 - binary_accuracy: 0.9830 - average_precision: 0.2582 - val_loss: 0.0646 - val_binary_accuracy: 0.9818 - val_average_precision: 0.2066\n",
            "Epoch 124/200\n",
            "13/13 [==============================] - 32s 2s/step - loss: 0.0575 - binary_accuracy: 0.9830 - average_precision: 0.2581 - val_loss: 0.0648 - val_binary_accuracy: 0.9817 - val_average_precision: 0.2054\n",
            "Epoch 125/200\n",
            " 7/13 [===============>..............] - ETA: 11s - loss: 0.0574 - binary_accuracy: 0.9830 - average_precision: 0.2644"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[61], line 62\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#from tensorflow.keras.models import Sequential\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#from tensorflow.keras.layers import Dense, BatchNormalization, LayerNormalization, PReLU, Activation\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n\u001b[0;32m     56\u001b[0m model_bp\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     57\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m),\n\u001b[0;32m     58\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     59\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC(multi_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, curve\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPR\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_precision\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     60\u001b[0m )\n\u001b[1;32m---> 62\u001b[0m hist_bp \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_bp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_bp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_bp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_bp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_bp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\n\u001b[0;32m     67\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "INPUT_SHAPE = [train_df.shape[1]]\n",
        "BATCH_SIZE = 5120\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model_bp = tf.keras.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(input_shape=INPUT_SHAPE),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "    tf.keras.layers.Dense(units=len(labels_bp),activation='sigmoid')\n",
        "])\n",
        "\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import Dense, BatchNormalization, LayerNormalization, PReLU, Activation\n",
        "\n",
        "#model_bp = tf.keras.Sequential([\n",
        "#    tf.keras.layers.BatchNormalization(input_shape=(INPUT_SHAPE)),\n",
        "#    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "#    tf.keras.layers.Dropout(0.2),\n",
        "#    tf.keras.layers.PReLU(),\n",
        "#    tf.keras.layers.LayerNormalization(axis=1),\n",
        "#\n",
        "#    # Second layer with Batch Normalization\n",
        "#    tf.keras.layers.BatchNormalization(),\n",
        "#    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "#    tf.keras.layers.Dropout(0.2),\n",
        "#    tf.keras.layers.PReLU(),\n",
        "#    tf.keras.layers.LayerNormalization(axis=1),\n",
        "#\n",
        "#    # Third layer with Batch Normalization\n",
        "#    tf.keras.layers.BatchNormalization(),\n",
        "#    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "#    tf.keras.layers.Dropout(0.2),\n",
        "#    tf.keras.layers.PReLU(),\n",
        "#    tf.keras.layers.LayerNormalization(axis=1),\n",
        "#\n",
        "#    # Fourth layer with Batch Normalization\n",
        "#    # Note: This layer combines 600 and 400 unit layers in the original model\n",
        "#    tf.keras.layers.BatchNormalization(),\n",
        "#    tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='leaky_relu',kernel_regularizer=l2(0.00001)),\n",
        "#    tf.keras.layers.PReLU(),\n",
        "#    tf.keras.layers.LayerNormalization(axis=1),\n",
        "#\n",
        "#    # Sigmoid activation for the output layer\n",
        "#    tf.keras.layers.Dense(units=len(labels_bp),activation='sigmoid')\n",
        "#])\n",
        "#\n",
        "#\n",
        "# Compile model\n",
        "model_bp.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy', tf.keras.metrics.AUC(multi_label=True, curve='PR', name='average_precision')]\n",
        ")\n",
        "\n",
        "hist_bp = model_bp.fit(\n",
        "    X_train_bp.iloc[:,1:], y_train_bp,\n",
        "    validation_data=(X_val_bp.iloc[:,1:], y_val_bp),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_bp.save('model_bp.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CAFA Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cafaeval in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.2.0)Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\marco\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: numpy in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cafaeval) (1.26.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cafaeval) (3.8.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cafaeval) (2.1.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->cafaeval) (1.4.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->cafaeval) (1.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->cafaeval) (23.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->cafaeval) (0.12.1)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->cafaeval) (10.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->cafaeval) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->cafaeval) (3.1.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->cafaeval) (4.47.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from matplotlib->cafaeval) (6.1.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->cafaeval) (2023.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->cafaeval) (2023.3.post1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib->cafaeval) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\marco\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->cafaeval) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install cafaeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cc = load_model('model_cc.keras')\n",
        "model_mf = load_model('model_mf.keras')\n",
        "model_bp = load_model('model_bp.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 1025)\n",
            "32/32 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Length of values (678000) does not match length of index (678)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[82], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m df_submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(l)))\n\u001b[0;32m     62\u001b[0m df_submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProtein Id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m l\n\u001b[1;32m---> 63\u001b[0m \u001b[43mdf_submission\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGO Term Id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m labels_cc \u001b[38;5;241m*\u001b[39m predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     64\u001b[0m df_submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m     65\u001b[0m df_submission\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\frame.py:4091\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4090\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4091\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\frame.py:4300\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4292\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4293\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4299\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4300\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4303\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4304\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4305\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4306\u001b[0m     ):\n\u001b[0;32m   4307\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\frame.py:5039\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5039\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Length of values (678000) does not match length of index (678)"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the test embeddings\n",
        "test_embeddings = []\n",
        "test_protein_ids = []\n",
        "\n",
        "with h5py.File('data/test/test_embeddings.h5', 'r') as f:\n",
        "    for protein_id in f.keys():  # protein ids\n",
        "        embeddings = f[protein_id][:]\n",
        "        test_embeddings.append(embeddings)\n",
        "        test_protein_ids.append(protein_id)\n",
        "\n",
        "test_embeddings_array = np.array(test_embeddings)\n",
        "\n",
        "# https://www.kaggle.com/code/alexandervc/baseline-multilabel-to-multitarget-binary\n",
        "\n",
        "\n",
        "# Convert test_embeddings to dataframe\n",
        "column_num = test_embeddings_array.shape[1]\n",
        "test_df = pd.DataFrame(test_embeddings_array, columns=[\"Column_\" + str(i) for i in range(1, column_num+1)])\n",
        "\n",
        "# Concatenating protein_ids with the embeddings DataFrame\n",
        "test_df = pd.concat((pd.DataFrame(test_protein_ids, columns=['Protein_ID']), test_df), axis=1)\n",
        "#print(test_df.head())\n",
        "\n",
        "print(test_df.shape)\n",
        "\n",
        "# I don't have access to type of aspect in test set, I wanted to select only cc.\n",
        "## Filter for 'cellular_component' aspect\n",
        "#cc_proteins_df = protein2ipr_df[protein2ipr_df['Aspect'] == 'cellular_component']\n",
        "#\n",
        "## Merge to get the subset with the correct aspect and embeddings\n",
        "#cc_test_df = test_df.merge(cc_proteins_df[['Protein_ID']], on='Protein_ID', how='right')\n",
        "\n",
        "# Load the model\n",
        "ccmodel = load_model('model_cc.keras') \n",
        "\n",
        "\n",
        "# how do I know which is the actual go_term the predicted probability refers to?\n",
        "# the submission can have multiple rows pertaining to the same protein but different GO_terms and their probability\n",
        "# Therefore this needs to be addressed.\n",
        "\n",
        "# Maybe see Naive Baseline \n",
        "\n",
        "# Predictions require only the embeddings, not the Protein_ID\n",
        "predictions = ccmodel.predict(test_df.iloc[:, 1:])\n",
        "\n",
        "\n",
        "\n",
        "test_protein_ids = pd.read_csv('data/test/test_ids.txt',header = None)\n",
        "\n",
        "l = []\n",
        "for k in list(test_protein_ids):\n",
        "    l += [ k] * predictions.shape[1]   \n",
        "\n",
        "\n",
        "# Initialize df_submission with the correct number of rows\n",
        "df_submission = pd.DataFrame(index=range(len(l)))\n",
        "\n",
        "df_submission['Protein Id'] = l\n",
        "df_submission['GO Term Id'] = labels_cc * predictions.shape[0]\n",
        "df_submission['Prediction'] = predictions.ravel()\n",
        "df_submission.to_csv(\"submission.tsv\",header=False, index=False, sep=\"\\t\")\n",
        "\n",
        "\n",
        "df_submission.head()  # Display the first few entries in the submission DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_submission.to_csv('submission.tsv',header=None, index=False,sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cafaeval\n",
        "from cafaeval.evaluation import cafa_eval, write_results\n",
        "res = cafa_eval(pred_dir = \"../preds/\" , gt_file=\"ground_truth.tsv\")\n",
        "write_results(*res)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
