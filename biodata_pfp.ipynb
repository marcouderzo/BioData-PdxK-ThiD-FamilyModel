{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3kjFR1QJ84b",
        "outputId": "2303d030-bde1-4bf1-efab-47d7d28a9ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Flatten, Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import h5py\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "data_dir = '/content/drive/MyDrive/BiologicalData/biological_data_pfp/train/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbKoSl0PLyeq",
        "outputId": "9fc4be4e-0332-4fbf-f2f0-7aeaa1701e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_set = pd.read_csv(data_dir+'train_set.tsv', delimiter='\\t')\n",
        "\n",
        "protein_sequences = {}\n",
        "with open(data_dir+'train.fasta', 'r') as fasta_file:\n",
        "    current_protein = ''\n",
        "    for line in fasta_file:\n",
        "        if line.startswith('>'):\n",
        "            current_protein = line.strip()[1:]\n",
        "            protein_sequences[current_protein] = ''\n",
        "        else:\n",
        "            protein_sequences[current_protein] += line.strip()\n",
        "\n",
        "# Load Protein IDs\n",
        "\n",
        "train_ids_df = pd.read_csv(data_dir+'train_ids.txt',header = None)\n",
        "train_ids_df.columns = ['Protein_ID']\n",
        "\n",
        "# Load train_protein2ipr dat file\n",
        "\n",
        "train_protein2ipr = pd.read_csv(data_dir+'train_protein2ipr.dat',header = None, sep='\\t')\n",
        "train_protein2ipr.columns = ['Protein_ID','IPR_ID','desc','db','start','end']\n",
        "\n",
        "# Load ProtT5 embeddings\n",
        "with h5py.File(data_dir+'train_embeddings.h5', 'r') as f:\n",
        "    embeddings_list = []\n",
        "\n",
        "    for protein_id in f.keys(): #protein ids\n",
        "        # Extract the embeddings for each protein\n",
        "        embeddings = f[protein_id][:]\n",
        "        embeddings_list.append(embeddings)\n",
        "\n",
        "\n",
        "prott5_embeddings = np.array(embeddings_list)\n",
        "print(prott5_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6iwRsuYOSc0",
        "outputId": "3c3dfec7-b6b5-45d2-993c-e290cefa3b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(123969, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using only the first 10.000 rows for label encoding, since using the whole set would fill up completely Colab's 12GB resources.\n",
        "# If training on my machine, I have 32GB of memory to use (although training on CPU).\n",
        "subset_train_set = train_set.head(10000)\n",
        "\n",
        "label_encoder_mf = LabelEncoder()\n",
        "labels_mf = label_encoder_mf.fit_transform(subset_train_set[subset_train_set['aspect'] == 'molecular_function']['GO_term'])\n",
        "labels_mf = to_categorical(labels_mf)\n",
        "\n",
        "label_encoder_bp = LabelEncoder()\n",
        "labels_bp = label_encoder_bp.fit_transform(subset_train_set[subset_train_set['aspect'] == 'biological_process']['GO_term'])\n",
        "labels_bp = to_categorical(labels_bp)\n",
        "\n",
        "label_encoder_cc = LabelEncoder()\n",
        "labels_cc = label_encoder_cc.fit_transform(subset_train_set[subset_train_set['aspect'] == 'cellular_component']['GO_term'])\n",
        "labels_cc = to_categorical(labels_cc)\n"
      ],
      "metadata": {
        "id": "Mz_u0SV7QPc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map protein IDs to their corresponding ProtT5 embeddings\n",
        "protein_id_to_embedding = dict(zip(train_set['Protein_ID'], prott5_embeddings))\n",
        "\n",
        "\n",
        "X = []\n",
        "y_mf, y_bp, y_cc = [], [], []\n",
        "\n",
        "for protein_id, label_mf, label_bp, label_cc in zip(train_set['Protein_ID'], labels_mf, labels_bp, labels_cc):\n",
        "    sequence = protein_sequences.get(protein_id, '')  # Get protein sequence from fasta file\n",
        "    embedding = protein_id_to_embedding.get(protein_id, [])  # Get ProtT5 embedding\n",
        "\n",
        "    # Ensure both sequence and embedding are available\n",
        "    if sequence != '' and len(embedding) > 0:\n",
        "        X.append(embedding)  # Use ProtT5 embedding as input\n",
        "        y_mf.append(label_mf)\n",
        "        y_bp.append(label_bp)\n",
        "        y_cc.append(label_cc)\n",
        "\n",
        "# Convert lists to arrays\n",
        "X = np.array(X)\n",
        "y_mf = np.array(y_mf)\n",
        "y_bp = np.array(y_bp)\n",
        "y_cc = np.array(y_cc)\n",
        "\n"
      ],
      "metadata": {
        "id": "qH-SnwF2PXI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train_mf, y_val_mf, y_train_bp, y_val_bp, y_train_cc, y_val_cc = train_test_split(\n",
        "    X,\n",
        "    y_mf,\n",
        "    y_bp,\n",
        "    y_cc,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "7fsDFxR2QWkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I had to do some processing since the dimensions for the NN are not compatible.\n",
        "# Still trying to understand why dimensions don't add up.\n",
        "\n",
        "prott5_embeddings = np.array(embeddings_list)\n",
        "\n",
        "# Assuming prott5_embeddings.shape is (123969, 1024)\n",
        "print(prott5_embeddings.shape)\n",
        "\n",
        "# Reshape to add time step dimension\n",
        "prott5_embeddings = np.expand_dims(prott5_embeddings, axis=1)\n",
        "\n",
        "# Now, the shape should be (123969, 1, 1024)\n",
        "print(prott5_embeddings.shape)\n",
        "\n",
        "# Build the model with pre-trained embeddings\n",
        "embedding_dim = prott5_embeddings.shape[2]  # Assuming ProtT5 embeddings shape is (num_proteins, 1, embedding_dim)\n",
        "input_layer = Input(shape=(1, embedding_dim))\n",
        "conv1d_layer = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
        "\n",
        "num_classes_mf = len(np.unique(labels_mf))\n",
        "num_classes_bp = len(np.unique(labels_bp))\n",
        "num_classes_cc = len(np.unique(labels_cc))\n",
        "\n",
        "\n",
        "maxpooling_layer = MaxPooling1D(pool_size=1)(conv1d_layer)\n",
        "lstm_layer = LSTM(units=100, return_sequences=True)(maxpooling_layer)\n",
        "flatten_layer = Flatten()(lstm_layer)\n",
        "\n",
        "# Define output layers for each sub-ontology\n",
        "output_layer_mf = Dense(units=num_classes_mf, activation='softmax', name='output_mf')(flatten_layer)\n",
        "output_layer_bp = Dense(units=num_classes_bp, activation='softmax', name='output_bp')(flatten_layer)\n",
        "output_layer_cc = Dense(units=num_classes_cc, activation='softmax', name='output_cc')(flatten_layer)\n",
        "\n",
        "# Create the model with multiple outputs\n",
        "# Probably will need to change this and create a model for each sub-ontology\n",
        "model = Model(inputs=input_layer, outputs=[output_layer_mf, output_layer_bp, output_layer_cc])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Assuming X_train has shape (123969, 1024)\n",
        "# Reshape to add time step dimension\n",
        "X_train_reshaped = np.expand_dims(X_train, axis=1)\n",
        "\n",
        "# Now, the shape should be (123969, 1, 1024)\n",
        "\n",
        "print(X_train_reshaped.shape)\n",
        "\n",
        "# Not working from here\n",
        "\n",
        "model.fit(\n",
        "    X_train_reshaped,\n",
        "    {'output_mf': y_train_mf, 'output_bp': y_train_bp, 'output_cc': y_train_cc},\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "Kj8ovFI5LmIg",
        "outputId": "be184c02-910b-4413-c26e-6200f34c9e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(123969, 1024)\n",
            "(123969, 1, 1024)\n",
            "(900, 1, 1024)\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-32c5d7afb6ad>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mX_train_reshaped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'output_mf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output_bp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_bp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output_cc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_cc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 297) and (None, 2) are incompatible\n"
          ]
        }
      ]
    }
  ]
}